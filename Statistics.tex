%
% CHAPTER: Probability Theory
%

\chapterimage{owl} % Chapter heading image

\chapter{Descriptive Statistics}
\label{chap:descriptive_statistics}

\begin{quote}
\begin{flushright}
\emph{Pending}\\
Thomas Bayes
\end{flushright}
\end{quote}
\bigskip


Statistics involves the methods used for collecting, organizing, summarizing, presenting, and analyzing data. It also focuses on drawing valid conclusions and making informed decisions based on this analysis. Statistics typically does not deals with unusual or rare cases.

Rather than studying an entire population, statistics examines a smaller subset known as a sample. When a sample accurately represents a population, significant insights about the population can often be deduced from the sample analysis. The branch of statistics that addresses the conditions under which such deductions are valid is known as inductive statistics, or statistical inference. Since these inferences are not completely certain, probabilistic language is commonly employed when presenting conclusions. Conversely, the branch of statistics that solely aims to describe and analyze a specific group, without making any broader conclusions or inferences, is referred to as descriptive statistics.

{\color{red} TODO: Extend}

%
% Section: Descriptive Statistics
%

\section{Descriptive Statistics}

Statistics is a field that revolves around the collection, analysis, and interpretation of data. At the heart of statistical analysis is the concept of a population, which is fundamental in understanding how data reflects a group of interest. 

\begin{definition}
Let $\mathcal{S}$ be a set referred to as the \emph{population}\index{Population}. Its constituents are termed \emph{individuals}\index{Individual}.
\end{definition}

The set $\mathcal{S}$ must be well-defined, non-empty, and it can be either finite or infinite. 

A variable is a symbol used to represent the characteristics or attributes of individuals within a population. It is called 'variable' because these attributes can vary from one individual to another, although from a pure mathematical perpective the term function might have been more appropriate.

\begin{definition}
Let $\mathcal{S}$ be a population, and let $\mathcal{D}$ be the set containing all possible values that the individuals of $\mathcal{S}$ can assume. A variable $X$ is a function $X: \mathcal{S} \rightarrow \mathcal{D}$ that maps individuals from the population $\mathcal{S}$ to the values in $\mathcal{D}$.
\end{definition}

The set $\mathcal{D}$ can be any set, such as the real numbers, a set of integers, or a set of categorical labels. A variable is called a \emph{quantitative variable}\index{Quantitative variable} if it the elements of $\mathcal{S}$ can be measured numerically. These can be further classified as \emph{discrete variables}\index{Discrete variable} if they can take a countable number of distinct values, and \emph{continuous variables}\index{Continuous variable} if they can take any value within a given range or interval. A variable is called a \emph{qualitative}\index{Qualitative variable} or \emph{categorical variable}\index{Categorical variable} if it represents quantities that cannot be measured numerically. A qualitative variable can be a \emph{nominal variable}\index{Nominal variable} if no natural order exists among the categories, or an \emph{ordinal variable}\index{Ordinal variable} if a natural order exists among the categories.

\begin{example}
Consider a population consisting of the inhabitants of a small village, where we are interested in studying three attributes of these individuals: age, gender, and daily water consumption. The variable representing age, mapping individuals to their ages in years, is a quantitative discrete variable because it can be counted precisely in integer values. Gender, mapping individuals to categories like "Male" or "Female," is a qualitative nominal variable because the categories have no inherent order. Finally, daily water consumption, mapping individuals to the amount of water they drink in liters, is a quantitative continuous variable as it can take any real number within a range, reflecting precise measurements.
\end{example}

% Frequency distributions

\section{Frequency Distributions}

When summarizing large masses of raw data, it is often useful to distribute the data into classes, or categories, and to determine the number of individuals belonging to each class, called absolute frequency. The following definition formally introduces this concept.

\begin{definition}
Let $\mathcal{S}$ be a population consisting of $n$ individuals, and let a variable $X: \mathcal{S} \rightarrow \mathcal{D}$ represent the mapping of individuals in $\mathcal{S}$ to values in the set $\mathcal{D}$, where $k$ is the cardinality of $\mathcal{D}$. The \emph{absolute frequency}\index{Absolute frequency}, also known simply as \emph{frequency}, denoted by $n_i$ for $1 \leq i \leq k$, quantifies the number of individuals in $\mathcal{S}$ for which $X$ assigns the value corresponding to the $i$-th category of $\mathcal{D}$.
\end{definition}

The sum of the frequencies must be equal to population size, that is, $\sum_{i=1}^k n_i = n$.

If the variable $X$ is continuous, the elements of $\mathcal{D}$ are referred to as \emph{class intervals}\index{Class interval}. The endpoints of these intervals are known as \emph{class limits}\index{Class limits}; the smaller number is termed the \emph{lower class limit}\index{Lower class limit}, and the larger number, the \emph{upper class limit}\index{Upper class limit}. A \emph{class interval}\index{Class interval} that lacks either an upper or a lower class limit is known as an \emph{open class interval}\index{Open class interval}. The \emph{width}\index{Class interval width} of a class interval is defined as the difference between the upper and lower class boundaries, denoted by $a_i = e_i - e_{i-1}$. The \emph{class mark}\index{Class mark}, or the midpoint of a class interval, is calculated as $a_i = \frac{e_i + e_{i-1}}{2}$. It is assumed that all observations within a specific class interval are equivalent concerning their categorical assignment. Refer to Section \ref{sec:discretization_algorithms} for more information about how to discretize a continous variable into discrete intervals.

\begin{example}
In a study measuring adult heights within a community, researchers record heights ranging from 150 cm to 200 cm and organize them into 10 cm class intervals: 150-159 cm, 160-169 cm, 170-179 cm, 180-189 cm, and 190-199 cm. Each interval's lower and upper class limits are respectively the start and end points, such as 150 cm and 159 cm for the first interval. If the last interval had no specified upper limit, it would be considered an open class interval. The class width, typically 10 cm, is the operational span between boundaries, and the class mark, calculated as the midpoint of each interval (e.g., 154.5 cm for the first interval), provides a central value for summarizing data within that range.
\end{example}

A tabular arrangement of data by classes together with the corresponding class frequencies is called a frequency distribution, or frequency table.

\begin{definition}
Let $\mathcal{S}$ be a population consisting of $n$ individuals, and let $X: \mathcal{S} \rightarrow \mathcal{D}$ be a variable that maps individuals in $\mathcal{S}$ to $k$ distinct values of the set $\mathcal{D}$. A \emph{frequency distribution}\index{Frequency distribution} is represented the set of pairs $\{(d_i, n_i) : 1 \leq i \leq k\}$, where $d_i$ denotes the $i$-th interval and $n_i$ represents the number of individuals from $\mathcal{S}$ whose value under $X$ falls within the interval $d_i$.
\end{definition}

Frequency distributions are useful for statistical analysis and helps in visualizing data by grouping values, which simplifies the understanding of distribution and central tendencies within the data.

The relative frequency of a class is the frequency of the class divided by the total frequency of all classes

\begin{definition}
Let $\mathcal{S}$ be a population consisting of $n$ individuals, and let a variable $X: \mathcal{S} \rightarrow \mathcal{D}$ represent the mapping of individuals in $\mathcal{S}$ to values in the set $\mathcal{D}$, where $k$ is the cardinality of $\mathcal{D}$. The \emph{relative frequency}\index{Relative frequency}, denoted by $f_i$, is the ratio $f_i = \frac{n_i}{n}$ for $1 \leq i \leq k$.
\end{definition}

The sum of the relative frequencies is equal to one, that is, $\sum_{i=1}^k f_i = 1$.

The total frequency of all values less than the upper class boundary of a given class interval is called the cumulative frequency up to and including that class interval.

\begin{definition}
Let $\mathcal{S}$ be a population consisting of $n$ individuals, and let a variable $X: \mathcal{S} \rightarrow \mathcal{D}$ represent the mapping of individuals in $\mathcal{S}$ to values in the set $\mathcal{D}$, where $k$ is the cardinality of $\mathcal{D}$. The \emph{cumulative frequency}\index{Cumulative frequency}, denoted by $N_i$ for $1 \leq i \leq k$, represents the total number of individuals in $\mathcal{S}$ for which $X$ assigns a value less than or equal to the upper limit of the $i$-th category of $\mathcal{D}$. This is mathematically expressed as $N_i = \sum_{j=1}^i n_j$, where $n_j$ is the absolute frequency of the $j$-th category.
\end{definition}

By accumulating the frequencies up to each category or interval, cumulative frequencies provides a running total that shows how many data points fall below a certain value.

%
% Section: Characterizing Distributions
%

\section{Characterizing Distributions}
\label{sec:measures_central_tendency}

A \emph{measure of central tendency} is a number derived from a population, intended as a summary of that population. The most common measures of central tendency in use are the mode, the median and the mean. Each of these measures provides a different approach to characterize populations. It is also common to use \emph{metrics of dispersion} to describe the variability of a frequency distribution around the measures of centrality. We will review two metrics of dispersion, the variance and the standard deviation. All these measures allow us to summarize and compare populations.

% The Mean

\subsection*{Mean}
\label{sec:mean}

The most commonly used measure of central tendency is the \emph{mean}. The mean of a population is the weighted average of the distinct values of the individuals in the population, where weights are given by the frequencies.

\begin{definition}
\label{def:mean}
Let $X$ be a variable representing the mapping of the $N$ individuals of a population $\mathcal{S}$ to the set of possible values $\mathcal{D} = \{x_1, x_2, \dots, x_k\}$, each occurring with frequency $\{n_1, n_2, \dots, n_k\}$ and relative frequency $\{f_1, f_2, \dots, f_k\}$. The \emph{mean}\index{mean} of $X$, denoted by $\overline{X}$, is defined as:
\[
\overline{X} = \frac{1}{N} \sum_{i=1}^k n_i x_i = \sum_{i=1}^k f_i x_i
\]
\end{definition}

Definition \ref{def:mean} can be directly applied to the case of quantitative discrete variables, but it requires some considerations in the case of continuous variables. For continuous cases, we could use the average of the observed values. That is, if for individuals $s_1, \ldots, s_N$ we have observed the values $y_1, \ldots, y_N$ for the characteristic of interest, the mean would be given by:
\[
\overline{X} = \frac{1}{N} \sum_{i=1}^N y_i 
\]
Alternatively, we could group the observed values into a reduced number of intervals. In that case, the mean could be computed as the weighted average of the class marks $a_1, \ldots, a_k$:
\[
\overline{X} = \frac{1}{N} \sum_{i=1}^k n_i a_i
\]
We must be careful about the number of intervals, as it is described in Section \ref{sec:discretization_algorithms}.

The mean, while widely used as a measure of central tendency, has several notable limitations. First and foremost, it is not suitable for categorical variables, where numerical operations such as averaging are not meaningful. In such cases, the mode is typically used as a more appropriate measure of central tendency. Another limitation of the mean is that it may not correspond to an actual value observed in the dataset, particularly in datasets with wide ranges or unusual distributions. This can make the mean less representative of typical values in the dataset. Furthermore, the mean is sensitive to outliers or extreme values; even a small change in the frequency of a large number in the set $\mathcal{D}$ can disproportionately affect the mean, potentially skewing analysis. This sensitivity makes the mean less robust compared to other measures, such as the median, which is less influenced by outliers and extreme data points.

\begin{example}
Consider a small company with a population of 10 employees, and that th characteristic in which we are interetes is their salaries. Let's $\mathcal{D} = \{50, 300\}$ with  frequencies of $9$ and $1$ respectively. The mean salary is calculated as:
\[ 
\overline{X} = \frac{(50 \times 9) + (300 \times 1)}{10} = \frac{450 + 300}{10} = 75
\]
Now, suppose the executive receives a raise or another executive with the same high salary joins the company, making two employees earning \$300,000. The recalculated mean salary would be:
\[
\overline{X} = \frac{(50 \times 8) + (300 \times 2)}{10} = \frac{400 + 600}{10} = 100
\]
This example shows how just a small change, a single additional person earning a significantly higher salary, can substantially increase the mean salary from \$75,000 to \$100,000.
\end{example}

The preference for the mean over other measures of central tendency such as the median or mode is primarily due to its mathematical properties that facilitate algebraic manipulations and further statistical analysis. For example, the mean is linear, which means that the mean of a linear transformation of a variable is the same transformation applied to the mean of the variable.

\begin{proposition}
Let $X_1, \ldots, X_n$ be $n$ variables with means $\overline{X}_1, \ldots, \overline{X}_n$, and let $a_1, \ldots, a_n$ and $b$ constants. The mean of a linear combination of the variables $X' = a_1 X_1 + \ldots + a_n X_n + b$ is given by
\[
\overline{X}' = a_1 \overline{X}_1 + \ldots + a_n \overline{X}_n + b
\]
\end{proposition}
\begin{proof}
{\color{red} TODO: Review}
We have that
\begin{multline}
\overline{X}' = \sum_{x_1} \ldots \sum_{x_n} \left(a_ 1 x_1 + \ldots + a_n x_n + b  \right) f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} \ldots \sum_{x_n} a_1 x_1 f\left(x_1, \ldots, x_n \right) + \ldots + \sum_{x_1} \ldots \sum_{x_n} a_n x_n f\left(x_1, \ldots, x_n \right) + \sum_{x_1} \ldots \sum_{x_n} b f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} a_1 x_1 f\left(x_1\right) + \ldots + \sum_{x_n} a_n x_n f\left( x_n \right) + b = 
a_1 \sum_{x_1} x_1 f\left(x_1\right) + \ldots + a_n \sum_{x_n} x_n f\left( x_n \right) + b = \\
a_1 \overline{X}_1 + \ldots + a_n \overline{X}_n + b
\end{multline}
\end{proof}

% The Median

\subsubsection*{The Median}
\label{sec:median}

The median represents the middle value in a population when the individuals are arranged in order given a characteristic. This measure is particularly valuable because it provides a clear indication of the central location of the individuals, especially in cases where the population may be skewed or contain outliers. The median is often favored over the mean in these situations because it is less sensitive to extreme values, making it a more robust indicator of the population's typical value.

\begin{definition}
Let $X$ be a variable mapping individuals of a population $\mathcal{S}$ to the set of values $\mathcal{D} = \{x_1, x_2, \dots, x_k\}$, occurring with frequencies $\{n_1, n_2, \dots, n_k\}$. Assuming that the values of $\mathcal{D}$ are sorted in ascending order, the \emph{median}\index{Median} of $X$, denoted by $Me$, is the value $x_i$ that separates the higher half of the data from the lower half, that is:
\[
\sum_{j=1}^{i} n_j =  \sum_{j=i+1}^{k} n_j
\]
\end{definition}

In case of categorical variables, discrete numerical variables, or continuous variables that have been aggregated in intervals, the median might be non-unique, or it might event does not exsits.

\begin{example}
Consider a course feedback scenario where $N$ students rate their satisfaction on the ordinal scale $\mathcal{D} = \{"Poor", "Fair", "Good", "Very Good", "Excellent"\}$ with the following frequencies $\{0, 200, 300, 400, 100 \}$. Here, the median would be described as falling between "Good" and "Very Good," illustrating a case where the median is not uniquely determined.
\end{example}

The advantages of the median include its robustness against outliers; unlike the mean, the median is not significantly affected by extreme values or skewed data, offering a more accurate reflection of the typical value. It is also useful for ordinal data, as the median can be calculated where the mean cannot be meaningfully applied. However, the median has its disadvantages. It shows limited sensitivity to changes in data; small adjustments near the median do not affect its value, and changes at the ends of the dataset might not influence the median unless they alter its position. Additionally, sorting data to find the median can be computationally intensive for large datasets.

The median of a population remains invariant under any monotonic transformation, meaning if the population undergoes a transformation that preserves the order of data points, the position of the median remains unchanged.

\begin{proposition}
Let \( X = \{x_1, x_2, \dots, x_n\} \) be a dataset with median \( M \). For any monotonically increasing function \( f \), the dataset \( Y = \{f(x_1), f(x_2), \dots, f(x_n)\} \) has the median \( f(M) \).
\end{proposition}
\begin{proof}
Given that \( f \) is monotonically increasing, the order of the data points is preserved. Therefore, the middle value of \( Y \) after applying \( f \) is simply \( f \) applied to the median of \( X \), which is \( f(M) \).
\end{proof}

% The Mode

\subsubsection*{The Mode}
\label{sec:mode}

The mode is a measure of central tendency often described as the most frequently occurring value in a population. It is particularly significant because it provides a direct indication of the most common category or value in a population. This makes the mode especially useful in analyzing categorical data where numerical measures like mean and median are not applicable.

\begin{definition}
\label{def:mode}
Let $X$ be a variable mapping individuals of a population $\mathcal{S}$ to the set of values $\mathcal{D} = \{x_1, x_2, \dots, x_k\}$, occurring with frequencies $\{n_1, n_2, \dots, n_k\}$. The \emph{mode}\index{mode} of $X$, denoted by $Mo$, is defined as the value in $\mathcal{D}$ that has the highest frequency.
\end{definition}

The most significant advantages of the mode are its simplicity and ease of computation, along with its robustness to extreme values. Unlike the mean, the mode remains stable in the presence of outliers. However, it also has some drawbacks. For instance, it is not uniquely defined when two or more values occur with the same highest frequency, resulting in a dataset that may have multiple modes, making it bimodal or multimodal. Additionally, the mode is less informative about the population as a whole compared to the mean or median, particularly with numerical data. Finally, the mode cannot be applied to continuous variables unless the data has been categorized into intervals, in which case it is referred to as the \emph{class mode}.

The mode is invariant under scale transformations.

\begin{proposition}
Let $X$ be a variable mapping individuals of a population $\mathcal{S}$ to the set of values $\mathcal{D} = \{x_1, x_2, \dots, x_k\}$ with mode $Mo$. Then, for any real numbers $a \neq 0$ and $b$, the variable $X'$ mapping the individuals of $\mathcal{S}$ to the set $\mathcal{D}' = \{a x_1 + b, a x_2 + b, \dots, a x_k + b\}$ has the mode $a Mo + b$.
\end{proposition}
\begin{proof}
Since multiplying and adding constants to each element in $\mathcal{D}$ maintains the relative frequency of the values, the transformed version of $Mo$ will appear with the same frequency in dataset $\mathcal{D}'$, thus, $a Mo + b$ is the mode of $X'$.
\end{proof}

{\color{red} TODO: Compare mean with median and with mode.}

% The Variance

\subsubsection*{The Variance}
\label{sec:variance}

Variance is a statistical measure used to determine the spread or dispersion of a population around its mean. It quantifies how much the values of the attributes are likely to differ from the mean. Understanding variance is crucial because it provides insights into the variability within the population, and the reliability of its mean.

\begin{definition}
Let $X$ be a variable mapping individuals of a population $\mathcal{S}$ to the set of values $\mathcal{D} = \{x_1, x_2, \dots, x_k\}$, occurring with frequencies $\{n_1, n_2, \dots, n_k\}$, and let $\overline{X}$ be the mean of $X$. The \emph{variance}\index{variance} of $X$, denoted by \( \sigma^2 \), is defined as:
\[
\sigma^2 = \frac{1}{n} \sum_{i=1}^k n_i (x_i - \overline{X})^2
\]
\end{definition}

The variance offers a precise quantitative measure of how much each data point deviates from the mean, which is crucial for statistical tests and analyses. It serves as the foundation for other important statistical measures, such as the analysis of variance (ANOVA). However, variance has some disadvantages. It is highly sensitive to outliers because it squares the differences from the mean, which can disproportionately influence the result. Additionally, the units of variance are the squares of the units of the data, making interpretation challenging.

\begin{example}
Consider a financial analyst assessing the risk of different investment portfolios. The returns for two portfolios over the last year are as follows: Portfolio A $\{ 5\%, 7\%, 7\%, 5\%, 6\% \}$, Portfolio B $\{ 2\%, 10\%, 2\%, 10\%, 3\%\}$. Calculating the variance for both will show that Portfolio B has a higher variance compared to Portfolio A, indicating that B is riskier with more significant fluctuations in returns.
\end{example}

A property of variance is that it is always non-negative, as it is the sum of squared numbers.

{\color{red} Do we need a proposition for this?}

\begin{proposition}
For any statistical variable $X$, the variance $\sigma^2$ is non-negative.
\end{proposition}
\begin{proof}
The variance is defined as the average of the squared differences from the mean. Since squaring any real number always results in a non-negative value, the sum and thus the average of these squared differences cannot be negative.
\end{proof}

{\color{red} Introduce the following proposition}

\begin{proposition}
For constants a and b, let $Y=aX+b$, then $Var\left(Y\right)=a^{2}Var\left(X\right)$
and $\sigma_{Y}=\left|a\right|\sigma_{X}$.
\end{proposition}
\begin{proof}
{\color{red} PENDING}
\end{proof}

{\color{red} Introduce the following proposition}

\begin{proposition}
For every random variable X, $Var\left(X\right)=E\left(X^{2}\right)-\left[E\left(X\right)\right]^{2}$.
\end{proposition}
\begin{proof}
{\color{red} PENDING}
\end{proof}

{\color{red} The standard deviation of X is the nonnegative square root of $Var\left(X\right)$ if the variance exists. It is common to denote the standard deviation by the symbol $\sigma$, and the variance by $\sigma^{2}$. Variance depeds only on the distribution.}

Standard deviation is a widely used measure of variability or dispersion in statistics, indicating how spread out the numbers in a data set are around the mean. Essentially, it tells us how much the values in the dataset deviate, on average, from the mean. This measurement is crucial because it provides a clear, understandable metric of spread that is in the same units as the data, unlike variance, which squares these units.

\begin{definition}
For a population with values \(x_1, x_2, \dots, x_n\) and a population mean \(\mu\), the \emph{standard deviation} \( \sigma \) is defined as the square root of the variance:
\[
\sigma = \sqrt{\sigma^2} = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2}
\]
For a sample from this population, with sample mean \(\overline{x}\) and sample size \(n\), the sample standard deviation \( s \) is defined as:
\[
s = \sqrt{s^2} = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (x_i - \overline{x})^2}
\]
\end{definition}

Standar deviantion present some advantages, like its clarity in Interpretation, standard deviation is expressed in the same units as the data, making it easier to interpret compared to variance. It has a foundational character in statistical analysis, serving as the basis for the calculation of z-scores, confidence intervals, and is critical in hypothesis testing. Its main disacvantage is its sensitivity to outliers, like variance, standard deviation is also sensitive to outliers, as it involves the squares of differences from the mean.

\begin{example}
Imagine an educational researcher comparing test scores from two classrooms to determine consistency in student performance. Classroom A scores: 82, 86, 88, 85, 90; Classroom B scores: 70, 90, 71, 89, 72. Calculating the standard deviation will show a higher deviation for Classroom B, indicating greater variability in test scores compared to Classroom A.
\end{example}

A fundamental property of standard deviation is that it can never be negative. This is intrinsic to its definition as the square root of variance.

Standard deviation is indispensable in data analysis, providing insights into the consistency and variability of data. It aids in understanding how clustered or spread out the data is around the mean, and despite its limitations, remains one of the most important and widely used measures of dispersion in statistical practice.

%
% Section: Discretization of Continuous Data
%

\section{Discretization of Continuous Variables}
\label{sec:discretization_algorithms}

{\color{red} TODO: Rewrite this section.}

Let $\mathcal{X}$ a continuous random variable that follows a probability density function $P_\mathcal{X}$, and assume we have collected $n$ independent and identically distributed samples $\bold{x} = \{x_1, \ldots, x_n\}$ from $\mathcal{X}$. We are interested in computing the length of a compressed version of $\bold{x}$ using an optimal compressor. Unfortunately, and except for some degenerate distributions, there is no lossless compression algorithm that produces a string with fewer bits than encoding directly the elements $\bold{x}$. Compression algorithms for continuous data only work in case that the elements of $\bold{x}$ are not independent, as it is the case with images or sound. But, if this is not the case, the only option available to compress $\bold{x}$ is to use a lossy compression algorithm, where some information is lost.

We are looking for an algorithm to produce a finite non-overlapping partition of $m$ discrete intervals $D=\{ [d_o, d_1], (d_1, d_2], \ldots, (d_{m-1}, d_m] \}$, where $d_o = \min{\bold{x}_j}$, and $d_m = \max{\bold{x}_j}$, and $d_i < d_{i+1}$ for $i = 0, 1, \ldots, m-1$, assign a unique label to each interval, and encode the elements of $\bold{x}$ using this labeling schema. As compression algorithm we will use an optimal length code given the relative frequencies of the labels in the encoded vector. In this sense, our goal is to have a collection of intervals with sufficiently number of samples (so they are statistically significant) and that the distribution of frequencies resembles the original probability distribution $P_\mathcal{X}$.

A discretization algorithm is a mapping between a (possibly huge) number of numeric values and a reduced set of discrete values, and so, it is a process in which some information is potentially lost. The choice of discretization algorithm is something that could have a high impact in the practical computation of the nescience. We are interested in a discretization algorithm that produces a large number of intervals (low bias), with a large number of number of observations per interval (low variance). Common techniques include \emph{equal width discretization}, \emph{equal frequency discretization} and \emph{fixed frequency discretization}. However, these techniques require the optimization of an hyperparameter, and so, they are not suitable for our purposes.

In a \emph{proportional discretization approach} the number of intervals $m$ and the number of observations per interval $s$ are equally proportional to the number of observations $n$. The algorithm starts by sorting the values of $\bold{x}_j$ in ascending order and then discretizing them into $m$ intervals of approximately $s$ (possibly identical) values each. In this way, as the number of training observations increases, both interval frequency and number of intervals increases, taking advantage of the larger number of observations. In the same way, when the number of observations decreases, we reduce both.

\subsection{k-means Clustering}
\label{sec:kmeans_clustering}

K-means clustering is an algorithms that partitions n observations into k clusters in such a way that each observation belongs to the cluster with the nearest mean. In this way, we can replace the observation that belong to a cluster by its means, as a discretization. The optimization criteria in k-means is to minize the within-cluster variance. Given the fact that the problem is NP-Complete, some approximation algorithms are used instead.

{\color{red} TODO: Define the concept of Voronoi diagram / Voronoi cell}


%
% Multivariate Distributions
%

\section{Multivariate Distributions}
\label{sec:multivariate_distributions}

{\color{red} TODO: Introduce the concept of multivariate distribution}

\begin{definition}
Let $\mathcal{S}$ be a population consisting of $n$ individuals, and let a variable $X: \mathcal{S} \rightarrow \mathcal{D}$ represent the mapping of individuals in $\mathcal{S}$ to values in the set $\mathcal{D}$, where $k$ is the cardinality of $\mathcal{D}$. The \emph{absolute frequency}\index{Absolute frequency}, also known simply as \emph{frequency}, denoted by $n_i$ for $1 \leq i \leq k$, quantifies the number of individuals in $\mathcal{S}$ for which $X$ assigns the value corresponding to the $i$-th category of $\mathcal{D}$.
\end{definition}

The sum of the frequencies must be equal to population size, that is, $\sum_{i=1}^k n_i = n$.

% Marginal Distribution

{\color{red} TODO: Marginal distributon}

La suma de las frecuencias marginales es igual al tamaño de la población $\sum_{i=1}^k n_i = n$.

Given a multivariate statistical vairable, the marginal distribution of a subset of variables is derived by summing the joint distribution over all possible values of the other variables. This process essentially "marginalizes" out the variables not of interest, allowing focus on the distribution of a single variable or a subset of variables within the multivariate context.

\begin{definition}
Let $\mathbf{X}$ be an $n$-dimensional statistical vector with a joint distribution $f_\mathbf{X}$. The \emph{marginal probability mass function}\index{Marginal probability mass function} of a subset of these variables, say $(X_1, X_2, \ldots, X_k)$ where $k \leq n$, is obtained by summing over the frequencies of the other variables $(X_{k+1}, \ldots, X_n)$. That is, for any subset of values $(x_1, x_2, \ldots, x_k)$ in the respective domains of $(X_1, X_2, \ldots, X_k)$,
\[
f_{X_1, X_2, \ldots, X_k}(x_1, x_2, \ldots, x_k) = \sum_{x_{k+1}} \sum_{x_{k+2}} \ldots \sum_{x_n} f_\mathbf{X}(x_1, x_2, \ldots, x_n)
\]
where $f_{X_1, X_2, \ldots, X_k}$ is the marginal probability mass function of the variables $(X_1, X_2, \ldots, X_k)$.
\end{definition}

This definition underscores the process of marginalization in a discrete setting, which is key to understanding and analyzing the behavior of specific variables within a larger multivariate framework.

\begin{example}
{\color{red} TODO: Provide an example based on random variables.}
\end{example}

While the marginal PMF of the random variables $X_1, \ldots, X_n$ can be obtained from their joint PMF by summing over the range of the other variables, the reverse process is not straightforward. Specifically, reconstructing the joint PMF of $X_1, \ldots, X_n$ from their marginal PMF alone is not feasible without extra information about the dependence between $X_1, \ldots, X_n$. This limitation arises because marginal PMF encapsulate only the individual behavior of each variable, omitting details about how the variables interact or are related.

\begin{example}
{\color{red} TODO: Provide an example based on random variables.}
\end{example}

A set of $n$ random variables $X_1, X_2, \ldots, X_n$ are considered independent if the occurrence of an event associated with any one of these variables does not influence the probability of an event associated with any other variable in the set. Independence among these variables indicates that there is no association or correlation among them, implying that knowing the outcome of one provides no information about the outcomes of the others.

\begin{definition}
A set of random variables $X_1, X_2, \ldots, X_n$ are said to be independent if, for every choice of sets $A_1, A_2, \ldots, A_n$ of real numbers such that each $\left\{ X_i \in A_i \right\}$ is an event, the joint probability of these events can be expressed as the product of their individual probabilities:
\[
P\left(X_1 \in A_1 \text{ and } X_2 \in A_2 \text{ and } \ldots \text{ and } X_n \in A_n\right) = P\left(X_1 \in A_1\right)P\left(X_2 \in A_2\right)\ldots P\left(X_n \in A_n\right)
\]
\end{definition}

The concept of independence for a random vector $\mathbf{X}$ simplifies the computation and understanding of joint probability distributions, particularly in complex problems involving multiple variables. It allows the joint probability distribution of the vector $\mathbf{X}$ to be expressed as the product of the individual marginal distributions of $X_1, X_2, \ldots, X_n$. 

\begin{proposition}
A set of random variables \(X_1, X_2, \ldots, X_n\) forming a random vector \(\mathbf{X}\) are independent if and only if the following is satisfied for all real numbers \(x_1, x_2, \ldots, x_n\): 
\[
f\left(x_1, x_2, \ldots, x_n\right) = f_1\left(x_1\right) f_2\left(x_2\right) \ldots f_n\left(x_n\right)
\]
where \(f\) is the joint probability mass function (PMF) or probability density function (PDF) of \(\mathbf{X}\), and \(f_i(x_i)\) is the marginal PMF or PDF of \(X_i\).
\end{proposition}

\begin{proof}
Assuming \(X_1, X_2, \ldots, X_n\) are independent, by definition, for any sets \(A_1, A_2, \ldots, A_n\) of real numbers, we have that 
\[
P(X_1 \in A_1 \text{ and } X_2 \in A_2 \text{ and } \ldots \text{ and } X_n \in A_n) = P(X_1 \in A_1)P(X_2 \in A_2)\ldots P(X_n \in A_n).
\]
Applying this definition to the PMFs or PDFs, for all \(x_1, x_2, \ldots, x_n\), the independence of \(X_1, X_2, \ldots, X_n\) implies 
\[
P(X_1 = x_1 \text{ and } X_2 = x_2 \text{ and } \ldots \text{ and } X_n = x_n) = P(X_1 = x_1)P(X_2 = x_2)\ldots P(X_n = x_n).
\]
Since \(P(X_1 = x_1 \text{ and } X_2 = x_2 \text{ and } \ldots \text{ and } X_n = x_n) = f(x_1, x_2, \ldots, x_n)\), and the marginal probabilities \(P(X_i = x_i)\) are given by \(f_i(x_i)\) respectively, we get 
\[
f(x_1, x_2, \ldots, x_n) = f_1(x_1) f_2(x_2) \ldots f_n(x_n).
\]

Conversely, assume that for all \(x_1, x_2, \ldots, x_n\), the joint PMF or PDF of \(\mathbf{X}\) can be expressed as 
\[
f(x_1, x_2, \ldots, x_n) = f_1(x_1) f_2(x_2) \ldots f_n(x_n),
\]
where \(f_i(x_i)\) are the marginal PMFs or PDFs of \(X_i\) respectively. We need to show that this implies \(X_1, X_2, \ldots, X_n\) are independent. The probability that \(X_1, X_2, \ldots, X_n\) simultaneously take on values \(x_1, x_2, \ldots, x_n\) is given by 
\[
P(X_1 = x_1 \text{ and } X_2 = x_2 \text{ and } \ldots \text{ and } X_n = x_n) = f(x_1, x_2, \ldots, x_n).
\]
Substituting the given condition into this expression yields 
\[
P(X_1 = x_1 \text{ and } X_2 = x_2 \text{ and } \ldots \text{ and } X_n = x_n) = f_1(x_1) f_2(x_2) \ldots f_n(x_n).
\]
By the definition of independence, if the joint probability of any \(x_1, x_2, \ldots, x_n\) equals the product of their individual probabilities for all possible values of \(x_1, x_2, \ldots, x_n\), then \(X_1, X_2, \ldots, X_n\) must be independent.
\end{proof}

\begin{example}
{\color{red} TODO: Provide an example based on random variables.}
\end{example}


{\color{red} TODO: Conditional distribution}


%
% Measures fo Statistical Relationship
%

\section{Measures of Statistical Relationship}
\label{sec:measures_statistical_relationship}

The metrics of disperson can also be used in case of bivariate distributions, under the names of covariance and correlation, to measure the \emph{statistical relationship} between two random variables.

Covariance and correlation are attempst to measure the linear dependence between to random variables.

Covarance

\begin{definition}
Definition 183. Let X and Y be random variables having finite means. Let $E\left(X\right)=\mu_{X}$ and $E\left(Y\right)=\mu_{Y}$. The covariance of X and Y, which is denoted by $Cov\left(X,Y\right)$ is defined as $Cov\left(X,Y\right)=E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right]$
\end{definition}

if the expectation exists.

The covariance between X and Y is intended to measre the degree to which X and Y tend to be large at the same time or the degree to which one tends to be large while the other is small.

\begin{proposition}
For all random variables X and Y such that $\sigma_{X}^{2}<\infty$ and $\sigma_{Y}^{2}<\infty Cov\left(X,Y\right)=E\left(XY\right)-E\left(X\right)E\left(Y\right)$
\end{proposition}
\begin{proof}
\end{proof}

Correlation

Correlation is a measure of association between two random variables that is not driven by arbitrary changes in the scales.

\begin{definition}
Let X and Y be random variables with finite variances $\sigma_{X}^{2}$ and $\sigma_{Y}^{2}$ respectively. Then the correlation of X and Y, which is denoted by $\rho\left(X,Y\right)$, is defined as follows $\rho\left(X,Y\right)=\frac{Cov\left(X,Y\right)}{\sigma_{X}\sigma_{Y}}$
\end{definition}

XX

\begin{definition}
It is said that X and Y are positively correlated if $\rho\left(X,Y\right)>0$, that X and Y are negatively correlated if $\rho\left(X,Y\right)<0$ and that X and Yare uncorrelated if $\rho\left(X,Y\right)=0$.
\end{definition}

Properties of Covariance and Correlation

\begin{proposition}
Moreover $-1 \leq \rho\left(X,Y\right) \leq 1$
\end{proposition}
\begin{proof}
\end{proof}

\begin{proposition}
If X and Y are independent random variables with $0<\text{\ensuremath{\sigma_{X}^{2}}}<\infty$ and $0<\text{\ensuremath{\sigma_{Y}^{2}}}<\infty$ then $Cov\left(X,Y\right)=\rho\left(X,Y\right)=0$
\end{proposition}
\begin{proof}
\end{proof}

The converse is not true as a general rule. Two dependent random variables can be uncorrelated.

\begin{proposition}
Suppose that X is a random variable such that $0<\sigma_{X}^{2}\infty$ and $Y=aX+b$ for some constants a and b, where $a\neq0$. If $a>0$ then $\rho\left(X,Y\right)=1$. If $a<0$, then $\rho\left(X,Y\right)=-1$.
\end{proposition}
\begin{proof}
\end{proof}

The converse is also true, that is, if $\left|\rho\left(X,Y\right)\right|=1$ implies that X and Y are linearly related.

\begin{proposition}
If X and Y are random variables such that $Var\left(X\right)<\infty$ and $Var\left(Y\right)<\infty$, then $Var\left(X+Y\right)=Var\left(X\right)+Var\left(Y\right)+2Cov\left(X,Y\right)$
\end{proposition}
\begin{proof}
\end{proof}

For all constants a and b, it can be shown that $Cov\left(aX,bY\right)=abCov\left(X,Y\right)$.

\begin{proposition}
Let X and Y are random variables such that $Var\left(X\right)<\infty$ and $Var\left(Y\right)<\infty$, and let a, b and c be constants, then $Var\left(aX+bY+c\right)=a^{2}Var\left(X\right)+b^{2}Var\left(Y\right)+2abCov\left(X,Y\right)$
\end{proposition}
\begin{proof}
\end{proof}

A special case is $Var\left(X-Y\right)=Var\left(X\right)+Var\left(Y\right)-2Cov\left(X,Y\right)$

\begin{proposition}
If $X_{1},\ldots,X_{n}$ are random variables such that $Var\left(X_{i}\right)<\infty for i=1,\ldots,n$, then $Var\left(\sum_{i=1}^{n}X_{i}\right)=\sum_{i=1}^{n}Var\left(X_{i}\right)+2\sum\sum Cov\left(X_{i},X_{j}\right)$
\end{proposition}
\begin{proof}
\end{proof}

\begin{proposition}
If $X_{1},\ldots,X_{n}$ are uncorrelated random variables, then $Var\left(\sum_{i=1}^{n}X_{i}\right)=\sum_{i=1}^{n}Var\left(X_{i}\right)$
\end{proposition}
\begin{proof}
\end{proof}


%
% Section: References
%
\section*{References}

{\color{red} TODO: Pending}

