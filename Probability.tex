%
% CHAPTER: Probability Theory
%

\chapterimage{Galton_box} % Chapter heading image

\chapter{Discrete Probability}
\label{chap:Probability Theory}

\begin{quote}
\begin{flushright}
\emph{The purpose of models is not to fit the data\\
but to sharpen the questions.}\\
Samuel Karlin
\end{flushright}
\end{quote}
\bigskip

Probability theory is the branch of mathematics that studies random experiments and random phenomena. Probability assigns a numerical description to all the possible outcomes of an experiment based on how likely these outcomes are to occur. Even if the outcome of an experiment cannot be determined in advance, we can study its properties with probability theory and draw relevant results and conclusions. For example, while we cannot predict the next number that will appear in a lottery game, probability theory can help us understand why it is not a good strategy to spend all our savings on lottery tickets with the goal of becoming rich.

The importance of probability theory extends far beyond games of chance. It provides the mathematical foundation for statistical inference, enabling us to draw conclusions from data, and for machine learning, where it underpins algorithms used for classification and forecasting based on large datasets. Additionally, probability theory plays a crucial role in fields such as finance, risk management, and the natural sciences, where understanding uncertainty and variability is essential.

In this chapter, we are going to focus on the area of discrete probability. In this version of the theory, the possible outcomes of an event are finite or, at most, countably infinite. We are interested in discrete probability, first because of its practical applications in the area of learning from data, and second, because discrete probability has some very interesting connections to theories used in this book: the length of optimal codes, the probability that a random machine will halt, and the derivation of universal distributions based on Kolmogorov complexity. All of these connections are relevant to our theory of nescience.

We will cover only the most important concepts and results of probability theory. The contents have been selected based on their applicability to the theory of nescience. For example, moment-generating functions are not covered. For a more comprehensive introduction to probability theory, see the References section at the end of the chapter.

We are going to study probability theory from a formal, axiomatic point of view. We will start by formulating a basic collection of fundamental axioms and then derive the major results and properties from them. Axioms are crucial in mathematical theory because they provide the foundation for constructing a consistent, universal, and rigorous framework. In probability theory, they allow us to define and manipulate the elusive concept of probability in a way that is both precise and broadly applicable.

%
% Section: Foundations of Probability Theory
%

\section{Foundations of Probability Theory}
\label{sec:probability_foundations}

The concept of \emph{probability} represents a profound intellectual challenge. Let us consider an instance where a dice is rolled and the objective is to compute the probability of an even number being the outcome. The dice comprises six distinct outcomes, and given that half of these are even numbers, we posit that the probability of yielding an even number is $3/6$ or equivalently $1/2$. This embodies the \emph{classical interpretation}\index{Classical interpretation of probability} of probability which asserts that in an experiment where all finite potential outcomes possess an equal likelihood of occurrence, the probability of an event equates to the count of favorable instances over the total number of instances. This interpretation, however, confronts the issue of circularity in its definition as "equally likely" essentially amounts to "possessing the same probability". An alternative method for probability assignment might be the implementation of the \emph{principle of indifference}\index{Principle of indifference}, which postulates that in the absence of any relevant evidence, all potential outcomes should possess identical probability. This principle, however, encounters a predicament when there exists evidence that contradicts the presumption of equivalence amongst all outcomes. For instance, how should probability be assigned when knowledge of a loaded dice suggests that not all sides possess an equal likelihood of occurrence?

The \emph{frequentist interpretation}\index{Frequentist interpretation of probability} of probability posits that one should roll the dice multiple times and contrast the frequency of even numbers with the total number of rolls. The fundamental notion is to execute the experiments repeatedly under similar conditions and assign the relative frequency of each outcome as its probability. This interpretation, however, faces two primary limitations. Firstly, the definition of repeating an experiment under "similar conditions" remains vague; if the conditions were truly identical, the results across all trials would invariably be the same. Secondly, the notion of a "large number of times" is undefined (technically, the experiment should be executed an infinite number of times). From a practical standpoint, implementing the frequentist interpretation is fraught with challenges. Some experiments, such as predicting the probability of a candidate winning an election, cannot be repeated multiple times. Furthermore, probability is defined in the context of a sequence of experiments, hence precluding the computation of the probability of a solitary outcome. Lastly, this interpretation necessitates the existence of a relative frequency limit, a condition which is not always satisfied, as illustrated by financial time series.

The \emph{subjective interpretation}\index{Subjective interpretation of probability}, representing a third approach to the concept of probability, suggests assigning probabilities to each event that reflect our degree of belief: the higher our conviction in the event's occurrence, the greater its assigned probability. However, it's important to note that not all potential probability allocations are viable; certain coherence rules need to be satisfied. For instance, when placing bets on the outcome of a dice roll, an assignment of probabilities that ensures a complete loss of money - a scenario known as a \emph{dutch book}\index{Dutch book} - would contravene the conditions of the subjective interpretation. It transpires that the conditions both necessary and sufficient to ensure a fair bet align with the axioms of probability introduced subsequently. Hence, we are free to assign any probabilities we desire to events, provided they remain consistent with the axioms of probability. A key drawback of the subjective interpretation is the inherent variability in individuals' degrees of belief. The \emph{Bayesian interpretation}\index{Bayesian interpretation of probability} of probability offers a solution: we commence with a provisional assignment of probabilities and, upon accruing further evidence, adjust our degree of belief or probability accordingly. With the accumulation of more evidence, estimated probabilities will converge to the true probabilities. Regardless, the task of assigning probabilities to an infinite number of events is typically unattainable for humans in general.

Currently, the notion of probability is defined axiomatically via the \emph{axiomatic interpretation}\index{Axiomatic interpretation of probability}. This implies that we abandon attempts to explicitly define the concept of probability and instead accept some of its properties as inherently true. Intuitively, a probability should be a value between $0$ and $1$, wherein an event with a zero probability is deemed impossible, and an event with a probability of one is certain to occur. Additional properties are required of probabilities. For instance, should two events $A$ and $B$ with probabilities $P \left( A \right)$ and $P \left( B \right)$ respectively, be disjoint, the probability of either $A$ or $B$ occurring should be $P \left( A \right) + P \left( B \right)$. If $A$ and $B$ could occur simultaneously and are independent (however that is defined), the probability of both events occurring concurrently should be $P \left( A \right) P \left( B \right)$. Moreover, the probability of $A$ occurring given that $B$ has already happened should be the fraction of the probability of $A$ that intersects with $B$. Anything that satisfies these properties could be considered a probability.

Probability theory is fundamentally concerned with the task of assigning a numerical value to specific events drawn from a sample space. The term "event" in this context may be somewhat counterintuitive, as it suggest the occurrence of something, which is not always applicable. For instance, consider the sample space of all possible outcomes when tossing a fair coin. A subset of this sample space could be the empty set, which represents no coin toss happening at all. In the conventional understanding of an "event", this scenario may confuse people who arenâ€™t familiar with the mathematical meaning, as it does not correspond to something "happening". Nonetheless, for the sake of clarity, we will continue to employ the term "events" to designate what essentially are subsets.

\begin{definition}
Given $\left( \Omega, \mathcal{A} \right)$ as a field over a non-empty discrete set, $\Omega$ is referred to as the \emph{sample space}\index{Sample space}, its constituents are termed \emph{outcomes}\index{Outcome}, and the components of $\mathcal{A}$ are referred to as \emph{events}\index{Event}. Specifically, $\Omega$ is designated the \emph{certain event}\index{Certain event}, while the empty set $\varnothing$ is deemed the \emph{impossible event}\index{Impossible event}.
\end{definition}

As previously discussed in Section \ref{sec:sets}, given that $\left( \Omega, \mathcal{A} \right)$ is a field, we can deduce that $\Omega \in \mathcal{A}$ and that $\varnothing \in \mathcal{A}$. Additionally, the union of a finite collection of events constitutes an event $A_1 \cup A_2 \cup \ldots \cup A_n \in \mathcal{A}$, and the intersection of a finite collection of events is likewise deemed an event $A_1 \cap A_2 \cap \ldots \cap A_n \in \mathcal{A}$.

As alluded to in the introduction of this chapter, our principal interest lies in discrete mathematics, and hence, we will largely focus on probabilities pertaining to discrete sets (be they finite or countably infinite). An extension of the concept of probability to continuous sets would necessitate the utilization of $\sigma$-algebras\index{$\sigma$-algebra} of sets instead of fields and the application of measure theory. For example, consider a scenario where we measure the electric current in a circuit, which can take any value between -5 and 5 volts. Unlike a discrete set of outcomes, such as specific whole numbers, here we have infinitely many possible values within this interval. In this case, it doesn't make sense to talk about the probability of the current being exactly 2.5 volts (or any other specific real number), as there are infinitely many possible outcomes. Instead, we would talk about the probability of the current falling within a certain range, such as between 2 and 3 volts.

The prevailing axiomatization utilized in the realm of probability theory is encapsulated within the framework of the \emph{Kolmogorov axioms}\footnote{In discrete probability theory, the sample space consists of a finite or countably infinite set of distinct outcomes, which means events are typically composed of individual, separable outcomes. Since probabilities are assigned directly to these discrete events, only finite unions of disjoint events need to be considered in Axiom 3 to cover all practical cases.}.

\begin{definition} (\emph{Kolmogorov's Axioms})\index{Kolmogorov's axioms}\label{Kolmogorov_axioms}
A \emph{probability}\index{Probability} is a real number $P(A) \in \mathbb{R}$ allocated to each event $A \in \mathcal{A}$ in the field $\left( \Omega, \mathcal{A} \right)$. This allocation adheres to the following axioms:

\medskip

\begin{description}
\item [Axiom 1] Each probability is nonnegative: $P(A) \geq 0$.
\item [Axiom 2] The probability of the certain event is one: $P(\Omega) = 1$.
\item [Axiom 3] For any finite sequence of disjoint events $A_1, A_2, \ldots, A_n$, the probability of the union of these events is the sum of their probabilities: $P(\cup_{i=1}^n A_i) = \sum_{i=1}^n P(A_i)$.
\end{description}

The triplet $\left( \Omega, \mathcal{A}, P \right)$ constitutes what is known as a \emph{probability space}\index{Probability space}.
\end{definition}

Despite their significance, the Kolmogorov axioms encounter certain complexities. While they provide the foundational principles that probabilities must conform to, such as non-negativity, normalization, and additivity, they do not offer explicit guidance on how to assign probabilities to specific events. Essentially, the axioms define the rules that probability must follow but leave the determination of those probabilities open. This stems from the fact that Kolmogorov's axioms are highly general and abstract, designed to apply to any measure-theoretic structure. This level of generality means they can accommodate a wide range of constructs beyond probability, such as normalized mass, volume, or other physical quantities. Finally, the connection between model theory (as discussed in Appendix \ref{apx:foundations_mathematics}) and probability theory through the Kolmogorov axioms is not straightforward. Model theory primarily deals with first-order logic, which can express basic properties like non-negativity and finite additivity of sets. However, when it comes to formally defining a continuous quantity, such as probability as a real number, it fails because first-order logic cannot fully capture the complexity of real numbers and continuous structures.

\begin{example}
\label{ex:discrete_sample_space}
Consider a sample space $\Omega$ composed of $n$ equally probable elements. If we have an event $A \subset \Omega$ comprised of $d(A) = m$ elements, then the probability of event $A$ can be represented as $P(A) = m/n$.
\end{example}

We now venture to establish certain fundamental theorems concerning probabilities, beginning with the calculation of the complement of an event, which represents the probability of an event not occurring.

\begin{proposition}
For each event $A$, it holds true that $P \left( A^{c} \right) = 1 - P \left( A \right)$.
\end{proposition}
\begin{proof}
Sets $A$ and $A^c$ are disjoint, and their union $A \cup A^c$ equals $\Omega$. By applying Axiom 3, we infer that $P \left( A \cup A^c \right) = P \left( A \right) + P \left( A^c \right)$, and by applying Axiom 2, we conclude that $P \left( A \cup A^c \right) = P(\Omega) = 1$. Therefore, we can assert that $P \left( A \right) + P \left( A^c \right) = 1$.
\end{proof}

As a direct consequence of the aforementioned proposition, we can deduce the probability of the impossible event.

\begin{proposition}
The probability of the impossible event equals zero, that is, $P \left( \varnothing \right) = 0$.
\end{proposition}
\begin{proof}
Since $P \left( \varnothing \right) = 1 - P \left( \Omega \right) = 0$.
\end{proof}

As anticipated, sub-events (subsets) are associated with smaller probabilities than their corresponding events.

\begin{proposition}
Given that $A\subset B$, it follows that $P \left( A \right) \leq P \left( B \right)$.
\end{proposition}
\begin{proof}
The event $B$ can be dissected into the union of two disjoint events $A$ and $A^c \cap B$. Consequently, $P \left( B \right) = P \left( A \right) + P \left( A^c \cap B \right)$, which combined with the notion that  $P \left( A^c \cap B \right) \geq 0$, substantiates the proposition.
\end{proof}

With these fundamental elements established, we can demonstrate that probabilities range between zero and one.

\begin{proposition}
For each event $A$, $0 \leq P \left( A \right) \leq 1$.
\end{proposition}
\begin{proof}
By virtue of Axiom 1 and the consideration that $A \subset \Omega$ and consequently $P \left( A \right) \leq P \left( \Omega \right) = 1$.
\end{proof}

Axiom 3 provides the means to compute the probability of the union of disjoint events, however, it does not extend to scenarios involving non-disjoint events. The succeeding proposition illustrates the method for computing the probability of the union of non-disjoint events.

\begin{proposition}
For any two events $A$ and $B$, it follows that $P\left(A\cup B\right)=Pr\left(A\right)+Pr\left(B\right)-Pr\left(A\cap B\right)$.
\end{proposition}
\begin{proof}
The union of sets $A$ and $B$ can be represented as the union of two disjoint sets $A \cup B = B \cup \left( A \cap B^c \right)$. Given Axiom 3, we ascertain that
\[
P \left( A \cup B \right) = P \left( B \right) + P \left( A \cap B^c \right)
\]
Similarly, the set $A$ can be deconstructed as the union of the disjoint sets $A = \left( A \cap B \right) \cup \left( A \cap B^c \right)$. As a result, we get
\[
P \left( A \cup B^c \right) = P \left( A \right) - P \left( A \cap B \right)
\]
The combination of both expressions yields the desired result.
\end{proof}

The following equation extends to the scenario of $n$ events $A_1, \ldots, A_n$, employing the principle of inclusion-exclusion\index{Inclusion-exclusion principle} (see Section \ref{sec:counting}):
\begin{equation*}
\begin{split}
P \left( \bigcup_{i=1}^n A_i \right) & = \sum_{i=1}^n P \left( A_i \right) - \sum_{i<j} P \left( A_i \cap A_j \right) + \sum_{i<j<k} P \left( A_i \cap A_j \cap A_k \right) - \\
&  - \sum_{i<j<k<l} P \left( A_i \cap A_j \cap A_k \cap A_l \right) + \ldots +  (-1)^{n+1} P \left( A_1 \cap A_2 \cap \ldots \cap A_n \right) 
\end{split}
\end{equation*}

A probability function is characterized as a function that assigns to every possible event within a sample space its corresponding probability.

\begin{definition}
\label{def:probability_function}
Suppose $\left( \Omega, \mathcal{A} , P \right)$ denotes a probability space. A \emph{probability function}\index{Probability function} is a real-valued function $f : \mathcal{A} \rightarrow [0, 1]$ such that for every $A \in \mathcal{A}$, $f \left( A \right) = P \left( A \right)$ holds true.
\end{definition}

In Example \ref{ex:discrete_sample_space}, we introduced a probability space $\left( \Omega, \mathcal{A}, P \right)$ comprising $n$ equally probable elements. The probability function associated with this experiment is defined as $f : \mathcal{A} \rightarrow [0, 1]$, such that $f \left( A \right) = d\left( A \right)/n$ for all $A \in \mathcal{A}$.

%
% Section: Conditional Probability
%

\section{Conditional Probability}
\label{sec:probability_conditional}

The principle of conditional probability is a cornerstone within the discipline of statistical learning. The conventional interpretation of conditional probability posits it as the recalibrated probability of event $A$ following the occurrence of event $B$. This perspective, however, potentially implies a sequential or even causative linkage between events $B$ and $A$, a suggestion which may not necessarily hold validity.

\begin{example}
\label{ex:concurrent_events}
Suppose we are playing a game with a standard deck of 52 cards, and we draw two cards. Let event A be "drawing at least one heart" and event B be "drawing at least one queen". These two events are dependent since the occurrence of event B affects the probability of event A. However, these two events are not temporally related because the draw of the card happens at the same time - one event does not occur before the other. This example showcases the essence of dependency in probability theory without any temporal association between the events involved.
\end{example}

With reference to the axiomatization prescribed by Kolmogorov, conditional probability is initially introduced as a definitive construct. Certain scholars posit that, given its pivotal role within probability theory, conditional probability ought to be an attribute that is logically deduced from the foundational axioms. This perspective, naturally, necessitates an augmentation of Definition \ref{Kolmogorov_axioms}\index{Kolmogorov's axioms} with supplementary properties. Regrettably, there exists no agreed-upon method among mathematicians and philosophers regarding the manner in which this augmentation should be conducted.

\begin{definition}
Let $A$ and $B$ be two events such that $P \left( B \right) \neq 0$. The \emph{conditional probability}\index{Conditional probability} of $A$ given $B$, symbolized as $P \left( A \mid B \right)$, is elucidated as follows:
\[
P\left(A\mid B\right) = \frac{P\left(A\cap B\right)}{P\left(B\right)}
\]
\end{definition}

By virtue of satisfying the axioms, a conditional probability is, in itself, a probability. The conditional probability $P\left(A\mid B\right)$ is undefined in instances where $P\left(B\right)=0$.

The probability of two events transpiring concurrently (although not necessarily contemporaneously, as previously discussed in Example \ref{ex:concurrent_events}), given their respective conditional probabilities, is encapsulated by the formula $P \left( A \cap B \right) = P \left( A \mid B \right) P \left( B \right)$. This equation offers perhaps a more intuitive comprehension of the conditional probability concept. Indeed, there exist a number of authors who advocate for this interpretation to form the basis of the definition of conditional probability, as opposed to the quotient method.

The extension of this formula to accommodate $n$ events, termed the \emph{multiplication rule}\index{Multiplication rule}, is expressed as follows:
\begin{equation}\label{eq:multiplication_rule}
P \left( A_{1} \cap A_{2} \cap \ldots \cap A_{n} \right) = P \left( A_{1} \right) P \left( A_{2} \mid A_{1}\right) \ldots  P \left( A_{n} \mid A_{1}\cap A_{2} \cap \ldots \cap A_{n-1} \right)
\end{equation}

The notion of event independence holds significant importance in the realm of probability theory and statistical learning. 

\begin{definition}\label{independent_events}\index{Independent events}
Two events $A$ and $B$ are declared to be \emph{independent} if $P \left( A \cap B \right) = P \left( A \right) P \left(B \right)$.
\end{definition}

From an intuitive perspective, the events $A$ and $B$ are considered independent if witnessing the occurrence of event B does not influence the probability of event A. This characteristic can be logically inferred from the definition of independence.

\begin{proposition}
Given two events $A$ and $B$ such that $P \left( A \right) > 0$ and $P \left( B \right)>0$, $A$ and $B$ are independent if and only if $P \left( A \mid B\right) = P \left( A \right)$ and $P \left( B \mid  A \right) = P \left( B \right)$.
\end{proposition}
\begin{proof}
Assume $A$ and $B$ are independent, implying that $P \left( A \cap B \right) = P \left( A \right) P \left(B \right)$. Then,
\[
P \left( A \mid B \right) = \frac{P\left(A\cap B\right)}{P\left(B\right)} = \frac{P \left( A \right) P \left(B \right)}{P\left(B\right)} = P \left( A \right)
\]
Proceeding from the assumption that $P \left( A \mid B \right) = P \left( A \right)$, and utilizing the multiplication rule, it follows that
\[
P \left( A \cap B \right) =  P \left( A \mid B \right) P \left( B \right) = P \left( A \right) P \left( B \right)
\] 
The same conclusion is drawn if the roles of $A$ and $B$ are interchanged.
\end{proof}

Similar to the case of conditional probability, certain authors posit that independence, as a foundational concept in probability theory, ought to be a logical extension of the axioms, rather than being imposed as a definition.

The principle of independence can be expanded to accommodate multiple events: the events $A_{1}, \ldots, A_{n}$ are deemed to be independent (or mutually independent) if for every subset $A_{i_1}, \ldots, A_{i_j}$ comprising $j$ events $\left( j = 2, 3, \ldots, n \right)$, it holds true that $P \left( A_{i_1} \cap \ldots \cap A_{i_j} \right) = P \left( A_{i_1} \right) \ldots P \left( A_{i_j}\right)$.

\begin{example}
A degree of confusion often arises regarding the distinction between mutually exclusive (or disjoint) events and independent events. For two mutually exclusive events $A$ and $B$, the computation of the probability that $A$ will transpire given $B$ is somewhat nonsensical, since if $B$ occurs, $A$ is inherently impossible; analogously, discussing the conditional probability that $A$ will occur given $B$ when the probability of $B$ is zero is likewise flawed. However, as Definition \ref{independent_events} does not explicitly exclude the instance of $A$ and $B$ being mutually exclusive, we are compelled to conclude that two mutually exclusive events are independent if, and only if, the probability of at least one (or both) of them is zero.
\end{example}

An intriguing scenario arises when events $A$ and $B$ are not independent, yet attain independence contingent on the occurrence of another event $C$.

\begin{definition}
Consider $A$, $B$ and $C$ as events such that $P\left( B \cap C \right)>0$. $A$ and $B$ are considered \emph{conditionally independent}\index{Conditionally independent} given $C$ if $P\left(A \mid B \cap C \right) = P\left( A \mid C \right)$.
\end{definition}

\begin{example}
Consider the act of rolling two dice; it is reasonable to assert that the outcomes of the two dice are independent from each other. That is, observing the outcome of one die provides no insight into the outcome of the other die. However, suppose the first die results in a four, and a third event is introduced - that the sum of the outcomes is an odd number - then this additional piece of information narrows the potential outcomes for the second die to only odd numbers. This illustrates the point that two events can be independent, yet fail to maintain conditional independence.
\end{example}

The ensuing theorem presents Bayes' rule, a fundamental principle underpinning a significant statistical learning technique known as Bayesian inference (see Section \ref{sec:bayesian_inference}). 

\begin{theorem} (Bayes' Theorem)\index{Bayes' theorem} Let $A$ and $B$ represent two events with the condition that $P\left( B \right) \neq 0$. Consequently, we obtain that
\[
P \left( A \mid B \right) = \frac{P \left( B \mid A \right) P \left( A \right)}{P \left( B \right)}
\]
In this context, $P\left( A \right)$ is referred to as the \emph{prior probability}\index{Prior probability}, while $P\left( A \mid B \right)$ is deemed the \emph{posterior probability}\index{Posterior probability}.
\end{theorem}
\begin{proof}
As per the definition of conditional probability, $P \left( A \mid B \right) = P \left( A \cap B \right) / P \left( B \right)$ (given $P \left( B \right) \neq 0$) and $P \left( B \mid A \right) = P \left( A \cap B \right) / P \left( A \right)$ (provided $P \left( A \right) \neq 0$). By solving for $P(A\cap B)$ and substituting into the previous expressions for $P(A\mid B)$, we arrive at the theorem.
\end{proof}

As evident in the proof, Bayes' theorem is a direct derivative of the definition of conditional probability, notwithstanding our misgivings about conditional probability being a definition.

According to Bayesian inference, it facilitates the computation of how our degree of certainty about event $A$ (the prior probability $P\left( A \right)$) evolves when we acquire supplementary evidence via the occurrence of event $B$ (transforming into the posterior probability $P\left( A \mid B \right)$).

\begin{example}
Consider $E$ to be a disease affecting one in every million people, $P(E) = 1 \times 10^{-6}$, and let $+$ represent a test devised to detect the disease, with a failure rate of one in every thousand applications, $P(+ \mid E) = 999/1000$. We aim to determine the probability of disease presence if the test is positive $P(E \mid +)$. Upon employing Bayes' theorem, we find that:
\[
P(E \mid +) = \frac{P(+ \mid E) P(E)}{P(+)} = \frac{P(+ \mid E) P(E)}{P(+ \mid E) P(E) + P(+ \mid E^c) P(E^c)} = 0.001
\]
This implies that despite the test only failing once per thousand applications, it remains highly improbable that we have the disease following a positive result. This paradoxical outcome can be attributed to the higher probability of test failure $10^{-3}$ compared to the likelihood of disease occurrence $10^{-6}$. Practically, this issue is circumvented by applying a second test to individuals who received a positive result, as the probability of disease presence following two positive results is $0.5$ (under the assumption that the successive test repetitions are independent).
\end{example}

Bayes' theorem is most useful when the events involved are dependent and when new information about one event can update our understanding of the other event's probability.

\begin{example}
Suppose you're drawing a single card from a standard deck of 52 playing cards. Let Event $A$ be "drawing a red card" and Event $B$ be "drawing a queen". In this context, the use of Bayes' theorem to compute $P(A \mid B)$, the probability of drawing a red card given that a queen has been drawn, would not yield a meaningful result because the event $B$ provides no new information that would affect the probability of event $A$.
\end{example}

Bayes' theorem can indeed be extended to accommodate multiple events. Consider a set of events $A_{1}, \ldots, A_{k}$ such that $P\left( A_{j} \right)>0$ for all $j$ in the range of 1 to $k$. Suppose these events constitute a partition of the sample space $\Omega$. Now, let $B$ denote an event with the property that $P\left(B\right)>0$. In such a context, it can be deduced for each $i$ in the range of 1 to $k$ that the conditional probability $P\left(A_{i}\mid B\right)$ is given by the formula
\[
P\left(A_{i}\mid B\right)=\frac{P\left(B\mid A_{i}\right) P\left(A_{i}\right)}{\sum_{j=1}^{k} P\left(B \mid A_{j}\right) P\left(A_{j}\right)}
\]
This illustrates the capacity of Bayes' theorem to apply to a broader set of scenarios involving multiple events.

%
% Section: Random Variables
%

\section{Random Variables}
\label{sec:probability_random_variables}

A random variable is a function that assigns a real number to each possible outcome of an experiment. Therefore, it serves as a quantitative representation of the results of the experiment. Random variables are very useful since they offer a quantifiable means to examine the outcomes of the experiments and to discover their analytical properties. Such is the efficacy of random variables that a majority of statisticians primarily consider their investigations within the framework of random variables as opposed to probability spaces.

\begin{definition}
Let $\left( \Omega, \mathcal{A} , P \right)$ be a discrete probability space. A \emph{discrete random variable}\index{Discrete random variable}\index{Random variable} is a function $X : \Omega \rightarrow \mathbb{R}$ mapping from the sample space $\Omega$ to the real numbers.
\end{definition}

The terminology "random variable" might potentially lead to some confusion. Firstly, these are not variables in the conventional algebraic sense, but rather, they are functions. Secondly, they are not inherently random; it is the experiment that they represent which possesses randomness. Despite these points of potential confusion, we adhere to the established terminology.

Random variables hold increased utility when they encapsulate properties of the experiment. For instance, if the sample space consists of a school's student body, a random variable could associate each student with their respective height. Random variables also enable us to redistribute elements of the sample space into new events. For example, if two dice are rolled, a random variable could represent the sum of the dice's outcomes.

It is crucial to remember that we possess the liberty to assign a random variable to any sample space, even when the assignment might not seem intuitively meaningful. As an illustration, one could assign a numerical value to each possible color in a deck of cards, draw two cards randomly, and sum the assigned numbers of these two cards. Although such a setup may not yield a significant interpretation, it is nonetheless possible to calculate an array of probabilities based on this setup.

\begin{definition}
Let $X : \Omega \rightarrow \mathbb{R}$ be a discrete random variable, and let $\{ x_1, x_2, \ldots, x_i, \ldots \}$ be the range of $X$. The probability of $X$ being equal to $x_i$, expressed as $P\left(X = x_i \right)$, is given by $P\left( X = x_i \right)=P \left( \left\{ \omega \in \Omega \,:\, X \left( \omega \right) = x_i\right\} \right)$. Let $C \subset \mathbb{R}$ be a subset such that the set $\{ \omega \in \Omega \,:\, X \left( \omega \right) \in C\}$ constitutes an event. The probability of $X$ belonging to $C$, expressed as $P\left(X \in C \right)$, is given by $P\left( X \in C \right)=P \left( \left\{ \omega \in \Omega \,:\, X \left( \omega \right) \in C\right\} \right)$.
\end{definition}

The probability of a random variable $X$ essentially configures a probability space over the line of real numbers, specifically over the range of $X$. 

\begin{example}
\label{ex:probability_distribution_real_line}
Let $\Omega = \{1, 2, 3, 4, 5, 6\}$ be the sample space of possible outcomes when tossing a die, $\mathcal{A} = \mathcal{P} \left( \Omega \right)$ be all possible events of $\Omega$, and $P$ a probability that assigns $1/6$ to each single outcome in $\Omega$. Let $X: \Omega \rightarrow \mathbb{R}$ be a random variable defined as:
    \[
    X(\omega) = 
    \begin{cases} 
      0 & \text{if } \omega \text{ is even (2, 4, 6)}, \\
      1 & \text{if } \omega \text{ is odd (1, 3, 5)}.
    \end{cases}
    \]
This random variable maps the outcomes of the die toss to either 0 (if the outcome is even) or 1 (if the outcome is odd). For $C = \{0\}$, the probability $P(X \in C) = P(X = 0) = P(\{2, 4, 6\}) = 1/2$. For $C = \{1\}$ the probability $P(X \in C) = P(X = 1) = P(\{1, 3, 5\}) = 1/2$. Through this transformation, the original probability space has been mapped to the real numbers using the random variable $X$, establishing a new probability over $X$'s range.
\end{example}

Definition \ref{def:probability_function} introduced the concept of probability function for discrete probability spaces based on the probabilities of the events. Next definition extends the concept of probability function to discrete random variables.

\begin{definition}
Let $X$ be a random variable over a discrete probability space, and let $\{ x_1, x_2, \ldots \}$ be the range of $X$. The \emph{probability function}\index{Probability function} of the random variable $X$, abbreviated as p.f., is defined as the function $f : range \left( X \right) \rightarrow [0, 1]$ such that $f \left( x_i \right) = P \left( X = x_i \right)$.
\end{definition}

Of course, the probability function is defined only if $\{ \omega \in \Omega \,:\, X \left( \omega \right) = x_i\}$ is an event. Unless we say the contrary, and since we are dealing mostly with discrete probability spaces, we will assume that $\{ \omega \in \Omega \,:\, X \left( \omega \right) = x_i\}$ is an event for all the points that compose the range of $X$. The set of points for which the probability function is greater than zero, that is $\left\{ x \, : \, f \left( x \right) > 0 \right\}$, is called the \emph{support} of the distribution of $X$.

It is possible for two random variables to have identical probability functions but to differ in significant ways.

\begin{example}
Let $\Omega = \{H, T\}$ be the sample space of possible outcomes when tossing a coin, and $P$ a probability that assigns $1/2$ to each single outcome in $\Omega$. Let $X: \Omega \rightarrow \mathbb{R}$ be a random variable defined as: $X$ such that $X = 1$ if the coin shows Head and $X = 0$ if coin shows Tail. The individual probability distributions of $X$ is $P(X = 1) = P(Y = 1) = 0.5$. The random variables of this example and Example \ref{ex:probability_distribution_real_line} have same probability distribution even if they are different random variables.
\end{example}

Given the probability function of a random variable, we can derive the probabilty of any subset of the real line.

\begin{proposition}
Let $X$ be a discrete random variable with probability function $f$. The probability of each subset $C$ of the real line can be determined from the relation $P\left(X\in C\right)=\sum_{x_{i}\in C}f\left(x_{i}\right)$
\end{proposition}
\begin{proof}
Considering that each outcome in the sample space is associated with exactly one value in the range $\{ x_1, x_2, \ldots, x_i, \ldots \}$ of $X$, we have that:
\[
P\left(X\in C\right) = P \left( \left\{ \omega \in \Omega \,:\, X \left( \omega \right) \in C\right\} \right) = \sum_{x_{i}\in C} P\left( X = x_i \right) = \sum_{x_{i}\in C}f\left(x_{i}\right)
\]
\end{proof}

Next proposition outlines a fundamental property, that the total sum of probabilities across all possible outcomes of a discrete random variable is 1.

\begin{proposition}
Let $X$ be a discrete random variable with probability function $f$. If $\{ x_1, x_2, \ldots, \}$ is the range of $X$, then $\sum_{i=1}^{\infty}f\left(x_{i}\right)=1$.
\end{proposition}
\begin{proof}
Considering that $X$ is a total function, that each outcome in the sample space is associated with exactly one value in the range $\{ x_1, x_2, \ldots \}$, and given the axiomatic definition of probability we have that:
\begin{equation*}
\begin{split}
\sum_{i=1}^{\infty} f(x_i) & = f(x_1) + f(x_2) + \ldots = P\left( X = x_1 \right) + P\left( X = x_2 \right) + \ldots = \\
&  = P \left( \left\{ \omega \in \Omega \,:\, X \left( \omega \right) = x_1 \right\} \right) + P \left( \left\{ \omega \in \Omega \,:\, X \left( \omega \right) = x_2 \right\} \right) + \ldots = 1 
\end{split}
\end{equation*}
\end{proof}

The cumulative distribution function represents the probability that a random variable takes on a value less than or equal to a specific point.

\begin{definition}
The \emph{cumulative distribution function} (abbreviated c.d.f.) $F$ of a random variable $X$ is the function $F(x)=Pr(X\leq x)$ for all $-\infty < x < \infty$
\end{definition}

If $X$ follows a discrete distribution characterized by the probability function $f(x)$, its cumulative distribution function $F(x)$ will exhibit the following behavior: at each distinct value $x_i$ of $X$, $F(x)$ will display a jump equal to $f(x_i)$; between these distinct values, $F(x)$ remains unchanged.

The cumulative distribution function allows us to see how probabilities accumulate over the range of a random variable, offering insights into the overall distribution of the data.

\begin{example}
Let's $X$ be a discrete random variable that represents the grades of students in a class. Each grade is between 0 and 10. The probability that a student receives a grade of $x$ is given by the probability mass function $p(x)$. The cumulative distribution function represents the probability that a randomly selected student scores $x$ or less. For example, a value of $F(7) =
0.6$ would mean that there's a 60\% chance a student picked at random scored 7 or below.
\end{example}

The cumulative distribution function of a random variable is non-decreasing. 

\begin{proposition}
Let $F(X)$ be the cumulative distribution function of a random variable $X$. Then, if $x_{1}<x_{2}$ we have that $F\left(x_{1}\right)\leq F\left(x_{2}\right)$.
\end{proposition}
\begin{proof}
Given two values $x_1$ and $x_2$ where $x_1 < x_2$, the set of outcomes where $X \leq x_1$ is a subset of the outcomes where $X \leq x_2$.  Therefore, the probability of $X$ taking a value less than or equal to $x_1$ will be less than or equal to the probability of $X$ taking a value less than or equal to $x_2$. Then $F(x_1) \leq F(x_2)$.
\end{proof}

Next proposition delineates the asymptotic properties of the cumulative distribution function of a random variable, showcasing its bounds as we approach negative and positive infinity.

\begin{proposition}
Let $F(X)$ be the cumulative distribution function of a random variable $X$. Then, we have that $\lim_{x\rightarrow-\infty}F\left(x\right)=0$ and that $\lim_{x\rightarrow\infty}F\left(x\right)=1$.
\end{proposition}
\begin{proof}
As $x$ tends to negative infinity, the probability that the random variable $X$ takes on a value less than or equal to this increasingly smaller $x$ tends to zero. This is because there are fewer and fewer values (or none, depending on the specifics of the distribution) that $X$ can assume which are less than this increasingly negative $x$. Therefore:
\[
\lim_{x \rightarrow -\infty} F(x) = \lim_{x \rightarrow -\infty} P(X \leq x) = 0 
\]
As $x$ tends to positive infinity, the probability that the random variable $X$ takes on a value less than or equal to this increasingly larger $x$ approaches 1. This is because, given the unbounded increase of $x$, it encapsulates all possible values that $X$ can take on. Therefore:
\[
\lim_{x \rightarrow \infty} F(x) = \lim_{x \rightarrow \infty} P(X \leq x) = 1
\]
\end{proof}

The probability of $X$ exceeding $x$ is given by the complement of the cumulative distribution function at that point.

\begin{proposition}
Let $F(X)$ be the cumulative distribution function of a random variable $X$. Then, for every $x \in X$ we have that $P\left(X>x\right)=1-F\left(x\right)$.
\end{proposition}
\begin{proof}
The probability that $X$ takes on a value greater than $x$ plus the probability that $X$ takes on a value less than or equal to $x$ should sum up to 1. Given this, the probability that $X$ takes a value greater than $x$ is:
\[
P(X > x) = 1 - P(X \leq x)
\]
Using the definition of the cumulative distribution function, we get:
\[
P(X > x) = 1 - F(x) 
\]
\end{proof}

The following proposition establishes a relationship between the probabilities of a random variable \( X \) falling between two specific values and the corresponding differences in its cumulative distribution function values at those points.

\begin{proposition}
Let $F(X)$ be the cumulative distribution function of a random variable $X$. Then, for all values $x_1, x_2 \in X$ such that $x_1 < x_2$ we have that $P\left(x_1 < X \leq x_2 \right) = F\left(x_2\right) - F\left(x_1\right)$
\end{proposition}
 \begin{proof}
The probability that $X$ is less than or equal to $x_2$ is $F(x_2)$. From this, if we subtract the probability that $X$ is less than or equal to $x_1$, which is $F(x_1)$, we'll get the probability that $X$ falls strictly between $x_1$ and $x_2$:
\[
P(x_1 < X \leq x_2) = F(x_2) - F(x_1)
\]
\end{proof}

% Multivariate Distributions

\subsection*{Multivariate Distributions}

A multivariate probability distribution extends the concept of a probability distribution across multiple random variables, each with its own set of possible outcomes. Unlike univariate distributions that describe phenomena with a single random variable, multivariate distributions capture the relationships and dependencies between two or more variables. This allows for the exploration of complex phenomena where the outcome of interest is influenced by multiple factors simultaneously, providing insights into how these variables interact and impact the probability of various outcomes.

\begin{definition}
Let $X_1, X_2, \ldots, X_n$ be $n$ random variables, where $X_i : \Omega_i \rightarrow \mathbb{R}$ for all $i=1, \ldots, n$. The \emph{joint probability distribution}\index{Joint probability distribution} of $X_1, X_2, \ldots, X_n$ is defined as the collection of all probabilities of the form $P\left( \left( X_1, X_2, \ldots, X_n \right) \in C \right)$ for all sets $C \subset \mathbb{R}^n$ of real numbers such that $\left\{ \left( \omega_1, \omega_2, \ldots, \omega_n \right) \in \Omega_1 \times \ldots \times \Omega_n : \left( X_1 \left( \omega_1 \right), X_2 \left( \omega_2 \right), \ldots, X_n \left( \omega_n \right) \right) \in C \right\}$ is an event.
\end{definition}

The joint probability distribution of the random variables $X_1, X_2, \ldots, X_n$ defines a probability space in $\mathbb{R}^n$. If the random variables $X_1, X_2, \ldots, X_n$ each have a discrete distribution, then the joint distribution is also a discrete distribution.

\begin{definition}
Let $X_1, X_2, \ldots, X_n$ be $n$ random variables over discrete probability spaces. The \emph{joint probability mass function}\index{Joint probability mass function} of the random variables $X_1, X_2, \ldots, X_n$ is defined as the function $f : \text{range} \left( X_1 \right) \times \ldots \times \text{range} \left( X_n \right) \rightarrow [0, 1]$ such that $f \left( x_1, \ldots, x_n \right) = P \left( X_1 = x_1, \ldots, X_n = x_n \right)$.
\end{definition}

\begin{example}
A classic example of a bivariate discrete joint distribution involves rolling two six-sided dice. Let's define two random variables: $X_1$ is the outcome of the first die, and $X_2$ is the outcome of the second die. Both $X_1$ and $X_2$ have a discrete uniform distribution over the set $\{1, 2, 3, 4, 5, 6\}$. The joint distribution of $X_1$ and $X_2$ describes the probability of each possible pair of outcomes when the two dice are rolled. The joint probability mass function $f(x_1, x_2)$ for $X_1$ and $X_2$ can be expressed as:
\[
f(x_1, x_2) = P(X_1 = x_1, X_2 = x_2) = \frac{1}{36}, \quad \text{for} \ x_1, x_2 \in \{1, 2, 3, 4, 5, 6\}
\]
The joint distribution allows us to analyze the relationship between $X_1$ and $X_2$. For instance, we can compute the probability that the sum of the two dice is equal to a certain number, or that one die shows a higher number than the other.
\end{example}

Before exploring the properties of multivariate random variables, we will introduce the concept of a random vector. This concept simplifies notation and enhances clarity by grouping multiple random variables into a single entity.

\begin{definition}
A \emph{random vector}\index{Random vector} $\mathbf{X}$ is an ordered collection of $n$ random variables $X_1, X_2, \ldots, X_n$, where $X_i : \Omega_i \rightarrow \mathbb{R}$ for all $i=1, \ldots, n$. If the $n$ random variables are discrete, the random vector is called discrete random vector.
\end{definition}

Given the joint probability function of a vector of random variables, we can derive the probability of any subset within the multidimensional real space.

\begin{proposition}
Let $\mathbf{X}=\left(X_{1}, \ldots, X_{n}\right)$ be a discrete random vector with a joint probability function $f$. The probability of each subset $C$ of the $n$-dimensional real space can be determined from the relation $P\left(\mathbf{X} \in C\right)=\sum_{\mathbf{x} \in C}f\left(\mathbf{x}\right)$, where $\mathbf{x} = (x_{1}, \ldots, x_{n})$.
\end{proposition}
\begin{proof}
Considering that each outcome in the sample space is associated with exactly one value in the range of $\mathbf{X}$, we have that:
\[
P\left(\mathbf{X} \in C\right) = P \left( \left\{ \omega \in \Omega \,:\, \mathbf{X}(\omega) \in C\right\} \right) = \sum_{\mathbf{x}\in C} P\left( \mathbf{X} = \mathbf{x} \right) = \sum_{\mathbf{x}\in C}f\left(\mathbf{x}\right)
\]
\end{proof}

The next proposition outlines a fundamental property, that the total sum of probabilities across all possible outcomes of a vector of discrete random variables is 1.

\begin{proposition}
Let $\mathbf{X}=\left(X_{1}, \ldots, X_{n}\right)$ be a discrete random vector with a joint probability function $f$. If the range of $\mathbf{X}$ is represented as a set of vectors $\mathbf{x} = (x_{1}, \ldots, x_{n})$, then $\sum_{\mathbf{x}}f\left(\mathbf{x}\right)=1$.
\end{proposition}
\begin{proof}
Considering that $\mathbf{X}$ is a total function, that each outcome in the sample space is associated with exactly one value in the range of $\mathbf{X}$, and given the axiomatic definition of probability we have that:
\[
\sum_{\mathbf{x}} f(\mathbf{x}) = \sum_{\mathbf{x}} P\left( \mathbf{X} = \mathbf{x} \right) = P \left( \left\{ \omega \in \Omega_1 \,:\, \mathbf{X}_1(\omega) = \mathbf{x_1} \right\} \right) + P \left( \left\{ \omega \in \Omega_2 \,:\, \mathbf{X}_2(\omega) = \mathbf{x_2} \right\} \right) + \ldots = 1 
\]
\end{proof}

A particular interesting case of multivariate distribution is given by the sum of $n$ random variables. This is a highly confusing scenario, since we are not adding $n$ probability distributions, as the notation $X_1 + \ldots + X_n$ might suggest. Instead, we are defining a new random variable over the cartesian product of the original sample spaces.

\begin{definition}
Let $X_1, X_2, \ldots, X_n$ be $n$ random variables, where $X_i : \Omega_i \rightarrow \mathbb{R}$ for all $i=1, \ldots, n$ The sum distribution of $X_1, X_2, \ldots, X_n$, denoted by $X_1 + \ldots + X_n$, is defined as the random variable $X + \ldots + X_n : \Omega_1 \times \ldots \times \Omega_n \rightarrow \mathbb{R}$ that assigns to each $\left( \omega_1, \omega_2, \ldots, \omega_n \right) \in \Omega_1 \times \ldots \Omega_n$ the number $X_1 \left( \omega_1 \right) + X_2 \left( \omega_2 \right) + \ldots + X_n \left( \omega_n \right)$.
\end{definition}

The concept of sum of $n$ randon variables will be applied in the law of large numbers (see Theorem \ref{th:law_large_numbers}), one of the most important theorems in probability theory.

\begin{example}
Let $X: \Omega \rightarrow \mathbb{R}$ be the random variable representing the outcome of rolling a six-sided dice. The sum distribution corresponding to rolling three times a dice, denoted by $S = X + X + X$, is defined as the random variable $S: \Omega \times \Omega \times \Omega \rightarrow \mathbb{R}$ that assigns to each $\left( \omega_1, \omega_2, \omega_3 \right) \in \Omega \times \Omega \times \Omega$ the number $X \left( \omega_1 \right) + X \left( \omega_2 \right) + X \left( \omega_3 \right)$.
\end{example}

% Marginal Distribution

\subsection*{Marginal Probability Mass Function}

Given a multivariate discrete PMF, the marginal PMF of a subset of variables is derived by summing the joint PMF over all possible values of the other variables. This process essentially "marginalizes" out the variables not of interest, allowing focus on the PMF of a single variable or a subset of variables within the multivariate context.

\begin{definition}
Let $\mathbf{X}$ be an $n$-dimensional discrete random vector with a joint probability mass function $f_\mathbf{X}$. The \emph{marginal probability mass function}\index{Marginal probability mass function} of a subset of these variables, say $(X_1, X_2, \ldots, X_k)$ where $k \leq n$, is obtained by summing over the probabilities of the other variables $(X_{k+1}, \ldots, X_n)$. That is, for any subset of values $(x_1, x_2, \ldots, x_k)$ in the respective domains of $(X_1, X_2, \ldots, X_k)$,
\[
f_{X_1, X_2, \ldots, X_k}(x_1, x_2, \ldots, x_k) = \sum_{x_{k+1}} \sum_{x_{k+2}} \ldots \sum_{x_n} f_\mathbf{X}(x_1, x_2, \ldots, x_n)
\]
where $f_{X_1, X_2, \ldots, X_k}$ is the marginal probability mass function of the variables $(X_1, X_2, \ldots, X_k)$.
\end{definition}

This definition underscores the process of marginalization in a discrete setting, which is key to understanding and analyzing the behavior of specific variables within a larger multivariate framework.

\begin{example}
{\color{red} TODO: Provide an example based on random variables.}
\end{example}

While the marginal PMF of the random variables $X_1, \ldots, X_n$ can be obtained from their joint PMF by summing over the range of the other variables, the reverse process is not straightforward. Specifically, reconstructing the joint PMF of $X_1, \ldots, X_n$ from their marginal PMF alone is not feasible without extra information about the dependence between $X_1, \ldots, X_n$. This limitation arises because marginal PMF encapsulate only the individual behavior of each variable, omitting details about how the variables interact or are related.

\begin{example}
{\color{red} TODO: Provide an example based on random variables.}
\end{example}

A set of $n$ random variables $X_1, X_2, \ldots, X_n$ are considered independent if the occurrence of an event associated with any one of these variables does not influence the probability of an event associated with any other variable in the set. Independence among these variables indicates that there is no association or correlation among them, implying that knowing the outcome of one provides no information about the outcomes of the others.

\begin{definition}
A set of random variables $X_1, X_2, \ldots, X_n$ are said to be independent if, for every choice of sets $A_1, A_2, \ldots, A_n$ of real numbers such that each $\left\{ X_i \in A_i \right\}$ is an event, the joint probability of these events can be expressed as the product of their individual probabilities:
\[
P\left(X_1 \in A_1 \text{ and } X_2 \in A_2 \text{ and } \ldots \text{ and } X_n \in A_n\right) = P\left(X_1 \in A_1\right)P\left(X_2 \in A_2\right)\ldots P\left(X_n \in A_n\right)
\]
\end{definition}

The concept of independence for a random vector $\mathbf{X}$ simplifies the computation and understanding of joint probability distributions, particularly in complex problems involving multiple variables. It allows the joint probability distribution of the vector $\mathbf{X}$ to be expressed as the product of the individual marginal distributions of $X_1, X_2, \ldots, X_n$. 

\begin{proposition}
A set of random variables \(X_1, X_2, \ldots, X_n\) forming a random vector \(\mathbf{X}\) are independent if and only if the following is satisfied for all real numbers \(x_1, x_2, \ldots, x_n\): 
\[
f\left(x_1, x_2, \ldots, x_n\right) = f_1\left(x_1\right) f_2\left(x_2\right) \ldots f_n\left(x_n\right)
\]
where \(f\) is the joint probability mass function (PMF) or probability density function (PDF) of \(\mathbf{X}\), and \(f_i(x_i)\) is the marginal PMF or PDF of \(X_i\).
\end{proposition}

\begin{proof}
Assuming \(X_1, X_2, \ldots, X_n\) are independent, by definition, for any sets \(A_1, A_2, \ldots, A_n\) of real numbers, we have that 
\[
P(X_1 \in A_1 \text{ and } X_2 \in A_2 \text{ and } \ldots \text{ and } X_n \in A_n) = P(X_1 \in A_1)P(X_2 \in A_2)\ldots P(X_n \in A_n).
\]
Applying this definition to the PMFs or PDFs, for all \(x_1, x_2, \ldots, x_n\), the independence of \(X_1, X_2, \ldots, X_n\) implies 
\[
P(X_1 = x_1 \text{ and } X_2 = x_2 \text{ and } \ldots \text{ and } X_n = x_n) = P(X_1 = x_1)P(X_2 = x_2)\ldots P(X_n = x_n).
\]
Since \(P(X_1 = x_1 \text{ and } X_2 = x_2 \text{ and } \ldots \text{ and } X_n = x_n) = f(x_1, x_2, \ldots, x_n)\), and the marginal probabilities \(P(X_i = x_i)\) are given by \(f_i(x_i)\) respectively, we get 
\[
f(x_1, x_2, \ldots, x_n) = f_1(x_1) f_2(x_2) \ldots f_n(x_n).
\]

Conversely, assume that for all \(x_1, x_2, \ldots, x_n\), the joint PMF or PDF of \(\mathbf{X}\) can be expressed as 
\[
f(x_1, x_2, \ldots, x_n) = f_1(x_1) f_2(x_2) \ldots f_n(x_n),
\]
where \(f_i(x_i)\) are the marginal PMFs or PDFs of \(X_i\) respectively. We need to show that this implies \(X_1, X_2, \ldots, X_n\) are independent. The probability that \(X_1, X_2, \ldots, X_n\) simultaneously take on values \(x_1, x_2, \ldots, x_n\) is given by 
\[
P(X_1 = x_1 \text{ and } X_2 = x_2 \text{ and } \ldots \text{ and } X_n = x_n) = f(x_1, x_2, \ldots, x_n).
\]
Substituting the given condition into this expression yields 
\[
P(X_1 = x_1 \text{ and } X_2 = x_2 \text{ and } \ldots \text{ and } X_n = x_n) = f_1(x_1) f_2(x_2) \ldots f_n(x_n).
\]
By the definition of independence, if the joint probability of any \(x_1, x_2, \ldots, x_n\) equals the product of their individual probabilities for all possible values of \(x_1, x_2, \ldots, x_n\), then \(X_1, X_2, \ldots, X_n\) must be independent.
\end{proof}

\begin{example}
{\color{red} TODO: Provide an example based on random variables.}
\end{example}

% Conditional Distributions

\subsection*{Conditional Probability Mass Function}

The concept of conditional PMF offers a way to understand the probability of an event given that another event has occurred. Specifically, in the context of random variables, the conditional PMF of $Y$ given $X=x$ describes the PMF of $Y$ under the condition that $X$ takes a specific value $x$. This concept is pivotal for dissecting the interdependencies between random variables, allowing us to refine our probability assessments based on new information.

\begin{definition}
\label{def:conditional_probability_function}
Let $\mathbf{X}=\left(X_{1},\ldots,X_{n}\right)$ be an $n$-dimensional random vector, which is partitioned into two subvectors: $\mathbf{Y}$, a $k$-dimensional random vector consisting of $k$ random variables from $\mathbf{X}$, and $\mathbf{Z}$, an $(n-k)$-dimensional random vector containing the remaining $n-k$ random variables of $\mathbf{X}$. Let $f$ denote the joint probability mass function of the combined random vectors $\mathbf{Y}$ and $\mathbf{Z}$, and let $f_\mathbf{Z}$ represent the marginal probability mass function of $\mathbf{Z}$ across its $(n-k)$ dimensions. Provided that for any vector $\mathbf{z} \in \mathbb{R}^{n-k}$ the condition $f_\mathbf{Z}(\mathbf{z})>0$ holds, the \emph{conditional probability mass function} $g$ for $\mathbf{Y}$ given $\mathbf{Z}=\mathbf{z}$ is defined as:
\[
g_{\mathbf{Y}|\mathbf{Z}} \left(\mathbf{y}\mid\mathbf{z}\right)=\frac{f\left(\mathbf{y},\mathbf{z}\right)}{f_\mathbf{Z} \left(\mathbf{z}\right)}
\]
\end{definition}

\begin{example}
{\color{red} TODO: Provide an example based on random variables.}
\end{example}

Next proposition generalizes the multiplication rule (see Equation \ref{eq:multiplication_rule}) by combining marginal and conditional PMFs to derive the joint PMF for any configuration of random variables within a random vector, accounting for the complex dependencies and interactions among multiple variables.

\begin{proposition}
Let $\mathbf{X}$, $\mathbf{Y}$, $\mathbf{Z}$, $f_{\mathbf{Z}}(\mathbf{z})$ and $g_{\mathbf{Y}|\mathbf{Z}}(\mathbf{y}|\mathbf{z})$ be defined as in Definiton \ref{def:conditional_probability_function}. Then, for each $\mathbf{z} \in \mathbf{Z}$ such that $f_{\mathbf{Z}}(\mathbf{z})>0$ and each possible value of $\mathbf{y} \in \mathbf{Y}$, the joint probability mass function is given by:
\[
f(\mathbf{x}) = g_{\mathbf{Y}|\mathbf{Z}}(\mathbf{y}|\mathbf{z})f_{\mathbf{Z}}(\mathbf{z})
\]
where $\mathbf{x} $ represents a specific instantiation of the random vector $\mathbf{X}$.
\end{proposition}
\begin{proof}
The proposition essentially follows from the definition of conditional probability and the properties of joint and marginal probabilities. By the definition of conditional probability:
\[
P(\mathbf{Y}=\mathbf{y} \mid \mathbf{Z}=\mathbf{z}) = \frac{P(\mathbf{Y}=\mathbf{y}, \mathbf{Z}=\mathbf{z})}{P(\mathbf{Z}=\mathbf{z})}
\]
Rearranging this equation gives us the joint probability function in terms of the conditional and marginal probability functions:
\[
P(\mathbf{Y}=\mathbf{y}, \mathbf{Z}=\mathbf{z}) = P(\mathbf{Y}=\mathbf{y} \mid \mathbf{Z}=\mathbf{z}) P(\mathbf{Z}=\mathbf{z})
 \]
Or equivalently, in terms of the function notation:
\[
f(\mathbf{x}) = g_{\mathbf{Y}|\mathbf{Z}}(\mathbf{y}|\mathbf{z}) f_{\mathbf{Z}}(\mathbf{z})
\]
\end{proof}

Similarly, the conditional PMF of $\mathbf{Z}$ given $\mathbf{Y}=\mathbf{y}$, denoted as $g_{\mathbf{Z}|\mathbf{Y}}(\mathbf{z}|\mathbf{y})$, can be combined with the marginal PMF of $\mathbf{Y}$, $f_{\mathbf{Y}}(\mathbf{y})$, to yield the same joint probability mass function of $\mathbf{X}$:
\[
f(\mathbf{x}) = g_{\mathbf{Z}|\mathbf{Y}}(\mathbf{z}|\mathbf{y}) f_{\mathbf{Y}}(\mathbf{y})
\]

Next proposition provides a generalization of the law of total probability (see {\color{red} XXX }) to random variables. The law of total probability is used to calculate the probability of a single event based on a partition of the sample space into mutually exclusive events.

\begin{proposition}
Given a random vector $\mathbf{X} = (X_1, X_2, \ldots, X_n)$ where each $X_i$ for $i = 1, 2, \ldots, n$ is a discrete random variable, let $f_{X_i}(x_i)$ denote the marginal probability mass function of the variable $X_i$. If $g_{X_i|\mathbf{X}_{-i}}(x_i|\mathbf{x}_{-i})$ represents the conditional probability mass function of $X_i$ given the values of all other variables in the vector $\mathbf{X}$ except $X_i$ (denoted as $\mathbf{X}_{-i}$), then the marginal probability mass function of $X_i$ can be expressed as:
\[ f_{X_i}(x_i) = \sum_{\mathbf{x}_{-i}} g_{X_i|\mathbf{X}_{-i}}(x_i|\mathbf{x}_{-i}) \prod_{j \neq i} f_{X_j}(x_j) \]
where the summation $\sum_{\mathbf{x}_{-i}}$ is over all possible combinations of values for the random variables in $\mathbf{X}_{-i}$, and the product $\prod_{j \neq i} f_{X_j}(x_j)$ represents the multiplication of the marginal probability mass functionss of all other variables except $X_i$.
\end{proposition}
\begin{proof}
The marginal probability mass function of a random variable $X_i$ within the vector $\mathbf{X}$ is defined as the probability of $X_i$ assuming a particular value $x_i$, regardless of the specific values assumed by the other variables in $\mathbf{X}$. Mathematically, this can be expressed as:
\[
f_{X_i}(x_i) = P(X_i = x_i)
\]
To compute this probability, we consider all possible combinations of values for the variables in $\mathbf{X}_{-i}$, which represents the set of all variables in $\mathbf{X}$ excluding $X_i$. The joint probability mass function of the entire random vector $\mathbf{X}$, which includes both $X_i$ and $\mathbf{X}_{-i}$, can be expressed in terms of conditional probability mass functions as follows:
\[
f_{\mathbf{X}}(x_1, x_2, \ldots, x_n) = g_{X_i|\mathbf{X}_{-i}}(x_i|\mathbf{x}_{-i}) \cdot f_{\mathbf{X}_{-i}}(\mathbf{x}_{-i})
\]
Here, $g_{X_i|\mathbf{X}_{-i}}(x_i|\mathbf{x}_{-i})$ is the conditional probability mass function of $X_i$ given the specific values of $\mathbf{X}_{-i}$, and $f_{\mathbf{X}_{-i}}(\mathbf{x}_{-i})$ is the joint probability mass function of the variables in $\mathbf{X}_{-i}$.

To isolate the marginal PMF of $X_i$, we sum over all possible values of $\mathbf{x}_{-i}$, leveraging the law of total probability:
\[
f_{X_i}(x_i) = \sum_{\mathbf{x}_{-i}} f_{\mathbf{X}}(x_i, \mathbf{x}_{-i}) 
\]
\[
= \sum_{\mathbf{x}_{-i}} g_{X_i|\mathbf{X}_{-i}}(x_i|\mathbf{x}_{-i}) \cdot f_{\mathbf{X}_{-i}}(\mathbf{x}_{-i}) 
\]
Recognizing that $f_{\mathbf{X}_{-i}}(\mathbf{x}_{-i})$ can be further decomposed into the product of the marginal probability mass functions of the individual variables in $\mathbf{X}_{-i}$ (assuming independence for simplicity), we can write:
\[
f_{X_i}(x_i) = \sum_{\mathbf{x}_{-i}} g_{X_i|\mathbf{X}_{-i}}(x_i|\mathbf{x}_{-i}) \cdot \prod_{j \neq i} f_{X_j}(x_j)
\]
This equation illustrates that the marginal probability mass funcion of $X_i$ is a summation over all possible combinations of values for the variables excluding $X_i$, weighted by the conditional probability of $X_i$ given those values and the joint probabilities of those values.
\end{proof}

The law of total probability is utilized to decompose the probability of an event into a sum of probabilities conditional on a partition of the sample space, making it useful for problems where direct calculation is impractical.

\begin{example}
{\color{red} TODO: Provide an example based on random variables}
\end{example}

Bayes' theorem (see {\color{red} XXX}) provides a way to update our probability estimates for a hypothesis given new evidence. For random vectors, the theorem can be generalized to accommodate the multi-dimensional nature of the variables involved. 

\begin{theorem}
Let $\mathbf{X} = (X_1, X_2, \ldots, X_n)$ and $\mathbf{Y} = (Y_1, Y_2, \ldots, Y_m)$ be two random vectors representing different sets of random variables. Suppose we are interested in the conditional probability distribution of $\mathbf{X}$ given observed values of $\mathbf{Y}$, denoted as $\mathbf{y} = (y_1, y_2, \ldots, y_m)$. The generalized Bayes' theorem for random vectors can be stated as:
\[
P(\mathbf{X}=\mathbf{x}|\mathbf{Y}=\mathbf{y}) = \frac{P(\mathbf{Y}=\mathbf{y}|\mathbf{X}=\mathbf{x})P(\mathbf{X}=\mathbf{x})}{P(\mathbf{Y}=\mathbf{y})}
\]
where $P(\mathbf{X}=\mathbf{x}|\mathbf{Y}=\mathbf{y})$ is the conditional probability of $\mathbf{X}$ given $\mathbf{Y}=\mathbf{y}$, $P(\mathbf{Y}=\mathbf{y}|\mathbf{X}=\mathbf{x})$ is the conditional probability of $\mathbf{Y}$ given $\mathbf{X}=\mathbf{x}$,  $P(\mathbf{X}=\mathbf{x})$ is the prior probability of $\mathbf{X}$, and $P(\mathbf{Y}=\mathbf{y})$ is the marginal probability of $\mathbf{Y}$, which can also be expressed using the law of total probability as:
\[
P(\mathbf{Y}=\mathbf{y}) = \sum_{\mathbf{x}} P(\mathbf{Y}=\mathbf{y}|\mathbf{X}=\mathbf{x})P(\mathbf{X}=\mathbf{x})
\]
for discrete random vectors.
\end{theorem}
\begin{proof}
{\color{red} TODO}
\end{proof}

This generalized form of Bayes' theorem allows us to update our belief about the probability distribution of a set of random variables $\mathbf{X}$ based on new information encapsulated in another set of random variables $\mathbf{Y}$. It emphasizes the interplay between the prior information we have about $\mathbf{X}$, the likelihood of observing $\mathbf{Y}=\mathbf{y}$ given $\mathbf{X}$, and the evidence provided by the actual observation of $\mathbf{Y}=\mathbf{y}$.

\begin{example}
{\color{red} TODO: Provide an example based on random variables}
\end{example}

Building on the familiar concept of independence between random variables (see {\color{red} XXX}), an important extension is the idea of conditional independence. This concept comes into play when the independence of a set of random variables is considered in the context of being conditioned on another set of variables.

\begin{definition}
Let $\mathbf{Z}$ be a random vector with joint probability function $f_\mathbf{Z} \left( \mathbf{z} \right)$. The variables of the random vector $\mathbf{X} = \left( X_{1}, \ldots, X_{n} \right)$ are \emph{conditionally independent}\index{Conditional independence} given $\mathbf{Z}$ if, for all $\mathbf{z}$ such that $f_\mathbf{Z}\left(\mathbf{z}\right)>0$, we have $g_{\mathbf{X} \mid \mathbf{Z}} \left(\mathbf{x}\mid\mathbf{z}\right)=\prod_{i=1}^{n}g_{X_i}\left(x_{i}\mid\mathbf{z}\right)$ where $g_{\mathbf{X} \mid \mathbf{Z}} \left( \mathbf{x} \mid \mathbf{z} \right)$ stands for the conditional probability mass function of $\mathbf{X}$ given $\mathbf{Z}=\mathbf{z}$ and $g_{X_i}\left(x_{i}\mid\mathbf{z}\right)$ stands for the conditional probability mass function of $X_{i}$ given $\mathbf{Z}=\mathbf{z}$.
\end{definition}

Let's consider a practical example involving three discrete random variables $X$, $Y$, and $Z$, where $X$ and $Y$ are not independent, but they become conditionally independent given the third random variable $Z$.

\begin{example}
{\color{red} TODO: Provide a practical example involving three discrete random variables $X$, $Y$ , and $Z$. The domain of $X$, $Y$ and $Z$ must be infinitely countable. $X$ and $Y$ must be dependent, and they become conditionally independent given $Z$.}
\end{example}


%
% Section: Characterizing Distributions
%

\section{Characterizing Distributions}
\label{sec:probability_expectation}

A \emph{measure of central tendency} is a number derived from a probability distribution, intended as a summary of that distribution. The most common measures of central tendency in use are the expected value and the median. Each of these measures provides a different approach to characterize distributions. It is also common to use \emph{metrics of dispersion} to describe the variability of a distribution around the measures of centrality. We will review two metrics of dispersion, the variance and the standard deviation. The metrics of disperson can also be used in case of bivariate distributions, under the names of covariance and correlation, to measure the \emph{statistical relationship} between two random variables. All these measures allow us to summarize and compare distributions.

%
% Subsection: Measures of Central Tendency
%

\subsection{Measures of Central Tendency}

The most common measures of central tendency in use to characterize probability distributions are the expected value and the median.

% Expected Value

\subsubsection*{Expeced Value}

The expected value of a discrete random variable is computed as the weighted average of all possible values of the variable, where weights are given by the probabilities of the outcomes.

\begin{definition}\label{probability:expectation}
Let $X$ be a discrete random variable whose probability mass function is $f$. The \emph{expected value}\index{Expected value} of $X$, denoted by $E\left(X\right)$, is defined as:
\[
E\left(X\right)=\sum_{x}xf\left(x\right)
\]
\end{definition}

The definition of expected value considers only the distribution of the random variable, not the original outcomes. Thus, two different random variables with the same distribution will have equal expected values, even if the underlying probability spaces are different.

The term "expected value" can be somewhat misleading because, for many discrete distributions, the expected value is not necessarily one of the possible values. For instance, when rolling a six-sided die, the expected value is \(3.5\), which is not an actual outcome of the roll. This counterintuitive aspect of the concept of expected value has led to considerable confusion in scientific research.

A drawback of the expected value is that it can be greatly affected by a small change in the probability assigned to a large value of $X$.

\begin{example}\label{ex:expected_salary}
Consider a company with a population of 100 employees, and we define a random variable based on their salaries. Let's $X = \{300, 6000\}$ with probabilities of $99/100$ and $1/100$ respectively. The expected salary is calculated as:
\[ 
E\left(X\right) = (300 \times 99/100) + (6000 \times 1/100) = 357
\]
Now, suppose that one of the base employees is promoted to the executive level with the same salary of \$6000. The recalculated expected salary would be:
\[
E\left(X\right) = (300 \times 98/10) + (6000 \times 2/100) = 414
\]
This example shows how just changing the salary of one single person can increase the expected salary of the company by more than $13\%$.
\end{example}

The expected value of the linear combination of $n$ random variables is the linear combination of their respective expected values.

\begin{proposition}
Let $X_{1}, \ldots, X_{n}$ be $n$ independent discrete random variables with expectations $E\left(X_{i}\right)$, and let $a_1, \ldots, a_n$ and $b$ constants, then
\[
E\left(a_{1}X_{1}+\ldots+a_{n}X_{n}+b\right)=a_{1}E\left(X_{1}\right)+\ldots+a_{n}E\left(X_{n}\right)+b
\]
\end{proposition}
\begin{proof}
We have that
\begin{multline}
E \left(a_1 X_1 + \ldots + a_n X_n +b \right) = 
\sum_{x_1} \ldots \sum_{x_n} \left(a_ 1 x_1 + \ldots + a_n x_n + b  \right) f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} \ldots \sum_{x_n} a_1 x_1 f\left(x_1, \ldots, x_n \right) + \ldots + \sum_{x_1} \ldots \sum_{x_n} a_n x_n f\left(x_1, \ldots, x_n \right) + \sum_{x_1} \ldots \sum_{x_n} b f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} a_1 x_1 f\left(x_1\right) + \ldots + \sum_{x_n} a_n x_n f\left( x_n \right) + b = 
a_1 \sum_{x_1} x_1 f\left(x_1\right) + \ldots + a_n \sum_{x_n} x_n f\left( x_n \right) + b = \\
a_1 E\left(X_1\right) + \ldots + a_n E\left(X_n\right) + b
\end{multline}
\end{proof}

The expected value of the product of $n$ independent random variables is equal to the product of the individual expected values.

\begin{proposition}
Let $X_{1}, \ldots, X_{n}$ be $n$ independent discrete random variables with expectations $E\left(X_{i}\right)$, then:
\[
E\left(\prod_{i=1}^{n}X_{i}\right)=\prod_{i=1}^{n}E\left(X_{i}\right)
\]
\end{proposition}
\begin{proof}
We have that
\begin{multline}
E \left(X_1  \cdot \ldots \cdot X_n  \right) = 
\sum_{x_1} \ldots \sum_{x_n} \left(x_1 \cdot \ldots \cdot x_n  \right) f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} \ldots \sum_{x_n} x_1 f\left(x_1, \ldots, x_n \right) \cdot \ldots \cdot \sum_{x_1} \ldots \sum_{x_n} x_n f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} x_1 f\left(x_1\right) \cdot \ldots \cdot \sum_{x_n} x_n f\left( x_n \right) = 
E \left( X_1 \right) \cdot \ldots \cdot E \left( X_n \right)
\end{multline}
\end{proof}

The expected value of the product of non-independent random variables is not necesarily equal to the product of their individual expected values.

% The Median

\subsubsection*{The Median}

The median of a discrete random variable is a measure of central tendency that represents the point that separates the higher half from the lower half of a probability distribution.

\begin{definition}
Let $X$ be a discrete random variable. The \emph{median}\index{Median} of $X$, denoted by $m$, is the value that satisfies:
\[
Pr\left(X\leq m\right)\geq \frac{1}{2} \quad \text{and} \quad Pr\left(X\geq m\right)\geq \frac{1}{2}
\]
\end{definition}

The definition of the median considers the distribution of the random variable, ensuring that at least half of the probability mass lies on either side of the median.

The term "median" is often more intuitive than "expected value" because it is always one of the possible values, although in the case of an even number of outcomes it is customary to use the average of the two middle values.

The median is a robust measure of central tendency, especially useful when the data has outliers or is skewed.

\begin{example}
Consider the company from Example \ref{ex:expected_salary}. The median of the salaries is calculated as the value $m$ for which $Pr\left(X\leq m\right)\geq \frac{1}{2}$. In this case, $m=300$. Now, suppose that one of the base employees is promoted to the executive level with the same salary of \$6000. The recalculated median would still be $m=300$. This example shows that the median is a more robust measure than the expected value in the presence of outliers.
\end{example}

%
% subsection: Measures of Dispersion
%

\subsection{Measures of Dispersion}

The most common measures of dispersion in use to characterize probability distributions are the variance and its squared root, called standard deviation.

% The Variance

\subsubsection*{The Variance}

The variance of a discrete random variable is a measure of the spread or dispersion of the possible values of the variable around the expected value.

\begin{definition}
Let $X$ be a discrete random variable with expected value $E(X)$ and probability mass function $f$. The \emph{variance}\index{Variance} of $X$, denoted by $Var(X)$, is defined as:
\[
Var(X) = E[(X - E(X))^2] = \sum_{x} (x - E(X))^2 f(x)
\]
\end{definition}

Variance considers the distribution of the random variable and provides a measure of how much the values differ from the expected value. For instance, if all possible values of a random variable are the same, the variance is zero.

As it was the case of the expected value, a drawback of the variance is that it can be influenced significantly by outliers because it involves squaring the deviations from the mean.

\begin{example}
Consider the company from Example \ref{ex:expected_salary}, with an expected salary of $E(X) = 357$: We calculate the variance as:
\[
Var(X) = (300 - 357)^2 \times 99/100 + (6000 - 357)^2 \times 1/100 = 3218250
\]
\end{example}

Next proposition states that for a linear combination of independent random variables, the variance of the combination is the weighted sum of the variances of the individual variables, where the weights are the squares of the coefficients in the linear combination.

\begin{proposition}
Let \(X_{1}, \ldots, X_{n}\) be $n$ independent random variables with finite expected values, and \(a_{1},\ldots,a_{n}\) and \(b\) be arbitrary constants, then:
\[
Var\left(a_{1}X_{1}+\ldots+a_{n}X_{n}+b\right)=a_{1}^{2}Var\left(X_{1}\right)+\ldots+a_{n}^{2}Var\left(X_{n}\right)
\]
\end{proposition}
\begin{proof}
Let \(Y = a_{1}X_{1} + \ldots + a_{n}X_{n} + b\). The variance of \(Y\) is given by:
\[
Var(Y) = Var(a_{1}X_{1} + \ldots + a_{n}X_{n} + b)
\]
Since variance is unaffected by the addition of a constant, we can ignore \(b\):
\[
Var(Y) = Var(a_{1}X_{1} + \ldots + a_{n}X_{n})
\]
Now, using the linearity of variance for independent random variables, we have:
\[
Var(a_{1}X_{1} + \ldots + a_{n}X_{n}) = Var(a_{1}X_{1}) + \ldots + Var(a_{n}X_{n})
\]
Next, we use the property that for any random variable \(X_i\) and constant \(a_i\), \(Var(a_i X_i) = a_i^2 Var(X_i)\):
\[
Var(a_{1}X_{1}) + \ldots + Var(a_{n}X_{n}) = a_{1}^2 Var(X_{1}) + \ldots + a_{n}^2 Var(X_{n})
\]
Therefore, we have:
\[
Var(a_{1}X_{1} + \ldots + a_{n}X_{n} + b) = a_{1}^{2}Var(X_{1}) + \ldots + a_{n}^{2}Var(X_{n})
\]
\end{proof}

A key relationship in probability theory is the formula that expresses variance in terms of expected values. This relationship not only simplifies calculations but also provides deeper insight into the nature of variance as a measure of risk and variability.

\begin{proposition}
Let $X$ be a discrete random variable with expected value $E(X)$. Then, the variance of $X$ is given by:
\[
Var(X) = E(X^2) - [E(X)]^2
\]
\end{proposition}
\begin{proof}
To prove this proposition, we start from the definition of variance:
\[
Var(X) = E[(X - E(X))^2]
\]
Expanding the square inside the expectation yields:
\[
Var(X) = E[X^2 - 2X \cdot E(X) + (E(X))^2]
\]
Applying the linearity of expectation, this becomes:
\[
Var(X) = E(X^2) - 2E(X)E(X) + E((E(X))^2)
\]
Since \(E(X)\) is a constant, the expectation of a constant is the constant itself, so:
\[
E((E(X))^2) = (E(X))^2
\]
Thus, the expression simplifies to:
\[
Var(X) = E(X^2) - 2(E(X))^2 + (E(X))^2
\]
Simplifying further, we cancel out terms:
\[
Var(X) = E(X^2) - (E(X))^2
\]
\end{proof}

In the area of statistical inference, it is also highly convenient to compute the sample variance as a measure of the dispersion of $n$ random variables. In particular, we will compute the sample variance of random samples.

\begin{definition}
Let $X_1, X_2, \ldots, X_n$ be $n$ random variables with sample mean $\bar{X}_n$. The \emph{sample variance}\index{Sample variance} of $X_1, X_2, \ldots, X_n$, denoted by $S^2$, is defined as:
\[
S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X}_n)^2
\]
\end{definition}

Do not confuse the sample variance, which is a statistic based on observed values, with the theoretical variance of the distribution. We will study some important properties of the variance of large random samples in Section \ref{sec:probability_random_samples}.

% Standard Deviation

\subsubsection*{Standard Deviation}

The standard deviation is also a statistical measure that quantifies the dispersion or variability in a set of data values. However, unlike variance, which squares the differences from the mean, resulting in units that are the square of the original data units, the standard deviation is the square root of the variance. This adjustment is crucial because it brings the units back to the original units of the data, making the measure more intuitive and directly interpretable.

\begin{definition}
Let $X$ be a discrete random variable with expected value $E(X)$,  variance $Var(X)$ and probability mass function $f$. The \emph{standard deviation}\index{Standar deviation} of $X$, denoted by $\sigma$, is defined as:
\[
\sigma = \sqrt{Var(X)} = \sqrt{\sum_{x} (x - E(X))^2 f(x)}
\]
\end{definition}

The standard deviation provides a numerical summary of how scattered the values of $X$ are around the mean. A smaller standard deviation indicates that the values tend to be closer to the mean (less spread out), whereas a larger standard deviation indicates that the values are more spread out from the mean.

\begin{proposition}
Let $X$ be a random variable with standard deviation $\sigma_X$, and $a$ and $b$ be arbitrary constants, then the standard deviation $\sigma_Y$ of the random variable $Y = aX + b$ is $|a|$ times the standard deviation of $X$, i.e., $\sigma_Y = |a| \sigma_X$.
\end{proposition}
\begin{proof}
The variance of $Y$ can be calculated as:
\[
\sigma_Y^2 = \sum_{x} (a x + b - (a E(X) + b))^2 f(x) = a^2 \sum_{x} (x - E(X))^2 f(x) = a^2 \sigma_X^2
\]
Taking the square root of both sides, we get:
\[
\sigma_Y = \sqrt{a^2 \sigma_X^2} = |a| \sigma_X.
\]
\end{proof}

The sample standard deviation is used to estimate the standard deviation of a population based on a sample taken from it. Unlike the population standard deviation, which uses the true mean of the entire population, the sample standard deviation uses the mean of the sample as an estimate of the true mean.

\begin{definition}
Let $X_1, X_2, \ldots, X_n$ be $n$ random variables with sample mean $\bar{X}_n$ and sample variance $S^2$. The \emph{sample standard deviation}\index{Sample standard deviation} of $X_1, X_2, \ldots, X_n$, denoted by $S$, is defined as:
\[
S = \sqrt{S^2} = \sqrt{\frac{1}{n-1} \sum_{x} (X - \bar{X})^2}
\]
\end{definition}

%
% Measures of Statistical Relationship
%

\subsection{Measures of Statistical Relationship}

{\color{red} Covariance and correlation are attempst to measure the linear dependence between to random variables.}

% Covariance

\subsubsection*{Covariance}

Covariance is a measure used to determine the degree to which two random variables \(X\) and \(Y\) vary together. It gives an indication of the direction of the linear relationship between the variables.

\begin{definition}
Let $X$ and $Y$ be two random variables with finite means $E(X)$ and $E(Y)$ respectively. The \emph{covariance}\index{Covariance} of $X$ and $Y$, denoted by $Cov\left(X, Y\right)$ is defined as $Cov\left(X, Y\right) = E\left[\left(X- E(X) \right) \left(Y - E(Y) \right) \right]$
\end{definition}

The sign of the covariance indicates the direction of the relationship. A positive covariance indicates that as \(X\) increases, \(Y\) tends to increase. A negative covariance indicates that as \(X\) increases, \(Y\) tends to decrease. A covariance of zero suggests no linear relationship between the variables. However, it is crucial to note that zero covariance does not preclude the existence of a non-linear relationship between \(X\) and \(Y\). Variables can still be related in complex, non-linear ways that are not captured by covariance.

Next proposition represents a fundamental way to calculate covariance, showing how it measures the degree to which two variables vary together in relation to their individual means. 

\begin{proposition}
Let $X$ and $Y$ be two random variables with finite means $E(X)$ and $E(Y)$ respectively. We have that $Cov\left(X,Y\right)=E\left(XY\right)-E\left(X\right)E\left(Y\right)$.
\end{proposition}
\begin{proof}
The covariance of \(X\) and \(Y\) is defined as the expected value of the product of their deviations from their respective means. Expanding the product within the expectation gives:
\[
\operatorname{Cov}(X, Y) = \mathrm{E}[(X - \mathrm{E}(X))(Y - \mathrm{E}(Y))] = \mathrm{E}[XY - X\mathrm{E}(Y) - Y\mathrm{E}(X) + \mathrm{E}(X)\mathrm{E}(Y)]
\]
Using the linearity of expectation, this expression can be simplified as:
\[
\mathrm{E}[XY - X\mathrm{E}(Y) - Y\mathrm{E}(X) + \mathrm{E}(X)\mathrm{E}(Y)] = \mathrm{E}(XY) - \mathrm{E}(X\mathrm{E}(Y)) - \mathrm{E}(Y\mathrm{E}(X)) + \mathrm{E}(\mathrm{E}(X)\mathrm{E}(Y))
\]
And since \(\mathrm{E}(Y)\) and \(\mathrm{E}(X)\) are constants:
\[
\mathrm{E}(X\mathrm{E}(Y)) = \mathrm{E}(X)\mathrm{E}(Y) \quad \text{and} \quad \mathrm{E}(Y\mathrm{E}(X)) = \mathrm{E}(Y)\mathrm{E}(X)
\]
Hence, the expression further simplifies to:
\[
\mathrm{E}(XY) - \mathrm{E}(X)\mathrm{E}(Y) - \mathrm{E}(Y)\mathrm{E}(X) + \mathrm{E}(X)\mathrm{E}(Y) = \mathrm{E}(XY) - \mathrm{E}(X)\mathrm{E}(Y)
\]
Thus, the covariance of \(X\) and \(Y\) is proved to be:
\[
\operatorname{Cov}(X, Y) = \mathrm{E}(XY) - \mathrm{E}(X)\mathrm{E}(Y)
\]
\end{proof}

The sample covariance is used to estimate the covarianze of two populations based on a sample taken from them.

\begin{definition}
Let \(X_1, X_2, \ldots, X_n\) and \(Y_1, Y_2, \ldots, Y_n\) be two sets of random variables with sample means $\bar{X}$ and $\bar{Y}$ respectively. The \emph{sample covariance}\index{Sample covariance} between \(X\) and \(Y\) is defined as follows:
\[
s_{XY} = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})
\]
\end{definition}

% Correlation

\subsubsection*{Correlation}

The magnitude of the covariance is not standardized, meaning that it can be difficult to interpret the strength of the relationship without context. This is why correlation, which normalizes covariance, is often used to measure the strength of the linear relationship between two variables.

\begin{definition}
Let $X$ and $Y$ be two random variables with finite variances $Var(X)$ and $Var(Y)$ respectively. Then the \emph{correlation}\index{Correlation} of $X$ and $Y$, denoted by $Cor\left(X,Y\right)$, is defined as $Cor\left(X,Y\right)=\frac{Cov\left(X,Y\right)}{Var(X) Var(Y)}$
\end{definition}

Correlation is a statistical measure that quantifies the strength and direction of a linear relationship between two random variables. Unlike covariance, correlation is dimensionless and standardized, providing a value between -1 and 1.

\begin{proposition}
The correlation coefficient \(\rho_{X,Y}\) lies in the range \([-1, 1]\), where:
- \(+1\) indicates a perfect positive linear relationship,
- \(-1\) indicates a perfect negative linear relationship,
- \(0\) indicates no linear relationship (but not necessarily no relationship).
\end{proposition}
\begin{proof}
Correlation normalizes covariance by the product of the standard deviations of \(X\) and \(Y\), thus it is dimensionless:
\[
\rho_{X,Y} = \frac{\operatorname{Cov}(X, Y)}{\sigma_X \sigma_Y}
\]
where \(\sigma_X = \sqrt{\operatorname{Var}(X)}\) and \(\sigma_Y = \sqrt{\operatorname{Var}(Y)}\). By the Cauchy-Schwarz inequality, we have:
\[
(\mathrm{E}[XY])^2 \leq \mathrm{E}[X^2] \mathrm{E}[Y^2]
\]
Substituting for expectations from the definitions of variance and covariance, and rearranging terms, the absolute value of the covariance does not exceed the product of the standard deviations of \(X\) and \(Y\):
\[
|\operatorname{Cov}(X, Y)| \leq \sigma_X \sigma_Y
\]
This implies:
\[
-1 \leq \frac{\operatorname{Cov}(X, Y)}{\sigma_X \sigma_Y} \leq 1
\]
Thus:
\[
-1 \leq \rho_{X,Y} \leq 1
\]
\end{proof}

Correlation is a vital tool in statistics for assessing how strong a linear relationship exists between two variables. A correlation close to \(-1\) or \(+1\) indicates a strong linear relationship, whereas a correlation close to \(0\) suggests a weak linear relationship. This metric is particularly useful in fields such as finance, economics, and the natural sciences where understanding the relationship between variables is crucial for modeling and prediction.

\begin{proposition}
If X and Y are independent random variables with $0<\text{\ensuremath{\sigma_{X}^{2}}}<\infty$ and $0<\text{\ensuremath{\sigma_{Y}^{2}}}<\infty$ then $Cov\left(X,Y\right)=\rho\left(X,Y\right)=0$
\end{proposition}
\begin{proof}
\end{proof}

The converse is not true as a general rule. Two dependent random variables can be uncorrelated.

\begin{proposition}
Suppose that X is a random variable such that $0<\sigma_{X}^{2}\infty$ and $Y=aX+b$ for some constants a and b, where $a\neq0$. If $a>0$ then $\rho\left(X,Y\right)=1$. If $a<0$, then $\rho\left(X,Y\right)=-1$.
\end{proposition}
\begin{proof}
\end{proof}

The converse is also true, that is, if $\left|\rho\left(X,Y\right)\right|=1$ implies that X and Y are linearly related.

\begin{proposition}
If X and Y are random variables such that $Var\left(X\right)<\infty$ and $Var\left(Y\right)<\infty$, then $Var\left(X+Y\right)=Var\left(X\right)+Var\left(Y\right)+2Cov\left(X,Y\right)$
\end{proposition}
\begin{proof}
\end{proof}

For all constants a and b, it can be shown that $Cov\left(aX,bY\right)=abCov\left(X,Y\right)$.

\begin{proposition}
Let X and Y are random variables such that $Var\left(X\right)<\infty$ and $Var\left(Y\right)<\infty$, and let a, b and c be constants, then $Var\left(aX+bY+c\right)=a^{2}Var\left(X\right)+b^{2}Var\left(Y\right)+2abCov\left(X,Y\right)$
\end{proposition}
\begin{proof}
\end{proof}

A special case is $Var\left(X-Y\right)=Var\left(X\right)+Var\left(Y\right)-2Cov\left(X,Y\right)$

\begin{proposition}
If $X_{1},\ldots,X_{n}$ are random variables such that $Var\left(X_{i}\right)<\infty for i=1,\ldots,n$, then $Var\left(\sum_{i=1}^{n}X_{i}\right)=\sum_{i=1}^{n}Var\left(X_{i}\right)+2\sum\sum Cov\left(X_{i},X_{j}\right)$
\end{proposition}
\begin{proof}
\end{proof}

\begin{proposition}
If $X_{1},\ldots,X_{n}$ are uncorrelated random variables, then $Var\left(\sum_{i=1}^{n}X_{i}\right)=\sum_{i=1}^{n}Var\left(X_{i}\right)$
\end{proposition}
\begin{proof}
\end{proof}


%
% Section: Distribution
%

\section{Common Distributions}
\label{sec:probability_distributions}

{\color{red} In this section we are going to define and discuss serveral special families of distributions that are widely used in applications of probability theory. In particular, we will present the family of dicrete distributions uniform, bernioully, and binomial [...] we shall briefly describe how each of these families of distributions arise in applied problems and show why each might be an appropriate probability model for some experiment [...] For each family, we shall present the form of the probability mass function and disuss some of the basic properties of the distributions of the family} 

%
% Uniform Distribution
%

\subsection{Uniform Distribution}

The uniform distribution is the simplest probability distribution in probability theory. It models scenarios where all outcomes are equally likely.

\begin{definition}
A random variable $X$ is said to follow an \emph{uniform distribution}\index{Uniform distribution} if the probability of each value is the same. Specifically, if $X$ takes on values in the set $\{x_1, x_2, \ldots, x_n\}$, the probability mass function is given by:
\[
P(X = x_i) = \frac{1}{n} \quad \text{for } i = 1, 2, \ldots, n.
\]
\end{definition}

An example of the discrete uniform distribution would be throwing a fair die. In contrast to other distributions studied in this section, the uniform distribution is non-parametric, meaning it does not depend on a parameter to be fully specified. The expected value of a discrete uniform random variable $X$ that takes on values in $\{x_1, x_2, \ldots, x_n\}$ is $E(X) = \frac{x_1 + x_2 + \ldots + x_n}{n}$. The variance of a discrete uniform random variable $X$ that takes on values in $\{x_1, x_2, \ldots, x_n\}$ is $Var(X) = \frac{1}{n} \sum_{i=1}^n (x_i - E(X))^2$.

%
% Bernoulli Distribution
%

\subsection{Bernoulli Distribution}

The Bernoulli distribution is one of the simplest and most fundamental probability distributions in probability theory. It models scenarios where there are only two possible outcomes, often termed "success" and "failure."

\begin{definition}
A random variable $X$ is said to follow a \emph{Bernoulli distribution}\index{Bernoulli distribution} with parameter $p$, with $0 \leq p \leq 1$, if it takes the value 1 (representing "success") with probability $p$ and the value 0 (representing "failure") with probability $1-p$. The probability mass function is given by:
\[
P(X = x) = 
\begin{cases} 
p & \text{if } x = 1, \\
1 - p & \text{if } x = 0.
\end{cases}
\]
\end{definition}

The Bernoulli distribution arises naturally in many applied problems, especially those involving binary outcomes, in which the parameter $p$ represents the probability of success. For example, in medical studies, the outcome of a treatment can be a success (cure) or a failure (no cure), which can be modeled using a Bernoulli distribution.

The expected value of a Bernoulli random variable $X$ with parameter $p$ is $E(X) = p$. This makes intuitive sense, as the expected value represents the average outcome of many trials, which would tend toward the probability of success. The variance of a Bernoulli random variable is given by $\text{Var}(X) = p(1 - p)$. The variance measures the spread or dispersion of the distribution around the expected value. The product $p(1 - p)$ achieves its maximum value when $p = 0.5$, indicating maximum uncertainty when the probabilities of success and failure are equal.

\begin{definition}
Let $X_{1}, X_{2}, \ldots$ be a sequence of random variables that follows a Bernoulli distribution with parameter $p$, then it is said that $X_{1}, X_{2}, \ldots$ are \emph{Bernoulli trials}\index{Bernoulli trial} with parameter $p$. An infinite sequence of independent Bernoulli trials is called a \emph{Bernoulli process}\index{Bernoulli process}.
\end{definition}

The Bernoulli process serves as a foundation for more complex stochastic processes and is used in various applications, such as modeling binary events over time.

%
% Binomial Distribution
%

\subsection{Binomial Distribution}

The binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success. This distribution is widely used in probability theory for various applications involving binary outcomes.

\begin{definition}
A random variable $X$ is said to follow a \emph{binomial distribution}\index{Binomial distribution} with parameters $n$ and $p$ if it represents the number of successes in $n$ independent Bernoulli trials, each with success probability $p$. The probability mass function is given by:
\[
P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k} \quad \text{for } k = 0, 1, 2, \ldots, n.
\]
\end{definition}

The parameter $n$ is the number of trials, and $p$ is the probability of success on each trial. The term $\binom{n}{k}$ is the binomial coefficient, representing the number of ways to choose $k$ successes out of $n$ trials.

The binomial distribution arises naturally in many applied problems where the outcomes are binary (success/failure).

\begin{example}
Suppose a company produces light bulbs, and historically, they know that 95\% of the bulbs they produce are functional (success) and 5\% are defective (failure). The company wants to check the quality of a batch of 20 light bulbs by randomly selecting and testing them. This problem can be modeled using the binomial distribution, where the number of trials is $n = 20$ and the probability of success is $p = 0.95$. For example, the probability of exactly 18 out of 20 light bulbs being functional is:
\[
P(X = 18) = \binom{20}{18} (0.95)^{18} (0.05)^2.
\]
\end{example}

The expected value of a binomial random variable $X$ with parameters $n$ and $p$ is $E(X) = np$. This represents the average number of successes in $n$ trials. The variance of a binomial random variable is $Var(X) = np(1 - p)$. The variance measures the spread or dispersion of the distribution around the expected value.

%
% Discrete Normal Distribution
%

\subsection{Discrete Normal Distribution}

The \textit{normal distribution} is a continuous probability distribution that frequently appears in natural and social sciences, primarily due to the Central Limit Theorem (see Theorem \ref{th:central_limit_theorem}). However, many real-world situations involve discrete outcomes, such as counts of events or integer-valued random variables. In these cases, a discrete analog to the normal distribution is required. This need does not represent a limitation, as continuous distributions often emerge as limiting abstractions of inherently discrete processes.

In this section, we introduce the \textit{discrete normal distribution}, derived as the limit of a binomial distribution. This distribution serves as an approximation of the normal distribution for integer-valued random variables. We also derive its key properties through formal definitions and propositions.

To define the discrete normal distribution as a limit of the binomial distribution, we first need to introduce the concept of a normalized version of a random variable.

\begin{definition}
Let $X$ be a discrete random variable with expected value $E(X)$ and variance $Var(X)$. The standardized random variable of $X$, denoted by $Z$, is defined as:
\[
Z = \frac{X - E(X)}{\sqrt{Var(X)}}
\]
\end{definition}

This normalization centers the distribution around 0 (the standardized mean) and scales it by the standard deviation. 

\begin{example}
If the random variable $X$ follows a binomial distribution with parameters $n$ and $p$, its standardized version $Z$ is given by:
\[
Z = \frac{X - np}{\sqrt{np(1-p)}}
\]
The probability mass function of $Z$ can be written as:
\[
P\left(Z = k\right) = P\left( Z = \frac{k - np}{\sqrt{np(1-p)}} \right) = \binom{n}{k} p^k (1-p)^{n-k}, \quad k = 0, 1, \dots, n.
\]
\end{example}

The \textit{discrete normal distribution} is defined as the limit of a normalized binomial distribution as $n \to \infty$.

\begin{definition}
A random variable $X$ is said to follow a \emph{discrete normal distribution}\index{Discrete normal distribution} with expected value 0 and variance 1 if it has the following probability mass function:
\[
P(X = k) = \lim_{n \to \infty} P\left( Z_n = \frac{k - np}{\sqrt{np(1-p)}} \right),
\]
where $Z_n$ is a normalized binomial variable with parameters $n$ and $p$.
\end{definition}

This distribution can be regarded as a fundamental model for systems where outcomes are inherently discrete, such as the distribution of counts or events.

In practice, this distribution retains the discrete nature of the binomial distribution while adopting the bell-shaped curve characteristic of the normal distribution. The discrete normal distribution thus encapsulates the essence of the normal distribution while remaining defined on the set of integers.

\begin{proposition}
As $\sigma$ becomes large, the closed-form expression for the probability mass function of the discrete normal distribution is:
\[
P(X = k) = \frac{1}{Z} \exp\left(-\frac{(k - \mu)^2}{2\sigma^2}\right),
\]
where $Z$ is the normalization constant.
\end{proposition}

\begin{proof}
As $\sigma \to \infty$, the difference between consecutive integer values $k$ becomes negligible compared to $\sigma$, causing the discrete distribution to behave like a continuous distribution. Formally, we compare the discrete normal PMF with the probability density function (PDF) of the continuous normal distribution:
\[
f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right).
\]
For large $\sigma$, the normalization constant $Z$ of the discrete normal distribution approaches $\sqrt{2\pi \sigma^2}$, the normalization constant of the continuous normal distribution, and the PMF of the discrete normal distribution approaches the PDF of the continuous normal distribution evaluated at integer points.

Therefore, in the limit $\sigma \to \infty$, the discrete normal distribution converges to the continuous normal distribution.
\end{proof}

\begin{example}
Please insert a practical example here.
\end{example}

\begin{proposition}
The discrete normal distribution is symmetric about its mean $\mu$. Formally, for any integer $k$:
\[
P(X = \mu + k) = P(X = \mu - k).
\]
\end{proposition}

\begin{proof}
The symmetry follows directly from the functional form of the PMF:
\[
P(X = \mu + k) = \frac{1}{Z} \exp\left(-\frac{(\mu + k - \mu)^2}{2\sigma^2}\right) = \frac{1}{Z} \exp\left(-\frac{k^2}{2\sigma^2}\right).
\]
Similarly,
\[
P(X = \mu - k) = \frac{1}{Z} \exp\left(-\frac{(\mu - k - \mu)^2}{2\sigma^2}\right) = \frac{1}{Z} \exp\left(-\frac{k^2}{2\sigma^2}\right).
\]
Since both expressions are equal, the distribution is symmetric about $\mu$.
\end{proof}

The discrete normal distribution serves as a bridge between the discrete and continuous realms of probability distributions. It retains many key properties of the continuous normal distribution, such as symmetry, centrality, and the bell-shaped curve, while accommodating the discrete nature of certain random variables. By formally defining the distribution and studying its properties, we gain a powerful tool for modeling real-world phenomena where outcomes are discrete but exhibit normal-like behavior.

This distribution is especially useful in areas such as manufacturing, education, and epidemiology, where event counts or integer-valued outcomes naturally arise, and where the underlying variability can be captured using a normal-like distribution. The discrete normal distribution allows us to model these scenarios more precisely, ensuring that our probabilistic models align with the real-world structure of the data.

%
% Section: Large Random Samples
%

\section{Large Random Samples}
\label{sec:probability_random_samples}

A random sample is a collection of independent and identically distributed random variables. Random samples are fundamental in probability theory, providing a basis for making inferences about an unknown probability distribution. Two key results that arise from random samples are the law of large numbers, which ensures that the sample mean converges to the expected value of the distribution as the sample size increases, and the central limit theorem, which states that the distribution of the sample mean approaches a normal distribution as the sample size becomes large, regardless of the original population's distribution.

% Random Samples

\subsection{Random Samples}

The concept of a random sample is pivotal in the field of statistical learning, as assuming a set of random variables constitutes a random sample greatly simplifies the mathematical underpinnings of inferential methods. Nevertheless, the criteria for a collection of variables to be considered a random sample are not always met in real-world scenarios.

\begin{definition}
Let $f$ be a probability mass function, and let $X_1, X_2, \ldots, X_n$ a collection of random variables.  The variables $X_1, X_2, \ldots, X_n$ form a \emph{random sample}\index{Random sample} from the distribution $f$ if each random variable $X_i$ follows the probability mass function $f$, and the variables $X_1, X_2, \ldots, X_n$ are independent of each other. These random variables are also referred to as \emph{independent and identically distributed or i.i.d.}\index{Independent and identically distributed}. The number $n$ of random variables is referred to as the \emph{sample size}\index{Sample size}.
\end{definition}

The joint distribution $g$ of a random sample $X_1, X_2, \ldots, X_n$ is given by:
\[
g \left( x_1, x_2, \ldots, x_n \right) = f \left( x_1 \right) f \left( x_2 \right) \ldots f \left( x_n \right)
\]

\begin{example}
A factory produces light bulbs, and the quality control department is interested in determining the proportion of defective bulbs. The number of defective bulbs in a batch of 100 bulbs is modeled using a binomial distribution, where each bulb has a probability $p$ of being defective. The department randomly selects 5 batches, each containing 100 bulbs each. Let the random variables $X_1, X_2, X_3, X_4, X_5$ represent the number of defective bulbs in each of these 5 batches. Each $X_i$ follows the binomial distribution \( \text{Binomial}(100, p) \). The random variables $X_1, X_2, X_3, X_4, X_5$ are independent and identically distributed, and so, they form a random variable.
\end{example}

In the area of statistical inference, it is also highly convenient to compute the sample mean, as the average of $n$ random variables.
\begin{definition}
Let $X_1, X_2, \ldots, X_n$ be $n$ random variables. The \emph{sample mean}\index{Sample mean} of $X_1, X_2, \ldots, X_n$, denoted by $\bar{X}_n$, is defined as:
\[
\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i
\]
\end{definition}

Do not confuse $\frac{1}{n} \left( X_1 + X_2 + \ldots + X_n \right)$, which is a random variable\footnote{recall that with $X_1 + \ldots + X_n$ we are defining a new random variable over the cartesian product of the original sample spaces, not adding $n$ probability distributions} with the expected value $E \left( X_1 + X_2 + \ldots + X_n \right)$, which is a real number.

As we will show in the next proposition, the concept of sample mean is particurlarly relevant for the case of $X_1, X_2, \ldots, X_n$ being a random sample.

\begin{proposition}\label{prop:sample_mean}
Let $X_1, \ldots, X_n$ be a random sample with finite mean $E \left( X_i \right) = \mu$ and finite variance $Var \left( X_i \right) = \sigma^2$, and let $\overline {X}_n = \frac {1}{n} \left( X_1 + \ldots + X_n \right)$ be the sample mean. Then we have that $E\left(\bar{X}_{n}\right)=E(X)$ and that $Var\left(\bar{X}_{n}\right)=Var(X)/n$.
\end{proposition}
\begin{proof}
Using the linearity of expectation, we have:
\[
E\left(\overline{X}_n\right) = E\left(\frac{1}{n} \sum_{i=1}^{n} X_i\right) = \frac{1}{n} \sum_{i=1}^{n} E(X_i) = \frac{1}{n} \cdot n \cdot \mu = \mu
\]
Using the properties of variance and the fact that the \(X_i\) are independent, we have:
\[
Var\left(\overline{X}_n\right) = Var\left(\frac{1}{n} \sum_{i=1}^{n} X_i\right)
\]
Since variance is homogeneous of degree 2, we can factor out the constant \( \frac{1}{n} \):
\[
Var\left(\overline{X}_n\right) = \frac{1}{n^2} Var\left(\sum_{i=1}^{n} X_i\right)
\]
For independent random variables, the variance of the sum is the sum of the variances:
\[
Var\left(\sum_{i=1}^{n} X_i\right) = \sum_{i=1}^{n} Var(X_i) = n \cdot \sigma^2
\]
Thus:
\[
Var\left(\overline{X}_n\right) = \frac{1}{n^2} \cdot n \cdot \sigma^2 = \frac{\sigma^2}{n}
\]
\end{proof}

The probability distribution of $\bar{X}_n$ will be more concentrated arount the mean value $\mu$ that was the original distribution. 

% Law of Large Numbers

\subsection{Law of Large Numbers}

The law of large numbers is a theorem that states that the sample mean of a large random sample, i.e. a large number of independent and identically distributed random variables, should be close to the expected value of the random variables, and that the more variables we add, the closer will be to that value. Before to prove this important theorem we have to prove two other related propostions: Markov's inequality and Chebyshev's inequality.

Markov's inequality provides a simple way to measure how unlikely it is for a random variable to take on large values, given its expected value. It leverages the fact that if the average value of random variable $X$ is small, then it is improbable for $X$ to take on a large value (recall Example \ref{ex:expected_salary}). Markov's inequality is particularly useful because it makes no assumptions about the distribution of the random variable, apart from it being non-negative and having a finite expected value.

\begin{proposition}[Markov's Inequality]\index{Markov's Inequality}
Let $X$ be a nonnegative random variable, i.e. $P\left( X \geq 0 \right) = 1$, with expected value $E(X)$. Then for every real number $t>0$ we have that 
\[
P \left( X \geq t \right) \leq \frac{E(X)}{t}
\]
\end{proposition}
\begin{proof}
Let $f$ be the probability mass function of $X$. Since $X$ is non-negative we have that $E(X) = \sum_{x>0} x f \left( x \right)$. Then
\[
E(X) = \sum_{x>0} x f \left( x \right) = \sum_{x=0}^{t} x f \left( x \right) + \sum_{x=t}^{\infty} x f \left( x \right) \geq
\sum_{x=t}^{\infty} x f \left( x \right) \geq \sum_{x=t}^{\infty} t f \left( x \right) = t \sum_{x=t}^{\infty} f \left( x \right) =
t P \left( X \geq t \right)
\]
that is, $E(X) \geq t P \left( X \geq t \right)$.
\end{proof}

For example, if the expected value of a random variable $X$ is $10$, and we are interested in the probability that $X$ is at least $50$, Markov's inequality tells us that $P(X \geq 50) \leq 10/50 = 0.2$. While Markov's inequality is very general and easy to use, it is often not tight (i.e., the bound is not very close to the true probability). This is because it does not use any other information about the distribution of $X$ other than its expectation.

Chebyshev's inequality provides a bound on the probability that a random variable deviates from its expected value by more than a specified number of standard deviations. Unlike Markov's inequality, which only requires non-negativity, Chebyshev's inequality does not require the random variable to be nonnegative, but it requires knowledge of both the mean and variance of the random variable. The inequality is applicable to any random variable with a finite mean and variance, regardless of the specific distribution.

\begin{corollary}[Chebyshev's inequality]\index{Chebyshev's inequality}
 Let $X$ be a random variable with expected value $E(X)$ and variance $Var(X)$. Then for every real number $t > 0$ we have that
\[
P \left( \left| X - E(X) \right| \geq t \right) \leq \frac{var(X)}{t^2}
\]
\end{corollary}
\begin{proof}
Applying Markov's inequality we have that
\[
P \left( \left| X - E(X) \right| \geq t \right) = P \left( \left( X - E(X) \right)^2 \geq t^2 \right) \leq \frac{E \left( \left( X - E(X) \right)^2 \right)}{t^2} = \frac{var(X)}{t^2}
\]
\end{proof}

Chebyshev's inequality provides insight into the concentration of a random variable around its mean. It quantifies the idea that most of the probability mass of a random variable lies within a certain range of its mean, with fewer values appearing further away. This concept is particularly useful in understanding the spread of a distribution and the likelihood of large deviations.

The last element we need to formally prove the law of large numbers is to introduce the concept of convergence in probability for random variables. Intuitively, a sequence of random variables converges in probability to a value $b$ if $X_n$ lies around $b$.

\begin{definition}
Let $X_{1}, X_{2}, \ldots$ be a sequence  of random variables. It is said that the sequence $X_{1}, X_{2}, \ldots$ \emph{converges in probability}\index{Convergence in probability} to $b$ , denoted by $X_{n} \overset{p}{\rightarrow}b$, if for every positive real number $\varepsilon>0$ we have that
\[
\lim_{n \rightarrow \infty} P \left( \left| X_{n} - b \right| < \varepsilon \right) = 1
\]
\end{definition}

Convergence in probability essentially means that as the sequence progresses (i.e., as $n$ increases), the random variables $X_n$ become arbitrarily close to the number $b$ with high probability. The distance between $X_n$ and $b$ can be made as small as desired, and the likelihood of $X_n$ being far from $b$ diminishes as $n$ increases.

Given the above definitions and partial results, we can now formally introduce and prove the law of large numbers.

\begin{theorem}[Law of Large Numbers]\index{Law of Large Numbers}
\label{th:law_large_numbers}
Let $X_1, \ldots, X_n$ be a random sample with finite expected value and finite variance, and let $\overline {X}_n = \frac {1}{n} \left( X_1 + \ldots + X_n \right)$ be the sample mean. Then we have that
\[
\overline {X}_n \overset{p}{\rightarrow} \mu
\]
\end{theorem}
\begin{proof}
Since the variables $X_1, \ldots, X_n$ are independent and identically distributed, we have that the variance of the sample mean is $Var \left( \overline {X}_n \right) = \frac{\sigma^2}{n}$ and that the mean is $E \left( \overline {X}_n \right) = \mu$ (see Proposition \ref{prop:sample_mean}). Applying the Chebyshev's inequality to the random variable $\overline {X}_n$ we have that
\[
P \left( \left| \overline {X}_n- \mu \right| \geq \varepsilon \right) \leq \frac{\sigma ^2}{n \varepsilon^2}.
\]
From there we can obtain
\[
P \left( \left| \overline {X}_n - \mu \right| < \varepsilon \right) = 1 - P \left( \left| \overline {X}_n - \mu \right| \geq \varepsilon \right) \geq 1 - \frac{\sigma ^2}{n \varepsilon^2}.
\]
As n approaches infinity, the above expression approaches 1. Aplying the definition of convergence in probability, we have that
\[
\overline{X}_n \overset{p}{\rightarrow} \mu
\]
\end{proof}

It is very important to note that the law of large numbers only works in case that the random variables are independent and identicaly distributed. Also, that the law is true only in the limit. If we have a finite number of random variables, the sample mean will be close to the distribution mean, but not necesarily equal.

Let's see an example of the use of the law of large numbers in practice.

\begin{example}
Consider the experiment of rolling a fair six-sided die $\Omega = \{1, 2, 3, 4, 5, 6\}$, being each side equally probable with $P(\omega)=1/6$ for $\omega \in \Omega$. Let $X:\Omega \rightarrow \mathbb{R}$ be a random variable that maps each side of the die with the number depicted on it. The probability mass function $f: range(X) \rightarrow [0, 1]$ of $X$ assigns $1/6$ to each value of $range(X)$. The expected value of $X$ is $E(X) = 3.5$.

Let's consider a random sample of size two, that is, trowing two dice (one after the other, so they are distinguishible), and call the corresponding random variables $X_1$ and $X_2$. The sample mean would be the random variable $\frac{X_1 + X_2}{2}: \Omega \times \Omega \rightarrow \mathbb{R}$ that maps each pair $(\omega_1, \omega_2) \in \Omega \times \Omega$ to the real value $X_1(\omega_1) + X_2(\omega_2) / 2$. 

The law of large numbers proposes to study the random variable $\frac{X_1 + X_2}{2} - E(X): \Omega \times \Omega \rightarrow \mathbb{R}$, whose probability mass function is depicted in Figure \ref{fig:law_large_numbers_1}. And lets, for example, compute the probability $P(  |\frac{X_1 + X_2}{2} - E(X)| < 1 )$ which is $0.444$.

\begin{figure}[t]
\centering
\begin{tikzpicture}
    \begin{axis}[
        ybar,
        bar width=12pt,
        ymin=0,
        xlabel={Random Variable},
        ylabel={Probability},
        xtick={-2.5,-2,...,2.5},
        xticklabel style={font=\footnotesize},
        yticklabel style={font=\footnotesize, /pgf/number format/fixed},
        xlabel style={font=\footnotesize},
        ylabel style={font=\footnotesize},
        xticklabel style={/pgf/number format/fixed},
        enlargelimits=0.15,
        ymajorgrids=true,
        grid style=dashed
    ]
    \addplot[fill={rgb,255:red,243;green,102;blue,25}] coordinates {
        (-2.5, 0.0278)
        (-2.0, 0.0556)
        (-1.5, 0.0833)
        (-1.0, 0.1111)
        (-0.5, 0.1389)
        (0.0, 0.1667)
        (0.5, 0.1389)
        (1.0, 0.1111)
        (1.5, 0.0833)
        (2.0, 0.0556)
        (2.5, 0.0278)
    };
    \end{axis}
\end{tikzpicture}
\caption{\label{fig:law_large_numbers_1}Probability mass function of the random variable $\frac{X_1 + X_2 }{2} - \mathbb{E}(X)$.}
\end{figure}

Now let's consider a random sample of size ten. In Figure \ref{fig:law_large_numbers_2} is depicted the probability mass function of the random variable $\frac{X_1 + X_2 + \ldots +  X_{10}}{10} - E(X): \Omega \times \ldots \times \Omega \rightarrow \mathbb{R}$. The probability $P(  |\frac{X_1 + X_2 + \ldots +  X_{10}}{10} - E(X)| < 1 )$ is $0.973$.

\begin{figure}[t]
\centering
\begin{tikzpicture}
    \begin{axis}[
        ybar,
        bar width=10pt,
        ymin=0,
        xlabel={Random Variable},
        ylabel={Probability},
        xtick={-2.5,-2,-1.5,-1,-0.5,0,0.5,1,1.5,2,2.5},
        xticklabel style={font=\footnotesize},
        yticklabel style={font=\footnotesize},
        xlabel style={font=\footnotesize},
        ylabel style={font=\footnotesize},
        xticklabel style={/pgf/number format/fixed},
        enlargelimits=0.15,
        ymajorgrids=true,
        grid style=dashed
    ]
    \addplot[fill={rgb,255:red,243;green,102;blue,25}] coordinates {
        (-2.5, 0.0010)
        (-2.0, 0.0100)
        (-1.5, 0.0400)
        (-1.0, 0.1200)
        (-0.5, 0.2100)
        (0.0, 0.2380)
        (0.5, 0.2100)
        (1.0, 0.1200)
        (1.5, 0.0400)
        (2.0, 0.0100)
        (2.5, 0.0010)
    };
    \end{axis}
\end{tikzpicture}
\caption{\label{fig:law_large_numbers_2}Probability mass function of the random variable $\frac{X_1 + X_2 + \ldots +  X_{10}}{10} - E(X)$.}
\end{figure}

\end{example}

It is important to clarify that the law of large numbers refers to the sample mean, that is $\frac {1}{n} \sum_{i=1}^{n} X_{i}$, and it is not necesarily true for other formulas, like for example, the deviation from the theoretical expected value $\sum_{i=1}^{n} X_{i} - n \times E(X)$ which not only it does not converge, but inscreases in absolute value as $n$ increases (see Example \ref{ex:gambler's_fallacy}).

\begin{example}
\label{ex:gambler's_fallacy}
If we toss a fair coin, the probability that the outcome will be head is equal to $1/2$. According to the law of large numbers, the proportion of heads in a large number of coin tosses will be close to $1/2$. However, the difference between the number of heads and tails will not be close to zero. In fact, the larger the number of coin tosses, the larger will be this difference. This is a highly conterintuitive fact, since most of the people think that the more we toss the coin, the closer will be the number of heads to the number of tails, which is not true.
\end{example}

% Central Limit Theorem

\subsection{Central Limit Theorem}

Let $X_1, \ldots, X_n$ be a sample of $n$ independent and identically distributed random variables with mean $\mu$ and variance $\sigma^2$. As we saw in the previous section, the law of large numbers states that the sample average $\overline {X}_n$ converges in probability to $\mu$ as $n$ increases. The central limit theorem states that the distribution of the difference between the sample average $\overline {X}_n$ and the population mean $\mu$, when multiplied by the factor $\sqrt {n}$ approximates to the normal distribution with mean $0$ and variance $\sigma^2 / n$. The theorem is true regardless of the shape of the original random vairables.

\begin{theorem}[Central Limit Theorem]
\label{th:central_limit_theorem_pdf}\index{Central limit theorem}
Let $X_{1}, \ldots, X_{n}$ be a random sample of size $n$ from a distribution with mean $\mu$ and finite variance $\sigma^{2}$. Define the standardized sum as
\[
Z_n = \frac{\overline{X}_{n}-\mu}{\sigma/\sqrt{n}},
\]
where $\overline{X}_{n} = \frac{1}{n} \sum_{i=1}^{n} X_i$ is the sample mean. Then, as $n \rightarrow \infty$, the probability density function of $Z_n$ converges to the probability density function of the standard normal distribution $\phi(x)$, that is,
\[
\lim_{n \rightarrow \infty} f_{Z_n}(x) = \phi(x),
\]
where $f_{Z_n}(x)$ is the probability density function of $Z_n$, and $\phi(x)$ is the probability density function of the standard normal distribution, given by
\[
\phi(x) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x^2}{2}\right).
\]
\end{theorem}
\begin{proof}
Let $X_{1}, \ldots, X_{n}$ be independent and identically distributed (i.i.d.) random variables with mean $\mu$ and finite variance $\sigma^2$. Define the sample mean as
\[
\overline{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i.
\]
We are interested in the distribution of the standardized variable
\[
Z_n = \frac{\overline{X}_n - \mu}{\sigma / \sqrt{n}}.
\]

First, consider the sum of the $X_i$'s, which we write as
\[
S_n = \sum_{i=1}^{n} X_i.
\]
The sample mean can then be written as
\[
\overline{X}_n = \frac{S_n}{n}.
\]
Thus, the standardized variable $Z_n$ becomes
\[
Z_n = \frac{S_n - n\mu}{\sigma\sqrt{n}}.
\]

To analyze the behavior of $Z_n$ as $n \to \infty$, we consider the sum $S_n = \sum_{i=1}^{n} X_i$. According to the Law of Large Numbers, $S_n/n$ converges to $\mu$, and hence $Z_n$ should converge to a normal distribution due to the nature of sums of independent random variables.

For large $n$, $Z_n$ can be approximated by considering the Taylor expansion of the exponential function. Since the $X_i$'s are i.i.d., the distribution of $S_n$ can be approximated by a normal distribution with mean $n\mu$ and variance $n\sigma^2$. The key idea is that as $n$ increases, the distribution of $Z_n$ approaches a normal distribution because the sum of i.i.d. random variables tends to be normally distributed by the Central Limit Theorem.

Let us consider the moment generating function (MGF) of $Z_n$. The MGF of $Z_n$ is given by:
\[
M_{Z_n}(t) = \mathbb{E}\left[\exp\left(t Z_n\right)\right] = \mathbb{E}\left[\exp\left(t \frac{S_n - n\mu}{\sigma\sqrt{n}}\right)\right].
\]
For large $n$, by the Central Limit Theorem, the MGF of $Z_n$ approaches that of a standard normal variable $N(0,1)$:
\[
M_{Z_n}(t) \approx \exp\left(\frac{t^2}{2}\right),
\]
which implies that $Z_n$ converges in distribution to $N(0,1)$ as $n \to \infty$.

Since $Z_n$ converges in distribution to $N(0,1)$, the probability density function of $Z_n$, denoted by $f_{Z_n}(x)$, must converge to the probability density function of a standard normal distribution $\phi(x)$, given by:
\[
\phi(x) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x^2}{2}\right).
\]
Thus, we have
\[
\lim_{n \rightarrow \infty} f_{Z_n}(x) = \phi(x).
\]

Therefore, as $n \to \infty$, the distribution of the standardized sum $Z_n$ approaches the standard normal distribution, completing the proof.
\end{proof}

The Central Limit Theorem is a fundamental concept in probability theory used in statistical analysis and inference. It allows us to compute the probability that the sample average is close to the distribution mean. It is important to recall the conditons for the central limit theorem to be true: the samples must be independent and identically distributed, the original distribution has to have a finite variance, and the sample size must be sufficiently large.

\begin{example}
Suppose a factory produces light bulbs, and the lifespan of each light bulb is a random variable with a mean of $\mu = 1000$ hours and a standard deviation of $\sigma = 50$ hours. The factory tests a random sample of $n = 36$ light bulbs to estimate the average lifespan of the bulbs produced in a particular batch. What is the probability that the sample mean lifespan of these 36 bulbs is between 990 and 1010 hours?

We can apply the Central Limit Theorem to solve this problem because we are dealing with the sample mean of a large number of independent, identically distributed random variables (lifespans of light bulbs). First, define the sample mean $\overline{X}_n$ as:
\[
\overline{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i,
\]
where $X_i$ represents the lifespan of the $i$-th light bulb in the sample, and $n = 36$ is the sample size.

By the Central Limit Theorem, for sufficiently large $n$, the distribution of the sample mean $\overline{X}_n$ approaches a normal distribution with mean $\mu$ and standard deviation $\sigma/\sqrt{n}$:
\[
\overline{X}_n \sim N\left(\mu, \frac{\sigma}{\sqrt{n}}\right) = N\left(1000, \frac{50}{\sqrt{36}}\right) = N\left(1000, \frac{50}{6}\right) = N\left(1000, 8.33\right).
\]

Next, we calculate the z-scores corresponding to 990 hours and 1010 hours using the formula:
\[
z = \frac{X - \mu}{\sigma/\sqrt{n}},
\]
where $X$ is the value for which we want to find the z-score.

For $X = 990$:
\[
z_{990} = \frac{990 - 1000}{8.33} \approx \frac{-10}{8.33} \approx -1.20.
\]

For $X = 1010$:
\[
z_{1010} = \frac{1010 - 1000}{8.33} \approx \frac{10}{8.33} \approx 1.20.
\]

Now, we use the standard normal distribution table (or a calculator) to find the probabilities corresponding to these z-scores:
\[
P(z_{990} \leq Z \leq z_{1010}) = P(-1.20 \leq Z \leq 1.20).
\]

From the standard normal distribution table:
\[
P(Z \leq 1.20) \approx 0.8849,
\]
\[
P(Z \leq -1.20) \approx 0.1151.
\]

Thus, the probability that the sample mean lifespan of the 36 light bulbs is between 990 and 1010 hours is:
\[
P(990 \leq \overline{X}_n \leq 1010) = P(-1.20 \leq Z \leq 1.20) = 0.8849 - 0.1151 = 0.7698.
\]

The probability that the sample mean lifespan of the 36 light bulbs falls between 990 and 1010 hours is approximately 76.98\%. This result demonstrates how the Central Limit Theorem allows us to use the normal distribution to approximate the sampling distribution of the sample mean, even when the original data is not normally distributed, as long as the sample size is sufficiently large.
\end{example}

%
% Section: References
%
\section*{References}

\cite{degroot1986probability} is a widely respected textbook in the fields of statistics and probability theory. First published in 1975, this book is known for its clear exposition of the fundamental concepts of probability and statistics, making it suitable for both beginners and those with some background in the subject. The book's approach balances theory and application, making it useful both for learning theoretical underpinnings and for applying probability and statistics to real-world problems.

\cite{childers2013philosophy} offers a comprehensive introduction to the foundational aspects of probability, with a focus on the philosophical questions it raises. Childers explores various interpretations of probability, including frequentist, propensity, classical, Bayesian, and objective Bayesian, and presents these complex ideas in a way that is accessible even to those without a strong background in probability or mathematics.

An example of the problems associated with the misinterpretation of expected value is the St. Petersburg Paradox\index{St. Petersburg Paradox}. Introduced by Nicholas Bernoulli in 1713, this paradox involves a gambling game with an infinite expected payoff, yet no reasonable person would pay more than \$25 to play it. Despite being three centuries old, the paradox continues to inspire new arguments and solutions in recent years (see \cite{huang2013three} for a historical review of the main proposed solutions).
