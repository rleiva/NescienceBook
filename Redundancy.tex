%
% CHAPTER: Surfeit
%

\chapterimage{Alexander_cuts_the_Gordian_Knot.pdf} % Chapter heading image

\chapter{Surfeit}
\label{chap:Redundancy}

\begin{quote}
\begin{flushright}
\emph{If you can't explain it simply,\\
you don't understand it well enough.}\\
Albert Einstein 
\end{flushright}
\end{quote}
\bigskip

{\color{red} TODO: Rewrite this intoduction}

Surfeit is a quantity that measures how redundant is a model of an entity. Intuitively, the more ignorant we are about an entity, the longer will be our current best model. Long models usually contain elements that are not needed, since a better understanding of the entity should allow us to remove all those unnecessary elements. 

We define the surfeit of a entity's model as the difference between the length of this model and the length of the best possible model for that entity. In the theory of nescience we assume that the theoretical limit of what can be known about an entity, that is, its perfect model, is the shortest possible model that allows us to fully reconstruct the entity. {\color{red} Again, clarify that entities are studied through the use of representations, and that surfeit is about representations}

The length of the shortest possible model of an entity is given by the Kolmogorov complexity of that entity. As we have seen, this quantity is not computable for the general case, that is, there is no algorithm that given the representation of an entity (a binary string) prints out its best possible model. Moreover, in practice, and given that our knowledge about entities is in general incomplete, we do not know the shortest possible model either. Surfeit is a quantity that has to be approximated in practice.

If we were able to come up with a perfect description of an entity, that model must be a random string, otherwise it would contain redundant elements that can be removed, and so, either it is not random or it is not perfect. By means of computing how far our current model is from being a random string we can estimate how far we are from having a perfect model.

{\color{red}

Merge the following ideas into the introduction:

The key idea of this book is that \emph{perfect knowledge implies randomness}. This is, in principle, a highly counterintuitive idea, since a lot of effort in science deals with the task to name, organize and classify our messy, chaotic, world. Even the kind of knowledge that explains how things work requires a previous ordering and classification. Science, apparently, is anything but random. Yet, this is not the case.

The Theory of Nescience is based on the fact that randomness effectively imposes a limit on how much we can know about a particular topic. Far from being a handicap, the proper understanding of this absolute epistemological limitation opens new opportunities in science and technology, both, to solve open problems, and to discover new interesting research topics.

Our common understanding suggests that random strings do not make any sense, since this is what randomness is all about. On the contrary, we will see that random descriptions are the ones which contain the maximum amount of information in the less space possible.
}

%
% Section: Surfeit
%

\section{Surfeit}
\label{sec:Definition_redundancy}

Given the length of a description of a representation of an entity, and the length of its shortest possible description, we can introduce a relative measure of how much unneeded effort we are using to explain the entity when using that model. We call this quantity \emph{surfeit}, and it will be part of our definition of nescience, that is, how much we do not know about that research entity.

\begin{definition}[Surfeit]
Given a representation $r \in \mathcal{B}^\ast$, and a valid description $d \in \mathcal{D}^\star_r$, we define the \emph{surfeit of the description $d$ with respect to the representation $t$}, denoted by $\sigma(d, r)$, as
\[
\sigma (d, r) = 1 - \frac{K(r)}{l(d)}
\]
\end{definition}

In our definition of surfeit we have used a relative measure instead of an absolute one (i.e., $l(d) - K(r)$), because besides to compare the surfeit of different descriptions for the same representation, we are also interested in to compare the surfeit of different entities. We prefer to use $K(r)$ instead of the equivalent $l \left( r^\star \right)$ in order to be consistent with the definition of inaccuracy provided in Section \ref{sec:inaccuracy:inaccuracy}.

Intuitively, the more ignorant we are about an entity, the longer will be our current best known model\footnote{The concept of \emph{current best model} will be introduced formally at Chapter \ref{chap:Nescience}.}, since a better understanding of that entity means that we should be able to remove all the redundant elements from that description.

{\color{red} A disclaimer that the descriptions may be non-valid}

\begin{example}
{\color{red} TODO: Provide an example to clarify the concept.}
\end{example}

The surfeit of a description is a number between $0$ and $1$.

\begin{proposition}
\label{prop:range_redundancy}
Let $r \in \mathcal{B}^\ast$ be a representation , and $d \in \mathcal{D}^\star_r$ one of its valid descriptions, then we have that $0 \leq \sigma(d, r) \leq 1$.
\end{proposition}
\begin{proof}
Given that $l\left( d \right)>0$ and that $K\left( r \right)>0$, since they are the lengths of non-empty strings, we only need to prove that $l\left( d \right) \geq K\left( r \right)$; however $l\left( d \right) < K\left( r \right)$ is a contradiction with the fact that $K\left( r \right)$ is the length of the shortest possible Turing machine that prints out $r$.
\end{proof}

{\color{red} Explain when it is the case that surfait is zero, and that there could be more than one description that makes surfeit zero for the same representation.}

Our definition of surfeit compares the length of a description with the Kolmogorov complexity of the representation, not with the Kolmogorov complexity of the description itself (i.e., $K\left( d \right)$). That is, surfeit is not a measure of the redundancy of a description. It might happen that we come up with an incompressible description (no redundant elements to remove), that it is not the shortest possible one that describes the representation (see Example \ref{ex:description_neural}). Such a description would not be redundant in the traditional sense, but it still will present some surfeit in the sense of the theory of nescience. Moreover, as we have said above, it migth happen that the description $d$ we are considering does not prefectly describes the representation $r$, that is, $d \in \mathcal{D}^\star_r$. In practice it is highly convenient to introduce the following alternative (we could say weaker) characterization of the concept of redundant model:

\begin{definition}[Redundancy]
Given a description $d \in \mathcal{D}$, we define the \emph{redundancy} of the description $d$, denoted by $\rho(d)$, as
\[
\rho(d) = 1 - \frac{K(d)}{l(d)}
\]
\end{definition}

The redundancy of a description $d$ is a quantity related to the description itself, and it does not depend on the representation $r$ being described.

\begin{example}
{\color{red} TODO: Provide an example to clarify the concept.}
\end{example}

We have that the redundancy of a description is  a number between $0$ and $1$.

\begin{proposition}
We have that $0 \leq \rho(d) \leq 1$ for all $d \in \mathcal{D}$.
\end{proposition}
\begin{proof}
Apply Proposition \ref{prop:kolmogorov_length}.
\end{proof}

{\color{red} Clarify when the redundancy is zero}

Finally, next proposition formalizes our intuition that the surfeit of a description is greater or equal than its redundancy.

\begin{proposition}
\label{prop:surfeit_comparison}
Let $r \in \mathcal{B}^\ast$ be a representation , and $d \in \mathcal{D}^\star_r$ one of its valid descriptions, then we have that $\rho(d) \leq \sigma(d, r)$.
\end{proposition}
\begin{proof}
Proving that $\rho(d) \leq \sigma(d, r)$ is equivalent to prove that $K(d) \geq K(r)$ for all $d$. Lets assume that there exist a $d$ such that $K(d) < K(r)$, that would mean there exists a Turing machine $\langle TM, a \rangle$ such that $TM(a)=r$ but $l(<TM, a>) < K(r)$. That is a contradiction with the fact that $K(r)$ is the length of the shortest possible Turing machine that prints $r$.
\end{proof}

It would be very nice if Proposition \ref{prop:surfeit_comparison} applies to all possible description. Unfortunately, the proposition is true only when we deal with valid descriptions (from $\mathcal{D}^\star_r$), as Example \ref{ex:surfeit_non_comparison} shows.

\begin{example}
\label{ex:surfeit_non_comparison}
{\color{red} TODO: Provide an example.}
\end{example}

%
% Section: Joint Surfeit
%

\section{Joint Surfeit}

{\color{red} This section requires a full rewrite}

The joint surfeit of two topics is given by our current understanding of both topics, and the shortest string that allows us to effectively reconstruct them.

\begin{definition}
Let $t,s \in \mathcal{T}$ be two different topics, and $d_{t,s}$ a joint description. We define the \emph{joint surfeit} of the description $d_{t,s}$, denoted by $\sigma(d_{t,s})$, as: 
\[
\sigma(d_{t,s}) = 1 - \frac{K(t, s)}{l \left( d_{t,s} \right)}
\]
\end{definition}

As it was the case of the concept of surfeit, joint surfeit, being a relative measure, is again a number between $0$ and $1$.

\begin{proposition}
We have that $0 \leq \sigma(d_{t,s}) \leq 1$ for all $t,s$ and all $d_{t,s}$.
\end{proposition}
\begin{proof}
Given that $K(t,s) \leq l(d_{t,s})$ we have that $\frac{K(t, s)}{l \left( d_{t,s} \right)} \leq 1$ and so, $1 - \frac{K(t, s)}{l \left( d_{t,s} \right)} \geq 0$. Also, since $\frac{K(t, s)}{l \left( d_{t,s} \right)} > 0$ (both quantities are positive integers), we have that $1 - \frac{K(t, s)}{l \left( d_{t,s} \right)} \leq 1$.
\end{proof}

When dealing with the joint surfeit of two topics, the order in which the topics are listed is not relevant.

\begin{proposition}
We have that $\sigma(d_{t,s}) = \sigma(d_{s,t})$ for all $t,s \in \mathcal{T}$.
\end{proposition}
\begin{proof}
Apply Propositions \ref{prop:kolmogorov_order} and \ref{prop:joint_order}.
\end{proof}

Our experience tell us that the more topics we include in a research, the less we understand the problem at hand. That is, the joint surfeit of two topics should be greater or equal than the surfeit of any of them isolated. Unfortunately, it is not the case that $\sigma(d_{t,s}) \geq \sigma(d_t)$ for all combinations of $d_{t,s}$ and $d_t$, since $d_{t,s}$ and $d_t$ are arbitrary descriptions of two different topics, $ts$ and $t$. We have to wait until Chapter \ref{chap:Nescience} when the concept of \emph{current best description} will be introduced to formalize our intuition.

In the meantime we can prove a weaker result. In the case that the description $d_{t,s}$ is just the concatenation of $d_t$ and $d_s$, the case we need in our methodology for the discovery of interesting questions (see Chapter \ref{chap:Interesting-Research-Questions}), we have that $\sigma(d_{t,s}) \geq \sigma(d_t)$.

\begin{proposition}
\label{prop:surfeit_joint_add}
Let $t, s \in \mathcal{T}$ two topics, and $ts$ the topic resulting as the concatenation of $t$ and $s$, and let $d_t$ and $d_s$ descriptions of $t$ and $s$ respectively. If $d_{t,s} = \langle d_t, d_s \rangle$ we have that  $\sigma(d_{t,s}) \geq \sigma(d_t)$  and $\sigma(d_{t,s}) \geq \sigma(d_s)$.
\end{proposition}
\begin{proof}
Given that Kolmogorov complexity is subadditive (see Proposition \ref{prop:additive_kolmogorov}) and the fact that $l(d_{t,s}) = l(d_t) + l(d_s)$.
\end{proof}

Intuitively, the joint surfeit of two topics in general should not be equal the sum of the individual surfeits, since it might happen that the topics partially overlap. However, as it was the case of Proposition \ref{prop:surfeit_joint_add}, we have to wait until we define the concept of current best description to fully formalize this concept.

In the same way we introduced the concept of redundancy of a description as a weaker version of the concept of surfeit, we can also introduce the concept of joint redundancy as a weaker version of the concept of joint surfeit.

\begin{definition}
Let $t,s \in \mathcal{T}$ be two different topics, and $d_{t,s}$ a joint description. We define the \emph{joint redundancy} of the description $d_{t,s}$, denoted by $\rho(d_{t,s})$, as: 
\[
\rho(d_{t,s}) = 1 - \frac{K\left( d_{t,s} \right)}{l \left( d_{t,s} \right)}
\]
\end{definition}

The joint redundancy satisfy the same properties that the joint surfeit.

\begin{proposition}

For all $t, s \in \mathcal{T}$ and all $d_t \in \mathcal{D}_t$, $d_s \mathcal{D}_s$ we have:

\renewcommand{\theenumi}{\roman{enumi}}
\begin{enumerate}
\item $0 \leq \rho(d_{t,s}) \leq 1$,
\item $\rho(d_{t,s}) = \rho(d_{s,t})$.
\item If $d_{t,s} = \langle d_t, d_s \rangle$, then $\rho(d_{t,s}) \geq \rho(d_t)$.
\end{enumerate}
\end{proposition}
\begin{proof}

\renewcommand{\theenumi}{\roman{enumi}}
\begin{enumerate}

\item Given that $K(d_{t,s}) \leq l(d_{t,s})$ and that both quantities are positive integers.

\item Apply Propositions \ref{prop:kolmogorov_order} and \ref{prop:joint_order}

\item Given that Kolmogorov complexity is subadditive (see Proposition \ref{prop:additive_kolmogorov}) and the fact that $l(d_{t,s}) = l(d_t) + l(d_s)$.

\end{enumerate}
\end{proof}

As it was the case of surfeit and redundancy, next Proposition shows that the joint surfeit is always greater or equal than the joint redundancy.

\begin{proposition}
We have that $\rho(d_t) \leq \sigma(d_t)$ for all $t \in \mathcal{T}$ and $d_t \in \mathcal{D}$.
\end{proposition}
\begin{proof}
Apply the fact that $K(d_t) \geq K(t)$ for all $d_t$ and $t$.
\end{proof}

Finally, we can extend our concepts of joint surface and joint redundancy to multiple, but fine, number of topics.

\begin{definition}
Let $t_1, t_2, \ldots, t_n \in \mathcal{T}$ a finite collection of topics, and $d_{t_1, t_2, \ldots, t_n}$ a joint description. We define the \emph{joint surfeit} of the description $d_{t_1, t_2, \ldots, t_n}$, denoted by $\sigma(d_{t_1, t_2, \ldots, t_n})$, as: 
\[
\sigma(d_{t_1, t_2, \ldots, t_n}) = 1 - \frac{K(t_1, t_2, \ldots, t_n)}{l \left( d_{t_1, t_2, \ldots, t_n} \right)}
\]
And the \emph{redundancy} of the description $d_{t_1, t_2, \ldots, t_n}$, denoted by $\rho(d_{t_1, t_2, \ldots, t_n})$, as:
\[
\rho(d_{t_1, t_2, \ldots, t_n}) = 1 - \frac{K(d_{t_1, t_2, \ldots, t_n})}{l \left( d_{t_1, t_2, \ldots, t_n} \right)}
\]
\end{definition}

It is easy to show that the properties of joint surface and joint redundancy apply to the case of multiple topics.

%
% Section: Conditional Surfeit
%

\section{Conditional Surfeit}

{\color{red} Rewrite this section, after Section 9.4 (conditional descriptions) is finished.}

We are interested into study how the surfeit of a description for a representation is affected when some (perfect) background knowledge is given. That is, we want to know the surfeit of a conditional description for a representation, what we call \emph{conditional surfeit}.

\begin{definition}
Let $t,s \in \mathcal{T}$ be two different topics, and let $m_{t \mid s^\star}$ a conditional model of $t$ given $s$. We define the \emph{conditional surfeit} of the model $m_{t \mid s^\star}$, denoted by $\sigma(m_{t \mid s^\star})$, as: 
\[
\sigma(m_{t \mid s^\star}) = 1 - \frac{K\left( t \mid s^\star \right)}{l \left( m_{t \mid s^\star} \right)}
\]
\end{definition}

This definition is required mostly for practical purposes, since given that our knowledge of $s$ is perfect, we can focus on studying what it is new in topic $t$, that is, in that part not covered by the assumed (perfect) background knowledge.

Conditional surfeit, being a relative measure, is a number between $0$ and $1$.

\begin{proposition}
We have that $0 \leq \sigma(m_{t \mid s^\star}) \leq 1$ for all $t,s$ and $m_{t \mid s^\star}$.
\end{proposition}
\begin{proof}
{\color{red} TODO: adapt this proof.}
Given that $K(t,s) \leq l(m_{t,s})$ we have that $\frac{K(t, s)}{l \left( m_{t,s} \right)} \leq 1$ and so, $1 - \frac{K(t, s)}{l \left( m_{t,s} \right)} \geq 0$. Also, since $\frac{K(t, s)}{l \left( m_{t,s} \right)} > 0$ (both quantities are positive integers), we have that $1 - \frac{K(t, s)}{l \left( m_{t,s} \right)} \leq 1$.
\end{proof}

Intuition tell us that the surfeit of a description could only decrease if we assume the background knowledge given by the description of another topic. This is because we require that this background knowledge must be a perfect description (it presents no surfeit). However, as it was the case of joint surfeit, we have to wait until Chapter \ref{chap:Nescience} to formalize this intuition.

In the same way we introduced the concept of redundancy of a description as a weaker version of the concept of surfeit, we can also introduce the concept of conditional redundancy as a weaker version of the concept of conditional surfeit.

\begin{definition}
Let $t,s \in \mathcal{T}$ be two different topics, and let $d_{t \mid s^\star}$ any conditional description of $t$ given $s$. We define the \emph{conditional surfeit} of the description $d_{t \mid s^\star}$, denoted by $\sigma(d_{t \mid s^\star})$, as: 
\[
\rho(d_{t \mid s^\star}) = 1 - \frac{K\left( d_{t \mid s^\star} \right)}{l \left( d_{t \mid s^\star} \right)}
\]
\end{definition}

Conditional surfeit is a relative measure, and so, a number between $0$ and $1$.

\begin{proposition}
We have that $0 \leq \rho(d_{t \mid s^\star}) \leq 1$ for all $t,s$ and all $d_{t,s}$.
\end{proposition}
\begin{proof}
Given that $K(d_{t \mid s^\star}) \leq l(d_{t \mid s^\star})$ we have that $\frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} \leq 1$ and so, $1 - \frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} \geq 0$. Also, since $\frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} > 0$ (both quantities are positive integers), we have that $1 - \frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} \leq 1$.
\end{proof}

Finally, we can extend our concepts of conditional surfeit and conditional redundancy to multiple, but fine, number of topics.

\begin{definition}
Let $t, s_1, s_2, \ldots, s_n \in \mathcal{T}$ be a finite collection of topics, and let $d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star}$ any conditional description of $t$ given $s_1, s_2, \ldots, s_n$. We define the \emph{conditional surfeit} of the description $d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star}$, denoted by $\sigma(d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star})$, as: 
\[
\sigma(d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star}) = 1 - \frac{K\left( t \mid s_1^\star, s_2^\star, \ldots,s_n^\star \right)}{l \left( d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star} \right)}
\]
And the \emph{conditional redundancy} of the description $d_{t_1, t_2, \ldots, t_n}$, denoted by $\rho(d_{t_1, t_2, \ldots, t_n})$, as:
\[
\rho(d_{t_1, t_2, \ldots, t_n}) = 1 - \frac{K(d_{t_1, t_2, \ldots, t_n})}{l \left( d_{t_1, t_2, \ldots, t_n} \right)}
\]
\end{definition}

It is easy to show that the properties of conditional surfeit and conditional redundancy apply to the case of multiple topics as well.

%
% Section: Surfeit of Areas
%

\section{Surfeit of Areas}

{\color{red} Review this section}

The concept of surfeit can be extended to research areas, to quantitative measure the amount of extra effort we are using to describe the topics of the area.

\begin{definition}
Let $A \subset \mathcal{T}$ be an area with known subset $\hat{A} = \{t_1, t_2, \ldots, t_n\}$, and let $d_{\hat{A}}$ be a description. We define the \emph{surfeit of the description} $d_{\hat{A}}$ as:
\[
\sigma \left( d_{\hat{A}} \right) = 1  - \frac{K( \langle t_1, t_2, \ldots, t_n \rangle )}{l \left( d_{\hat{A}} \right)}
\]
\end{definition}

As it was the case of the concept of redundancy, in general we do not know the complexity of the area $K(\hat{A})$, and so, in practice, it must be approximated by the complexity of the descriptions themselves $K(\hat{d}_{\hat{A}})$. However, in the particular case of areas, we could have also problems with the quantity $\hat{d}_{\hat{A}}$, since it requires to study the conditional descriptions of the topics included in the area.

\begin{definition}
Let $A \subset \mathcal{T}$ be an area with known subset $\hat{A} = \{t_1, t_2, \ldots, t_n\}$, and let $d_{\hat{A}}$ be a description. We define the \emph{weak redundancy of the description} $d_{\hat{A}}$ as:
\[
\rho(d_{\hat{A}}) =  1  - \frac{K \left( d_{\hat{A}} \right)}{l \left( d_{\hat{A}} \right)}
\]
\end{definition}

%
% Section: References
%

\section*{References}

The concept of redundancy has been also investigated in the context of information theory, since we are interested on using codes with low redundancy (see for example \cite{abramson1963information}).

