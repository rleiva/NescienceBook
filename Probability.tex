%
% CHAPTER: Probability Theory
%

\chapterimage{Galton_box} % Chapter heading image

\chapter{Discrete Probability}
\label{chap:Probability Theory}

\begin{quote}
\begin{flushright}
\emph{Inference from data is nothing other than the revision of an opinion\\
in the light of relevant new information.}\\
Thomas Bayes
\end{flushright}
\end{quote}
\bigskip

Probability theory is the branch of mathematics that studies random experiments and random phenomena. Probability assigns a numerical description to all the possible outcomes of an experiment, according to how likely is that these outcomes will occur. Even if the outcome of an experiment cannot be determined in advance, we can study its properties with probability theory and come to relevant results and conclusions. For example, we cannot predict the next number that will appear in a lottery game, but probability theory can help us to understand why it is not a good strategy to spend all our savings on lottery tickets with the goal of becoming rich.

Probability theory provides the mathematical foundations for statistical inference, one of the most relevant aproaches we have today for gathering knowledge based on the analysis of the results of carefully designed experiments. Probability theory also provides the foundation of machine learning, that is, the analysis of large volumes of data to derive intelligent algorithms.

In this chapter we are going to focus in the area of discrete probability. In this version of the theory, the possible outcomes of an event are finite, or at most, countably infinite. We are interested in the area of discrete probability, first because its applications in practice to the area of learning from data, and second, because discrete probability has some very interesting connections to theories used in this book: the length of optimal codes, the probability that a random machine will halt, and the derivation of universal distributions based on Kolmogorov complexity. All of these connection are relevant in our theory of nescience. We are going to study probability theory from a formal, axiomatic, point of view. We will start by formulating a very basic collection of fundamental axioms, and then we will derive the major results and properties from them. 

This chapter is a very brief overview of probability theory. We will cover only the most important techniques and results. The contents have been selected based on their applicability to the theory of nescience. For example, moment generating functions are not covered. For a more comprehensive introduction to probability theory, see the References section at the end of the Chapter.


%
% Section: Foundations of Probability Theory
%

\section{Foundations o Probability Theory}
\label{sec:probability_foundations}

The concept of \emph{probability} represents a profound intellectual challenge. Let us consider an instance where a dice is rolled and the objective is to compute the probability of an even number being the outcome. The dice comprises six distinct outcomes, and given that half of these are even numbers, we posit that the probability of yielding an even number is $3/6$ or equivalently $1/2$. This embodies the \emph{classical interpretation}\index{Classical interpretation of probability} of probability which asserts that in an experiment where all finite potential outcomes possess an equal likelihood of occurrence, the probability of an event equates to the count of favorable instances over the total number of instances. This interpretation, however, confronts the issue of circularity in its definition as "equally likely" essentially amounts to "possessing the same probability". An alternative method for probability assignment might be the implementation of the \emph{principle of indifference}\index{Principle of indifference}, which postulates that in the absence of any relevant evidence, all potential outcomes should possess identical probability. This principle, however, encounters a predicament when there exists evidence that contradicts the presumption of equivalence amongst all outcomes. For instance, how should probability be assigned when knowledge of a loaded dice suggests that not all sides possess an equal likelihood of occurrence?

The \emph{frequentist interpretation}\index{Frequentist interpretation of probability} of probability posits that one should roll the dice multiple times and contrast the frequency of even numbers with the total number of rolls. The fundamental notion is to execute the experiments repeatedly under similar conditions and assign the relative frequency of each outcome as its probability. This interpretation, however, faces two primary limitations. Firstly, the definition of repeating an experiment under "similar conditions" remains vague; if the conditions were truly identical, the results across all trials would invariably be the same. Secondly, the notion of a "large number of times" is undefined (technically, the experiment should be executed an infinite number of times). From a practical standpoint, implementing the frequentist interpretation is fraught with challenges. Some experiments, such as predicting the probability of a candidate winning an election, cannot be repeated numerous times. Furthermore, probability is defined in the context of a sequence of experiments, hence precluding the computation of the probability of a solitary outcome. Lastly, this interpretation necessitates the existence of a relative frequency limit, a condition which is not always satisfied, as illustrated by financial time series.

The \emph{subjective interpretation}\index{Subjective interpretation of probability}, representing a third approach to the concept of probability, suggests assigning probabilities to each event that reflect our degree of belief: the higher our conviction in the event's occurrence, the greater its assigned probability. However, it's important to note that not all potential probability allocations are viable; certain coherence rules need to be satisfied. For instance, when placing bets on the outcome of a dice roll, an assignment of probabilities that ensures a complete loss of money - a scenario known as a \emph{dutch book}\index{Dutch book} - would contravene the conditions of the subjective interpretation. It transpires that the conditions both necessary and sufficient to ensure a fair bet align with the axioms of probability introduced subsequently. Hence, we are free to assign any probabilities we desire to events, provided they remain consistent with the axioms of probability. A key drawback of the subjective interpretation is the inherent variability in individuals' degrees of belief. The \emph{Bayesian interpretation}\index{Bayesian interpretation of probability} of probability offers a solution: we commence with a provisional assignment of probabilities and, upon accruing further evidence, adjust our degree of belief or probability accordingly. With the accumulation of more evidence, estimated probabilities will converge to the true probabilities. Regardless, the task of assigning probabilities to an infinite number of events is typically unattainable for humans in general.

Currently, the notion of probability is defined axiomatically via the \emph{axiomatic interpretation}\index{Axiomatic interpretation of probability}. This implies that we abandon attempts to explicitly define probability and instead accept some of its properties as inherently true. Intuitively, a probability should be a value between $0$ and $1$, wherein an event with a zero probability is deemed impossible, and an event with a probability of one is certain to occur. Additional properties are required of probabilities. For instance, should two events $A$ and $B$ with probabilities $P \left( A \right)$ and $P \left( B \right)$ respectively, be disjoint, the probability of either $A$ or $B$ occurring should be $P \left( A \right) + P \left( B \right)$. If $A$ and $B$ could occur simultaneously and are independent (however that is defined), the probability of both events occurring concurrently should be $P \left( A \right) P \left( B \right)$. Moreover, the probability of $A$ occurring given that $B$ has already happened should be the fraction of the probability of $A$ that intersects with $B$. Thus, anything that satisfies these properties could be considered a probability. The issue with the axiomatic interpretation is that an abundance of constructs meet these requirements, including physical quantities such as normalized mass or normalized volume.

Probability theory is fundamentally concerned with the task of assigning a numerical value to specific events drawn from a sample space. The term "event" in this context may be somewhat misleading, as it intimates the occurrence of something, which is not always applicable. For instance, consider the sample space of all possible outcomes when tossing a fair coin. A subset of this sample space could be the empty set, which represents no coin toss happening at all. In the conventional understanding of an "event", this scenario is rather counterintuitive, as it does not correspond to something "happening". Nonetheless, for the sake of clarity, we will continue to employ the term "events" to designate what essentially are subsets.

\begin{definition}
Given $\left( \Omega, \mathcal{A} \right)$ as a field over a non-empty discrete set, $\Omega$ is referred to as the \emph{sample space}\index{Sample space}, its constituents are termed \emph{outcomes}\index{Outcome}, and the components of $\mathcal{A}$ are referred to as \emph{events}\index{Event}. Specifically, $\Omega$ is designated the \emph{certain event}\index{Certain event}, while the empty set $\varnothing$ is deemed the \emph{impossible event}\index{Impossible event}.
\end{definition}

As previously discussed in Section \ref{sec:sets}, given that $\left( \Omega, \mathcal{A} \right)$ is a field, we can deduce that $\Omega \in \mathcal{A}$ and that $\varnothing \in \mathcal{A}$. Additionally, the union of a finite collection of events constitutes an event $A_1 \cup A_2 \cup \ldots \cup A_n \in \mathcal{A}$, and the intersection of a finite collection of events is likewise deemed an event $A_1 \cap A_2 \cap \ldots \cap A_n \in \mathcal{A}$.

As alluded to in the introduction of this chapter, our principal interest lies in discrete mathematics, and hence, we will largely focus on probabilities pertaining to discrete sets (be they finite or countably infinite). An extension of the concept of probability to continuous sets would necessitate the utilization of $\sigma$-algebras\index{$\sigma$-algebra} of sets instead of fields and the application of measure theory. For example, consider a sample space representing the possible outcomes of a roll of a continuous, rather than discrete, die. Instead of the discrete outcomes 1, 2, 3, 4, 5, and 6, we might have any real number between 1 and 6. In this case, it doesn't make sense to talk about the probability of the outcome being exactly 2.5 (or any other specific real number), as there are infinitely many possible outcomes. Instead, we would talk about the probability of the outcome being in a certain interval, such as between 2 and 3.

The prevailing axiomatization utilized in the realm of probability theory is encapsulated within the framework of the \emph{Kolmogorov axioms}.

\begin{definition} (\emph{Kolmogorov's Axioms})\index{Kolmogorov's axioms}\label{Kolmogorov_axioms}
A \emph{probability}\index{Probability} is a real number $P(A) \in \mathbb{R}$ allocated to each event $A \in \mathcal{A}$ in the field $\left( \Omega, \mathcal{A} \right)$. This allocation adheres to the following axioms:

\medskip

\begin{description}
\item [Axiom 1] Each probability is nonnegative: $P(A) \geq 0$.
\item [Axiom 2] The probability of the certain event is one: $P(\Omega) = 1$.
\item [Axiom 3] For any finite sequence of disjoint events $A_1, A_2, \ldots, A_n$, the probability of the union of these events is the sum of their probabilities: $P(\cup_{i=1}^n A_i) = \sum_{i=1}^n P(A_i)$.
\end{description}

The triplet $\left( \Omega, \mathcal{A}, P \right)$ constitutes what is known as a \emph{probability space}\index{Probability space}.
\end{definition}

Despite their significance, the Kolmogorov axioms encounter certain complexities. One challenge is that they are incompatible with the reduction to first-order logic, as elaborated in Appendix \ref{apx:foundations_mathematics}. This incompatibility arises due to the fact that real numbers, to which probabilities are assigned, are not describable within the ambit of first-order logic. Furthermore, the axioms do not provide explicit guidance on the assignment of probabilities to events. Essentially, they set out the foundational principles that probabilities must conform to but do not dictate the process of how these probabilities should be determined.

\begin{example}
\label{ex:discrete_sample_space}
Consider a sample space $\Omega$ composed of $n$ equally probable elements. If we have an event $A \subset \Omega$ comprised of $d(A) = m$ elements, then the probability of event $A$ can be represented as $P(A) = m/n$.
\end{example}

We now venture to establish certain fundamental theorems concerning probabilities, beginning with the calculation of the complement of an event, which represents the probability of an event not occurring.

\begin{proposition}
For each event $A$, it holds true that $P \left( A^{c} \right) = 1 - P \left( A \right)$.
\end{proposition}
\begin{proof}
Sets $A$ and $A^c$ are disjoint, and their union $A \cup A^c$ equals $\Omega$. By applying Axiom 3, we infer that $P \left( A \cup A^c \right) = P \left( A \right) + P \left( A^c \right)$, and by applying Axiom 2, we conclude that $P \left( A \cup A^c \right) = P(\Omega) = 1$. Therefore, we can assert that $P \left( A \right) + P \left( A^c \right) = 1$.
\end{proof}

As a direct consequence of the aforementioned proposition, we can deduce the probability of the impossible event.

\begin{proposition}
The probability of the impossible event equals zero, that is, $P \left( \varnothing \right) = 0$.
\end{proposition}
\begin{proof}
Since $P \left( \varnothing \right) = 1 - P \left( \Omega \right) = 0$.
\end{proof}

As it was expected, sub-events (subsets) have smaller probabilities than events.

\begin{proposition}
If $A\subset B$ then $P \left( A \right) \leq P \left( B \right)$
\end{proposition}
\begin{proof}
The event $B$ can be decomposed as the union of two disjoint events $A$ and $A^c \cap B$, so we have that $P \left( B \right) = P \left( A \right) + P \left( A^c \cap B \right)$, that combined with the fact that  $P \left( A^c \cap B \right) \geq 0$ proves the proposition.
\end{proof}

As anticipated, sub-events (subsets) are associated with lesser probabilities than their corresponding events.

\begin{proposition}
Given that $A\subset B$, it follows that $P \left( A \right) \leq P \left( B \right)$.
\end{proposition}
\begin{proof}
The event $B$ can be dissected into the union of two disjoint events $A$ and $A^c \cap B$. Consequently, $P \left( B \right) = P \left( A \right) + P \left( A^c \cap B \right)$, which combined with the notion that  $P \left( A^c \cap B \right) \geq 0$, substantiates the proposition.
\end{proof}

With these fundamental elements established, we can demonstrate that probabilities range between zero and one.

\begin{proposition}
For each event $A$, $0 \leq P \left( A \right) \leq 1$.
\end{proposition}
\begin{proof}
By virtue of Axiom 1 and the consideration that $A \subset \Omega$ and consequently $P \left( A \right) \leq P \left( \Omega \right) = 1$.
\end{proof}

Axiom 3 provides the means to compute the probability of the union of disjoint events, however, it does not extend to scenarios involving non-disjoint events. The succeeding proposition illustrates the method for computing the probability of the union of non-disjoint events.

\begin{proposition}
For any two events $A$ and $B$, it follows that $P\left(A\cup B\right)=Pr\left(A\right)+Pr\left(B\right)-Pr\left(A\cap B\right)$.
\end{proposition}
\begin{proof}
The union of sets $A$ and $B$ can be represented as the union of two disjoint sets $A \cup B = B \cup \left( A \cap B^c \right)$. Given Axiom 3, we ascertain that
\[
P \left( A \cup B \right) = P \left( B \right) + P \left( A \cap B^c \right)
\]
Similarly, the set $A$ can be deconstructed as the union of the disjoint sets $A = \left( A \cap B \right) \cup \left( A \cap B^c \right)$. As a result, we get
\[
P \left( A \cup B^c \right) = P \left( A \right) - P \left( A \cap B \right)
\]
The combination of both expressions yields the desired result.
\end{proof}

The following equation extends to the scenario of $n$ events $A_1, \ldots, A_n$, employing the principle of inclusion-exclusion\index{Inclusion-exclusion principle} (see Section \ref{sec:counting}):
\begin{equation*}
\begin{split}
P \left( \bigcup_{i=1}^n A_i \right) & = \sum_{i=1}^n P \left( A_i \right) - \sum_{i<j} P \left( A_i \cap A_j \right) + \sum_{i<j<k} P \left( A_i \cap A_j \cap A_k \right) - \\
&  - \sum_{i<j<k<l} P \left( A_i \cap A_j \cap A_k \cap A_l \right) + \ldots +  (-1)^{n+1} P \left( A_1 \cap A_2 \cap \ldots \cap A_n \right) 
\end{split}
\end{equation*}
A probability function is characterized as a function that assigns to every possible event within a sample space its corresponding probability.

\begin{definition}
\label{def:probability_function}
Suppose $\left( \Omega, \mathcal{A} , P \right)$ denotes a probability space. A \emph{probability function}\index{Probability function} is a real-valued function $f : \mathcal{A} \rightarrow [0, 1]$ such that for every $A \in \mathcal{A}$, $f \left( A \right) = P \left( A \right)$ holds true.
\end{definition}

In Example \ref{ex:discrete_sample_space}, we introduced a probability space $\left( \Omega, \mathcal{A}, P \right)$ comprising $n$ equally probable elements. The probability function associated with this experiment is defined as $f : \mathcal{A} \rightarrow [0, 1]$, such that $f \left( A \right) = d\left( A \right)/n$ for all $A \in \mathcal{A}$.

%
% Section: Conditional Probability
%

\section{Conditional Probability}
\label{sec:probability_conditional}

The principle of conditional probability is a cornerstone within the discipline of statistical learning. The conventional interpretation of conditional probability posits it as the recalibrated probability of event $A$ following the occurrence of event $B$. This perspective, however, potentially implies a sequential or even causative linkage between events $B$ and $A$, a suggestion which may not necessarily hold validity.

\begin{example}
\label{ex:concurrent_events}
Suppose we are playing a game with a standard deck of 52 cards, and we draw two cards. Let event A be "drawing at least one heart" and event B be "drawing at least one queen". These two events are dependent since the occurrence of event B affects the probability of event A. However, these two events are not temporally related because the draw of the card happens at the same time - one event does not occur before the other. This example showcases the essence of dependency in probability theory without any temporal association between the events involved.
\end{example}

With reference to the axiomatization prescribed by Kolmogorov, conditional probability is initially introduced as a definitive construct. Certain scholars posit that, given its pivotal role within probability theory, conditional probability ought to be an attribute that is logically deduced from the foundational axioms. This perspective, naturally, necessitates an augmentation of Definition \ref{Kolmogorov_axioms}\index{Kolmogorov's axioms} with supplementary properties. Regrettably, there exists no agreed-upon method among mathematicians and philosophers regarding the manner in which this augmentation should be conducted.

\begin{definition}
Let $A$ and $B$ be two events such that $P \left( B \right) \neq 0$. The \emph{conditional probability}\index{Conditional probability} of $A$ given $B$, symbolized as $P \left( A \mid B \right)$, is elucidated as follows:
\[
P\left(A\mid B\right) = \frac{P\left(A\cap B\right)}{P\left(B\right)}
\]
\end{definition}

By virtue of satisfying the axioms, a conditional probability is, in itself, a probability. The conditional probability $P\left(A\mid B\right)$ is undefined in instances where $P\left(B\right)=0$.

The probability of two events transpiring concurrently (although not necessarily contemporaneously, as previously discussed in Example \ref{ex:concurrent_events}), given their respective conditional probabilities, is encapsulated by the formula $P \left( A \cap B \right) = P \left( A \mid B \right) P \left( B \right)$. This equation offers perhaps a more intuitive comprehension of the conditional probability concept. Indeed, there exist a number of authors who advocate for this interpretation to form the basis of the definition of conditional probability, as opposed to the quotient method.

The extension of this formula to accommodate $n$ events, termed the \emph{multiplication rule}\index{Multiplication rule}, is expressed as follows:
\begin{equation}\label{eq:multiplication_rule}
P \left( A_{1} \cap A_{2} \cap \ldots \cap A_{n} \right) = P \left( A_{1} \right) P \left( A_{2} \mid A_{1}\right) \ldots  P \left( A_{n} \mid A_{1}\cap A_{2} \cap \ldots \cap A_{n-1} \right)
\end{equation}

The notion of event independence holds significant importance in the realm of probability theory and statistical learning. 

\begin{definition}\label{independent_events}\index{Independent events}
Two events $A$ and $B$ are declared to be \emph{independent} if $P \left( A \cap B \right) = P \left( A \right) P \left(B \right)$.
\end{definition}

From an intuitive perspective, the events $A$ and $B$ are considered independent if witnessing the occurrence of event B does not influence the probability of event A. This characteristic can be logically inferred from the definition of independence.

\begin{proposition}
Given two events $A$ and $B$ such that $P \left( A \right) > 0$ and $P \left( B \right)>0$, $A$ and $B$ are independent if and only if $P \left( A \mid B\right) = P \left( A \right)$ and $P \left( B \mid  A \right) = P \left( B \right)$.
\end{proposition}
\begin{proof}
Assume $A$ and $B$ are independent, implying that $P \left( A \cap B \right) = P \left( A \right) P \left(B \right)$. Then,
\[
P \left( A \mid B \right) = \frac{P\left(A\cap B\right)}{P\left(B\right)} = \frac{P \left( A \right) P \left(B \right)}{P\left(B\right)} = P \left( A \right)
\]
Proceeding from the assumption that $P \left( A \mid B \right) = P \left( A \right)$, and utilizing the multiplication rule, it follows that
\[
P \left( A \cap B \right) =  P \left( A \mid B \right) P \left( B \right) = P \left( A \right) P \left( B \right)
\] 
The same conclusion is drawn if the roles of $A$ and $B$ are interchanged.
\end{proof}

Similar to the case of conditional probability, certain authors posit that independence, as a foundational concept in probability theory, ought to be a logical extension of the axioms, rather than being imposed as a definition.

The principle of independence can be expanded to accommodate multiple events: the events $A_{1}, \ldots, A_{n}$ are deemed to be independent (or mutually independent) if for every subset $A_{i_1}, \ldots, A_{i_j}$ comprising $j$ events $\left( j = 2, 3, \ldots, n \right)$, it holds true that $P \left( A_{i_1} \cap \ldots \cap A_{i_j} \right) = P \left( A_{i_1} \right) \ldots P \left( A_{i_j}\right)$.

\begin{example}
A degree of confusion often arises regarding the distinction between mutually exclusive (or disjoint) events and independent events. For two mutually exclusive events $A$ and $B$, the computation of the probability that $A$ will transpire given $B$ is somewhat nonsensical, since if $B$ occurs, $A$ is inherently impossible; analogously, discussing the conditional probability that $A$ will occur given $B$ when the probability of $B$ is zero is likewise flawed. However, as Definition \ref{independent_events} does not explicitly exclude the instance of $A$ and $B$ being mutually exclusive, we are compelled to conclude that two mutually exclusive events are independent if, and only if, the probability of at least one (or both) of them is zero.
\end{example}

An intriguing scenario arises when events $A$ and $B$ are not independent, yet attain independence contingent on the occurrence of another event $C$.

\begin{definition}
Consider $A$, $B$ and $C$ as events such that $P\left( B \cap C \right)>0$. $A$ and $B$ are considered \emph{conditionally independent}\index{Conditionally independent} given $C$ if $P\left(A \mid B \cap C \right) = P\left( A \mid C \right)$.
\end{definition}

\begin{example}
Consider the act of rolling two dice; it is reasonable to assert that the outcomes of the two dice are independent from each other. That is, observing the outcome of one die provides no insight into the outcome of the other die. However, suppose the first die results in a four, and a third event is introduced - that the sum of the outcomes is an odd number - then this additional piece of information narrows the potential outcomes for the second die to only odd numbers. This illustrates the point that two events can be independent, yet fail to maintain conditional independence.
\end{example}

The ensuing theorem presents Bayes' rule, a fundamental principle underpinning a significant statistical learning technique known as Bayesian inference (see Section \ref{sec:bayesian_inference}). 

\begin{theorem} (Bayes' Theorem)\index{Bayes' theorem} Let $A$ and $B$ represent two events with the condition that $P\left( B \right) \neq 0$. Consequently, we obtain that
\[
P \left( A \mid B \right) = \frac{P \left( B \mid A \right) P \left( A \right)}{P \left( B \right)}
\]
In this context, $P\left( A \right)$ is referred to as the \emph{prior probability}\index{Prior probability}, while $P\left( A \mid B \right)$ is deemed the \emph{posterior probability}\index{Posterior probability}.
\end{theorem}
\begin{proof}
As per the definition of conditional probability, $P \left( A \mid B \right) = P \left( A \cap B \right) / P \left( B \right)$ (given $P \left( B \right) \neq 0$) and $P \left( B \mid A \right) = P \left( A \cap B \right) / P \left( A \right)$ (provided $P \left( A \right) \neq 0$). By solving for $P(A\cap B)$ and substituting into the previous expressions for $P(A\mid B)$, we arrive at the theorem.
\end{proof}

As evident in the proof, Bayes' theorem is a direct derivative of the definition of conditional probability, notwithstanding our misgivings about conditional probability being a definition.

According to Bayesian inference, it facilitates the computation of how our degree of certainty about event $A$ (the prior probability $P\left( A \right)$) evolves when we acquire supplementary evidence via the occurrence of event $B$ (transforming into the posterior probability $P\left( A \mid B \right)$).

\begin{example}
Consider $E$ to be a disease affecting one in every million people, $P(E) = 1 \times 10^{-6}$, and let $+$ represent a test devised to detect the disease, with a failure rate of one in every thousand applications, $P(+ \mid E) = 999/1000$. We aim to determine the probability of disease presence if the test is positive $P(E \mid +)$. Upon employing Bayes' theorem, we find that:
\[
P(E \mid +) = \frac{P(+ \mid E) P(E)}{P(+)} = \frac{P(+ \mid E) P(E)}{P(+ \mid E) P(E) + P(+ \mid E^c) P(E^c)} = 0.001
\]
This implies that despite the test only failing once per thousand applications, it remains highly improbable that we have the disease following a positive result. This paradoxical outcome can be attributed to the higher probability of test failure $10^{-3}$ compared to the likelihood of disease occurrence $10^{-6}$. Practically, this issue is circumvented by applying a second test to individuals who received a positive result, as the probability of disease presence following two positive results is $0.5$ (under the assumption that the successive test repetitions are independent).
\end{example}

Bayes' theorem is most useful when the events involved are dependent and when new information about one event can update our understanding of the other event's probability.

\begin{example}
Suppose you're drawing a single card from a standard deck of 52 playing cards. Let Event $A$ be "drawing a red card" and Event $B$ be "drawing a queen". In this context, the use of Bayes' theorem to compute $P(A \mid B)$, the probability of drawing a red card given that a queen has been drawn, would not yield a meaningful result because the event $B$ provides no new information that would affect the probability of event $A$.
\end{example}

Bayes' theorem can indeed be extended to accommodate multiple events. Consider a set of events $A_{1}, \ldots, A_{k}$ such that $P\left( A_{j} \right)>0$ for all $j$ in the range of 1 to $k$. Suppose these events constitute a partition of the sample space $\Omega$. Now, let $B$ denote an event with the property that $P\left(B\right)>0$. In such a context, it can be deduced for each $i$ in the range of 1 to $k$ that the conditional probability $P\left(A_{i}\mid B\right)$ is given by the formula
\[
P\left(A_{i}\mid B\right)=\frac{P\left(B\mid A_{i}\right) P\left(A_{i}\right)}{\sum_{j=1}^{k} P\left(B \mid A_{j}\right) P\left(A_{j}\right)}
\]
This illustrates the capacity of Bayes' theorem to apply to a broader set of scenarios involving multiple events.

%
% Section: Random Variables
%

\section{Random Variables}
\label{sec:probability_random_variables}

A random variable is a function that assigns a real number to each possible outcome of an experiment. Therefore, it serves as a quantitative representation of the results of the experiment. Random variables are very useful since they offer a quantifiable means to examine the outcomes of the experiments and to discover their analytical properties. Such is the efficacy of random variables that a majority of statisticians primarily consider their investigations within the framework of random variables as opposed to probability spaces.

\begin{definition}
Let $\left( \Omega, \mathcal{A} , P \right)$ be a discrete probability space. A \emph{discrete random variable}\index{Discrete random variable}\index{Random variable} is a function $X : \Omega \rightarrow \mathbb{R}$ mapping from the sample space $\Omega$ to the real numbers.
\end{definition}

The terminology "random variable" might potentially lead to some confusion. Firstly, these are not variables in the conventional algebraic sense, but rather, they are functions. Secondly, they are not inherently random; it is the experiment that they represent which possesses randomness. Despite these points of potential confusion, we adhere to the established terminology.

Random variables hold increased utility when they encapsulate properties of the experiment. For instance, if the sample space consists of a school's student body, a random variable could associate each student with their respective height. Random variables also enable us to redistribute elements of the sample space into new events. For example, if two dice are rolled, a random variable could represent the sum of the dice's outcomes.

It is crucial to remember that we possess the liberty to assign a random variable to any sample space, even when the assignment might not seem intuitively meaningful. As an illustration, one could assign a numerical value to each possible color in a deck of cards, draw two cards randomly, and sum the assigned numbers of these two cards. Although such a setup may not yield a significant interpretation, it is nonetheless possible to calculate an array of probabilities based on this setup.

\begin{definition}
Let $X : \Omega \rightarrow \mathbb{R}$ be a discrete random variable, and let $\{ x_1, x_2, \ldots, x_i, \ldots \}$ be the range of $X$. The probability of $X$ being equal to $x_i$, expressed as $P\left(X = x_i \right)$, is given by $P\left( X = x_i \right)=P \left( \left\{ \omega \in \Omega \,:\, X \left( \omega \right) = x_i\right\} \right)$. Let $C \subset \mathbb{R}$ be a subset such that the set $\{ \omega \in \Omega \,:\, X \left( \omega \right) \in C\}$ constitutes an event. The probability of $X$ belonging to $C$, expressed as $P\left(X \in C \right)$, is given by $P\left( X \in C \right)=P \left( \left\{ \omega \in \Omega \,:\, X \left( \omega \right) \in C\right\} \right)$.
\end{definition}

The probability of a random variable $X$ essentially configures a probability space over the line of real numbers, specifically over the range of $X$. 

\begin{example}
\label{ex:probability_distribution_real_line}
Let $\Omega = \{1, 2, 3, 4, 5, 6\}$ be the sample space of possible outcomes when tossing a die, $\mathcal{A} = \mathcal{P} \left( \Omega \right)$ be all possible events of $\Omega$, and $P$ a probability that assigns $1/6$ to each single outcome in $\Omega$. Let $X: \Omega \rightarrow \mathbb{R}$ be a random variable defined as:
    \[
    X(\omega) = 
    \begin{cases} 
      0 & \text{if } \omega \text{ is even (2, 4, 6)}, \\
      1 & \text{if } \omega \text{ is odd (1, 3, 5)}.
    \end{cases}
    \]
This random variable maps the outcomes of the die toss to either 0 (if the outcome is even) or 1 (if the outcome is odd). For $C = \{0\}$, the probability $P(X \in C) = P(X = 0) = P(\{2, 4, 6\}) = 1/2$. For $C = \{1\}$ the probability $P(X \in C) = P(X = 1) = P(\{1, 3, 5\}) = 1/2$. Through this transformation, the original probability space has been mapped to the real numbers using the random variable $X$, establishing a new probability over $X$'s range.
\end{example}

Definition \ref{def:probability_function} introduced the concept of probability function for discrete probability spaces based on the probabilities of the events. Next definition extends the concept of probability function to discrete random variables.

\begin{definition}
Let $X$ be a random variable over a discrete probability space, and let $\{ x_1, x_2, \ldots \}$ be the range of $X$. The \emph{probability function}\index{Probability function} of the random variable $X$, abbreviated as p.f., is defined as the function $f : range \left( X \right) \rightarrow [0, 1]$ such that $f \left( x_i \right) = P \left( X = x_i \right)$.
\end{definition}

Of course, the probability function is defined only if $\{ \omega \in \Omega \,:\, X \left( \omega \right) = x_i\}$ is an event. Unless we say the contrary, and since we are dealing mostly with discrete probability spaces, we will assume that $\{ \omega \in \Omega \,:\, X \left( \omega \right) = x_i\}$ is an event for all the points that compose the range of $X$. The set of points for which the probability function is greater than zero, that is $\left\{ x \, : \, f \left( x \right) > 0 \right\}$, is called the \emph{support} of the distribution of $X$.

It is possible for two random variables to have identical probability functions but to differ in significant ways.

\begin{example}
Let $\Omega = \{H, T\}$ be the sample space of possible outcomes when tossing a coin, and $P$ a probability that assigns $1/2$ to each single outcome in $\Omega$. Let $X: \Omega \rightarrow \mathbb{R}$ be a random variable defined as: $X$ such that $X = 1$ if the coin shows Head and $X = 0$ if coin shows Tail. The individual probability distributions of $X$ is $P(X = 1) = P(Y = 1) = 0.5$. The random variables of this example and Example \ref{ex:probability_distribution_real_line} have same probability distribution even if they are different random variables.
\end{example}

Given the probability function of a random variable, we can derive the probabilty of any subset of the real line.

\begin{proposition}
Let $X$ be a discrete random variable with probability function $f$. The probability of each subset $C$ of the real line can be determined from the relation $P\left(X\in C\right)=\sum_{x_{i}\in C}f\left(x_{i}\right)$
\end{proposition}
\begin{proof}
Considering that each outcome in the sample space is associated with exactly one value in the range $\{ x_1, x_2, \ldots, x_i, \ldots \}$ of $X$, we have that:
\[
P\left(X\in C\right) = P \left( \left\{ \omega \in \Omega \,:\, X \left( \omega \right) \in C\right\} \right) = \sum_{x_{i}\in C} P\left( X = x_i \right) = \sum_{x_{i}\in C}f\left(x_{i}\right)
\]
\end{proof}

Next proposition outlines a fundamental property, that the total sum of probabilities across all possible outcomes of a discrete random variable is 1.

\begin{proposition}
Let $X$ be a discrete random variable with probability function $f$. If $\{ x_1, x_2, \ldots, \}$ is the range of $X$, then $\sum_{i=1}^{\infty}f\left(x_{i}\right)=1$.
\end{proposition}
\begin{proof}
Considering that $X$ is a total function, that each outcome in the sample space is associated with exactly one value in the range $\{ x_1, x_2, \ldots \}$, and given the axiomatic definition of probability we have that:
\begin{equation*}
\begin{split}
\sum_{i=1}^{\infty} f(x_i) & = f(x_1) + f(x_2) + \ldots = P\left( X = x_1 \right) + P\left( X = x_2 \right) + \ldots = \\
&  = P \left( \left\{ \omega \in \Omega \,:\, X \left( \omega \right) = x_1 \right\} \right) + P \left( \left\{ \omega \in \Omega \,:\, X \left( \omega \right) = x_2 \right\} \right) + \ldots = 1 
\end{split}
\end{equation*}
\end{proof}

The cumulative distribution function represents the probability that a random variable takes on a value less than or equal to a specific point.

\begin{definition}
The \emph{cumulative distribution function} (abbreviated c.d.f.) $F$ of a random variable $X$ is the function $F(x)=Pr(X\leq x)$ for all $-\infty < x < \infty$
\end{definition}

If $X$ follows a discrete distribution characterized by the probability function $f(x)$, its cumulative distribution function $F(x)$ will exhibit the following behavior: at each distinct value $x_i$ of $X$, $F(x)$ will display a jump equal to $f(x_i)$; between these distinct values, $F(x)$ remains unchanged.

The cumulative distribution function allows us to see how probabilities accumulate over the range of a random variable, offering insights into the overall distribution of the data.

\begin{example}
Let's $X$ be a discrete random variable that represents the grades of students in a class. Each grade is between 0 and 10. The probability that a student receives a grade of $x$ is given by the probability mass function $p(x)$. The cumulative distribution function represents the probability that a randomly selected student scores $x$ or less. For example, a value of $F(7) =
0.6$ would mean that there's a 60\% chance a student picked at random scored 7 or below.
\end{example}

The cumulative distribution function of a random variable is non-decreasing. 

\begin{proposition}
Let $F(X)$ be the cumulative distribution function of a random variable $X$. Then, if $x_{1}<x_{2}$ we have that $F\left(x_{1}\right)\leq F\left(x_{2}\right)$.
\end{proposition}
\begin{proof}
Given two values $x_1$ and $x_2$ where $x_1 < x_2$, the set of outcomes where $X \leq x_1$ is a subset of the outcomes where $X \leq x_2$.  Therefore, the probability of $X$ taking a value less than or equal to $x_1$ will be less than or equal to the probability of $X$ taking a value less than or equal to $x_2$. Then $F(x_1) \leq F(x_2)$.
\end{proof}

Next proposition delineates the asymptotic properties of the cumulative distribution function of a random variable, showcasing its bounds as we approach negative and positive infinity.

\begin{proposition}
Let $F(X)$ be the cumulative distribution function of a random variable $X$. Then, we have that $\lim_{x\rightarrow-\infty}F\left(x\right)=0$ and that $\lim_{x\rightarrow\infty}F\left(x\right)=1$.
\end{proposition}
\begin{proof}
As $x$ tends to negative infinity, the probability that the random variable $X$ takes on a value less than or equal to this increasingly smaller $x$ tends to zero. This is because there are fewer and fewer values (or none, depending on the specifics of the distribution) that $X$ can assume which are less than this increasingly negative $x$. Therefore:
\[
\lim_{x \rightarrow -\infty} F(x) = \lim_{x \rightarrow -\infty} P(X \leq x) = 0 
\]
As $x$ tends to positive infinity, the probability that the random variable $X$ takes on a value less than or equal to this increasingly larger $x$ approaches 1. This is because, given the unbounded increase of $x$, it encapsulates all possible values that $X$ can take on. Therefore:
\[
\lim_{x \rightarrow \infty} F(x) = \lim_{x \rightarrow \infty} P(X \leq x) = 1
\]
\end{proof}

The probability of $X$ exceeding $x$ is given by the complement of the cumulative distribution function at that point.

\begin{proposition}
Let $F(X)$ be the cumulative distribution function of a random variable $X$. Then, for every $x \in X$ we have that $P\left(X>x\right)=1-F\left(x\right)$.
\end{proposition}
\begin{proof}
The probability that $X$ takes on a value greater than $x$ plus the probability that $X$ takes on a value less than or equal to $x$ should sum up to 1. Given this, the probability that $X$ takes a value greater than $x$ is:
\[
P(X > x) = 1 - P(X \leq x)
\]
Using the definition of the cumulative distribution function, we get:
\[
P(X > x) = 1 - F(x) 
\]
\end{proof}

The following proposition establishes a relationship between the probabilities of a random variable \( X \) falling between two specific values and the corresponding differences in its cumulative distribution function values at those points.

\begin{proposition}
Let $F(X)$ be the cumulative distribution function of a random variable $X$. Then, for all values $x_1, x_2 \in X$ such that $x_1 < x_2$ we have that $P\left(x_1 < X \leq x_2 \right) = F\left(x_2\right) - F\left(x_1\right)$
\end{proposition}
 \begin{proof}
The probability that $X$ is less than or equal to $x_2$ is $F(x_2)$. From this, if we subtract the probability that $X$ is less than or equal to $x_1$, which is $F(x_1)$, we'll get the probability that $X$ falls strictly between $x_1$ and $x_2$:
\[
P(x_1 < X \leq x_2) = F(x_2) - F(x_1)
\]
\end{proof}

% Multivariate Distributions

\subsection*{Multivariate Distributions}

A multivariate probability distribution extends the concept of a probability distribution across multiple random variables, each with its own set of possible outcomes. Unlike univariate distributions that describe phenomena with a single random variable, multivariate distributions capture the relationships and dependencies between two or more variables. This allows for the exploration of complex phenomena where the outcome of interest is influenced by multiple factors simultaneously, providing insights into how these variables interact and impact the probability of various outcomes.

\begin{definition}
Let $X_1, X_2, \ldots, X_n$ be $n$ random variables, where $X_i : \Omega_i \rightarrow \mathbb{R}$ for all $i=1, \ldots, n$. The \emph{joint probability distribution}\index{Joint probability distribution} of $X_1, X_2, \ldots, X_n$ is defined as the collection of all probabilities of the form $P\left( \left( X_1, X_2, \ldots, X_n \right) \in C \right)$ for all sets $C \subset \mathbb{R}^n$ of real numbers such that $\left\{ \left( \omega_1, \omega_2, \ldots, \omega_n \right) \in \Omega_1 \times \ldots \times \Omega_n : \left( X_1 \left( \omega_1 \right), X_2 \left( \omega_2 \right), \ldots, X_n \left( \omega_n \right) \right) \in C \right\}$ is an event.
\end{definition}

The joint probability distribution of the random variables $X_1, X_2, \ldots, X_n$ defines a probability space in $\mathbb{R}^n$. If the random variables $X_1, X_2, \ldots, X_n$ each have a discrete distribution, then the joint distribution is also a discrete distribution.

\begin{definition}
Let $X_1, X_2, \ldots, X_n$ be $n$ random variables over discrete probability spaces. The \emph{joint probability mass function}\index{Joint probability mass function} of the random variables $X_1, X_2, \ldots, X_n$ is defined as the function $f : \text{range} \left( X_1 \right) \times \ldots \times \text{range} \left( X_n \right) \rightarrow [0, 1]$ such that $f \left( x_1, \ldots, x_n \right) = P \left( X_1 = x_1, \ldots, X_n = x_n \right)$.
\end{definition}

\begin{example}
A classic example of a bivariate discrete joint distribution involves rolling two six-sided dice. Let's define two random variables: $X_1$ is the outcome of the first die, and $X_2$ is the outcome of the second die. Both $X_1$ and $X_2$ have a discrete uniform distribution over the set $\{1, 2, 3, 4, 5, 6\}$. The joint distribution of $X_1$ and $X_2$ describes the probability of each possible pair of outcomes when the two dice are rolled. The joint probability mass function $f(x_1, x_2)$ for $X_1$ and $X_2$ can be expressed as:
\[
f(x_1, x_2) = P(X_1 = x_1, X_2 = x_2) = \frac{1}{36}, \quad \text{for} \ x_1, x_2 \in \{1, 2, 3, 4, 5, 6\}
\]
The joint distribution allows us to analyze the relationship between $X_1$ and $X_2$. For instance, we can compute the probability that the sum of the two dice is equal to a certain number, or that one die shows a higher number than the other.
\end{example}

Before exploring the properties of multivariate random variables, we will introduce the concept of a random vector. This concept simplifies notation and enhances clarity by grouping multiple random variables into a single entity.

\begin{definition}
A \emph{random vector}\index{Random vector} $\mathbf{X}$ is an ordered collection of $n$ random variables $X_1, X_2, \ldots, X_n$, where $X_i : \Omega_i \rightarrow \mathbb{R}$ for all $i=1, \ldots, n$. If the $n$ random variables are discrete, the random vector is called discrete random vector.
\end{definition}

Given the joint probability function of a vector of random variables, we can derive the probability of any subset within the multidimensional real space.

\begin{proposition}
Let $\mathbf{X}=\left(X_{1}, \ldots, X_{n}\right)$ be a discrete random vector with a joint probability function $f$. The probability of each subset $C$ of the $n$-dimensional real space can be determined from the relation $P\left(\mathbf{X} \in C\right)=\sum_{\mathbf{x} \in C}f\left(\mathbf{x}\right)$, where $\mathbf{x} = (x_{1}, \ldots, x_{n})$.
\end{proposition}
\begin{proof}
Considering that each outcome in the sample space is associated with exactly one value in the range of $\mathbf{X}$, we have that:
\[
P\left(\mathbf{X} \in C\right) = P \left( \left\{ \omega \in \Omega \,:\, \mathbf{X}(\omega) \in C\right\} \right) = \sum_{\mathbf{x}\in C} P\left( \mathbf{X} = \mathbf{x} \right) = \sum_{\mathbf{x}\in C}f\left(\mathbf{x}\right)
\]
\end{proof}

The next proposition outlines a fundamental property, that the total sum of probabilities across all possible outcomes of a vector of discrete random variables is 1.

\begin{proposition}
Let $\mathbf{X}=\left(X_{1}, \ldots, X_{n}\right)$ be a discrete random vector with a joint probability function $f$. If the range of $\mathbf{X}$ is represented as a set of vectors $\mathbf{x} = (x_{1}, \ldots, x_{n})$, then $\sum_{\mathbf{x}}f\left(\mathbf{x}\right)=1$.
\end{proposition}
\begin{proof}
Considering that $\mathbf{X}$ is a total function, that each outcome in the sample space is associated with exactly one value in the range of $\mathbf{X}$, and given the axiomatic definition of probability we have that:
\[
\sum_{\mathbf{x}} f(\mathbf{x}) = \sum_{\mathbf{x}} P\left( \mathbf{X} = \mathbf{x} \right) = P \left( \left\{ \omega \in \Omega_1 \,:\, \mathbf{X}_1(\omega) = \mathbf{x_1} \right\} \right) + P \left( \left\{ \omega \in \Omega_2 \,:\, \mathbf{X}_2(\omega) = \mathbf{x_2} \right\} \right) + \ldots = 1 
\]
\end{proof}

A particular interesting case of multivariate distribution is given by the sum of $n$ random variables. This is a highly confusing scenario, since we are not adding $n$ probability distributions, as the notation $X_1 + \ldots + X_n$ might suggest. Instead, we are defining a new random variable over the cartesian product of the original sample spaces.

\begin{definition}
Let $X_1, X_2, \ldots, X_n$ be $n$ random variables, where $X_i : \Omega_i \rightarrow \mathbb{R}$ for all $i=1, \ldots, n$ The sum distribution of $X_1, X_2, \ldots, X_n$, denoted by $X_1 + \ldots + X_n$, is defined as the random variable $X + \ldots + X_n : \Omega_1 \times \ldots \times \Omega_n \rightarrow \mathbb{R}$ that assigns to each $\left( \omega_1, \omega_2, \ldots, \omega_n \right) \in \Omega_1 \times \ldots \Omega_n$ the number $X_1 \left( \omega_1 \right) + X_2 \left( \omega_2 \right) + \ldots + X_n \left( \omega_n \right)$.
\end{definition}

The concept of sum of $n$ randon variables will be applied in the law of large numbers (see Theorem \ref{th:law_large_numbers}), one of the most important theorems in probability theory.

\begin{example}
Let $X: \Omega \rightarrow \mathbb{R}$ be the random variable representing the outcome of rolling a six-sided dice. The sum distribution corresponding to rolling three times a dice, denoted by $S = X + X + X$, is defined as the random variable $S: \Omega \times \Omega \times \Omega \rightarrow \mathbb{R}$ that assigns to each $\left( \omega_1, \omega_2, \omega_3 \right) \in \Omega \times \Omega \times \Omega$ the number $X \left( \omega_1 \right) + X \left( \omega_2 \right) + X \left( \omega_3 \right)$.
\end{example}

% Marginal Distribution

\subsection*{Marginal Probability Mass Function}

Given a multivariate discrete PMF, the marginal PMF of a subset of variables is derived by summing the joint PMF over all possible values of the other variables. This process essentially "marginalizes" out the variables not of interest, allowing focus on the PMF of a single variable or a subset of variables within the multivariate context.

\begin{definition}
Let $\mathbf{X}$ be an $n$-dimensional discrete random vector with a joint probability mass function $f_\mathbf{X}$. The \emph{marginal probability mass function}\index{Marginal probability mass function} of a subset of these variables, say $(X_1, X_2, \ldots, X_k)$ where $k \leq n$, is obtained by summing over the probabilities of the other variables $(X_{k+1}, \ldots, X_n)$. That is, for any subset of values $(x_1, x_2, \ldots, x_k)$ in the respective domains of $(X_1, X_2, \ldots, X_k)$,
\[
f_{X_1, X_2, \ldots, X_k}(x_1, x_2, \ldots, x_k) = \sum_{x_{k+1}} \sum_{x_{k+2}} \ldots \sum_{x_n} f_\mathbf{X}(x_1, x_2, \ldots, x_n)
\]
where $f_{X_1, X_2, \ldots, X_k}$ is the marginal probability mass function of the variables $(X_1, X_2, \ldots, X_k)$.
\end{definition}

This definition underscores the process of marginalization in a discrete setting, which is key to understanding and analyzing the behavior of specific variables within a larger multivariate framework.

\begin{example}
{\color{red} TODO: Provide an example based on random variables.}
\end{example}

While the marginal PMF of the random variables $X_1, \ldots, X_n$ can be obtained from their joint PMF by summing over the range of the other variables, the reverse process is not straightforward. Specifically, reconstructing the joint PMF of $X_1, \ldots, X_n$ from their marginal PMF alone is not feasible without extra information about the dependence between $X_1, \ldots, X_n$. This limitation arises because marginal PMF encapsulate only the individual behavior of each variable, omitting details about how the variables interact or are related.

\begin{example}
{\color{red} TODO: Provide an example based on random variables.}
\end{example}

A set of $n$ random variables $X_1, X_2, \ldots, X_n$ are considered independent if the occurrence of an event associated with any one of these variables does not influence the probability of an event associated with any other variable in the set. Independence among these variables indicates that there is no association or correlation among them, implying that knowing the outcome of one provides no information about the outcomes of the others.

\begin{definition}
A set of random variables $X_1, X_2, \ldots, X_n$ are said to be independent if, for every choice of sets $A_1, A_2, \ldots, A_n$ of real numbers such that each $\left\{ X_i \in A_i \right\}$ is an event, the joint probability of these events can be expressed as the product of their individual probabilities:
\[
P\left(X_1 \in A_1 \text{ and } X_2 \in A_2 \text{ and } \ldots \text{ and } X_n \in A_n\right) = P\left(X_1 \in A_1\right)P\left(X_2 \in A_2\right)\ldots P\left(X_n \in A_n\right)
\]
\end{definition}

The concept of independence for a random vector $\mathbf{X}$ simplifies the computation and understanding of joint probability distributions, particularly in complex problems involving multiple variables. It allows the joint probability distribution of the vector $\mathbf{X}$ to be expressed as the product of the individual marginal distributions of $X_1, X_2, \ldots, X_n$. 

\begin{proposition}
A set of random variables \(X_1, X_2, \ldots, X_n\) forming a random vector \(\mathbf{X}\) are independent if and only if the following is satisfied for all real numbers \(x_1, x_2, \ldots, x_n\): 
\[
f\left(x_1, x_2, \ldots, x_n\right) = f_1\left(x_1\right) f_2\left(x_2\right) \ldots f_n\left(x_n\right)
\]
where \(f\) is the joint probability mass function (PMF) or probability density function (PDF) of \(\mathbf{X}\), and \(f_i(x_i)\) is the marginal PMF or PDF of \(X_i\).
\end{proposition}

\begin{proof}
Assuming \(X_1, X_2, \ldots, X_n\) are independent, by definition, for any sets \(A_1, A_2, \ldots, A_n\) of real numbers, we have that 
\[
P(X_1 \in A_1 \text{ and } X_2 \in A_2 \text{ and } \ldots \text{ and } X_n \in A_n) = P(X_1 \in A_1)P(X_2 \in A_2)\ldots P(X_n \in A_n).
\]
Applying this definition to the PMFs or PDFs, for all \(x_1, x_2, \ldots, x_n\), the independence of \(X_1, X_2, \ldots, X_n\) implies 
\[
P(X_1 = x_1 \text{ and } X_2 = x_2 \text{ and } \ldots \text{ and } X_n = x_n) = P(X_1 = x_1)P(X_2 = x_2)\ldots P(X_n = x_n).
\]
Since \(P(X_1 = x_1 \text{ and } X_2 = x_2 \text{ and } \ldots \text{ and } X_n = x_n) = f(x_1, x_2, \ldots, x_n)\), and the marginal probabilities \(P(X_i = x_i)\) are given by \(f_i(x_i)\) respectively, we get 
\[
f(x_1, x_2, \ldots, x_n) = f_1(x_1) f_2(x_2) \ldots f_n(x_n).
\]

Conversely, assume that for all \(x_1, x_2, \ldots, x_n\), the joint PMF or PDF of \(\mathbf{X}\) can be expressed as 
\[
f(x_1, x_2, \ldots, x_n) = f_1(x_1) f_2(x_2) \ldots f_n(x_n),
\]
where \(f_i(x_i)\) are the marginal PMFs or PDFs of \(X_i\) respectively. We need to show that this implies \(X_1, X_2, \ldots, X_n\) are independent. The probability that \(X_1, X_2, \ldots, X_n\) simultaneously take on values \(x_1, x_2, \ldots, x_n\) is given by 
\[
P(X_1 = x_1 \text{ and } X_2 = x_2 \text{ and } \ldots \text{ and } X_n = x_n) = f(x_1, x_2, \ldots, x_n).
\]
Substituting the given condition into this expression yields 
\[
P(X_1 = x_1 \text{ and } X_2 = x_2 \text{ and } \ldots \text{ and } X_n = x_n) = f_1(x_1) f_2(x_2) \ldots f_n(x_n).
\]
By the definition of independence, if the joint probability of any \(x_1, x_2, \ldots, x_n\) equals the product of their individual probabilities for all possible values of \(x_1, x_2, \ldots, x_n\), then \(X_1, X_2, \ldots, X_n\) must be independent.
\end{proof}

\begin{example}
{\color{red} TODO: Provide an example based on random variables.}
\end{example}

% Conditional Distributions

\subsection*{Conditional Probability Mass Function}

The concept of conditional PMF offers a way to understand the probability of an event given that another event has occurred. Specifically, in the context of random variables, the conditional PMF of $Y$ given $X=x$ describes the PMF of $Y$ under the condition that $X$ takes a specific value $x$. This concept is pivotal for dissecting the interdependencies between random variables, allowing us to refine our probability assessments based on new information.

\begin{definition}
\label{def:conditional_probability_function}
Let $\mathbf{X}=\left(X_{1},\ldots,X_{n}\right)$ be an $n$-dimensional random vector, which is partitioned into two subvectors: $\mathbf{Y}$, a $k$-dimensional random vector consisting of $k$ random variables from $\mathbf{X}$, and $\mathbf{Z}$, an $(n-k)$-dimensional random vector containing the remaining $n-k$ random variables of $\mathbf{X}$. Let $f$ denote the joint probability mass function of the combined random vectors $\mathbf{Y}$ and $\mathbf{Z}$, and let $f_\mathbf{Z}$ represent the marginal probability mass function of $\mathbf{Z}$ across its $(n-k)$ dimensions. Provided that for any vector $\mathbf{z} \in \mathbb{R}^{n-k}$ the condition $f_\mathbf{Z}(\mathbf{z})>0$ holds, the \emph{conditional probability mass function} $g$ for $\mathbf{Y}$ given $\mathbf{Z}=\mathbf{z}$ is defined as:
\[
g_{\mathbf{Y}|\mathbf{Z}} \left(\mathbf{y}\mid\mathbf{z}\right)=\frac{f\left(\mathbf{y},\mathbf{z}\right)}{f_\mathbf{Z} \left(\mathbf{z}\right)}
\]
\end{definition}

\begin{example}
{\color{red} TODO: Provide an example based on random variables.}
\end{example}

Next proposition generalizes the multiplication rule (see Equation \ref{eq:multiplication_rule}) by combining marginal and conditional PMFs to derive the joint PMF for any configuration of random variables within a random vector, accounting for the complex dependencies and interactions among multiple variables.

\begin{proposition}
Let $\mathbf{X}$, $\mathbf{Y}$, $\mathbf{Z}$, $f_{\mathbf{Z}}(\mathbf{z})$ and $g_{\mathbf{Y}|\mathbf{Z}}(\mathbf{y}|\mathbf{z})$ be defined as in Definiton \ref{def:conditional_probability_function}. Then, for each $\mathbf{z} \in \mathbf{Z}$ such that $f_{\mathbf{Z}}(\mathbf{z})>0$ and each possible value of $\mathbf{y} \in \mathbf{Y}$, the joint probability mass function is given by:
\[
f(\mathbf{x}) = g_{\mathbf{Y}|\mathbf{Z}}(\mathbf{y}|\mathbf{z})f_{\mathbf{Z}}(\mathbf{z})
\]
where $\mathbf{x} $ represents a specific instantiation of the random vector $\mathbf{X}$.
\end{proposition}
\begin{proof}
The proposition essentially follows from the definition of conditional probability and the properties of joint and marginal probabilities. By the definition of conditional probability:
\[
P(\mathbf{Y}=\mathbf{y} \mid \mathbf{Z}=\mathbf{z}) = \frac{P(\mathbf{Y}=\mathbf{y}, \mathbf{Z}=\mathbf{z})}{P(\mathbf{Z}=\mathbf{z})}
\]
Rearranging this equation gives us the joint probability function in terms of the conditional and marginal probability functions:
\[
P(\mathbf{Y}=\mathbf{y}, \mathbf{Z}=\mathbf{z}) = P(\mathbf{Y}=\mathbf{y} \mid \mathbf{Z}=\mathbf{z}) P(\mathbf{Z}=\mathbf{z})
 \]
Or equivalently, in terms of the function notation:
\[
f(\mathbf{x}) = g_{\mathbf{Y}|\mathbf{Z}}(\mathbf{y}|\mathbf{z}) f_{\mathbf{Z}}(\mathbf{z})
\]
\end{proof}

Similarly, the conditional PMF of $\mathbf{Z}$ given $\mathbf{Y}=\mathbf{y}$, denoted as $g_{\mathbf{Z}|\mathbf{Y}}(\mathbf{z}|\mathbf{y})$, can be combined with the marginal PMF of $\mathbf{Y}$, $f_{\mathbf{Y}}(\mathbf{y})$, to yield the same joint probability mass function of $\mathbf{X}$:
\[
f(\mathbf{x}) = g_{\mathbf{Z}|\mathbf{Y}}(\mathbf{z}|\mathbf{y}) f_{\mathbf{Y}}(\mathbf{y})
\]

Next proposition provides a generalization of the law of total probability (see {\color{red} XXX }) to random variables. The law of total probability is used to calculate the probability of a single event based on a partition of the sample space into mutually exclusive events.

\begin{proposition}
Given a random vector $\mathbf{X} = (X_1, X_2, \ldots, X_n)$ where each $X_i$ for $i = 1, 2, \ldots, n$ is a discrete random variable, let $f_{X_i}(x_i)$ denote the marginal probability mass function of the variable $X_i$. If $g_{X_i|\mathbf{X}_{-i}}(x_i|\mathbf{x}_{-i})$ represents the conditional probability mass function of $X_i$ given the values of all other variables in the vector $\mathbf{X}$ except $X_i$ (denoted as $\mathbf{X}_{-i}$), then the marginal probability mass function of $X_i$ can be expressed as:
\[ f_{X_i}(x_i) = \sum_{\mathbf{x}_{-i}} g_{X_i|\mathbf{X}_{-i}}(x_i|\mathbf{x}_{-i}) \prod_{j \neq i} f_{X_j}(x_j) \]
where the summation $\sum_{\mathbf{x}_{-i}}$ is over all possible combinations of values for the random variables in $\mathbf{X}_{-i}$, and the product $\prod_{j \neq i} f_{X_j}(x_j)$ represents the multiplication of the marginal probability mass functionss of all other variables except $X_i$.
\end{proposition}
\begin{proof}
The marginal probability mass function of a random variable $X_i$ within the vector $\mathbf{X}$ is defined as the probability of $X_i$ assuming a particular value $x_i$, regardless of the specific values assumed by the other variables in $\mathbf{X}$. Mathematically, this can be expressed as:
\[
f_{X_i}(x_i) = P(X_i = x_i)
\]
To compute this probability, we consider all possible combinations of values for the variables in $\mathbf{X}_{-i}$, which represents the set of all variables in $\mathbf{X}$ excluding $X_i$. The joint probability mass function of the entire random vector $\mathbf{X}$, which includes both $X_i$ and $\mathbf{X}_{-i}$, can be expressed in terms of conditional probability mass functions as follows:
\[
f_{\mathbf{X}}(x_1, x_2, \ldots, x_n) = g_{X_i|\mathbf{X}_{-i}}(x_i|\mathbf{x}_{-i}) \cdot f_{\mathbf{X}_{-i}}(\mathbf{x}_{-i})
\]
Here, $g_{X_i|\mathbf{X}_{-i}}(x_i|\mathbf{x}_{-i})$ is the conditional probability mass function of $X_i$ given the specific values of $\mathbf{X}_{-i}$, and $f_{\mathbf{X}_{-i}}(\mathbf{x}_{-i})$ is the joint probability mass function of the variables in $\mathbf{X}_{-i}$.

To isolate the marginal PMF of $X_i$, we sum over all possible values of $\mathbf{x}_{-i}$, leveraging the law of total probability:
\[
f_{X_i}(x_i) = \sum_{\mathbf{x}_{-i}} f_{\mathbf{X}}(x_i, \mathbf{x}_{-i}) 
\]
\[
= \sum_{\mathbf{x}_{-i}} g_{X_i|\mathbf{X}_{-i}}(x_i|\mathbf{x}_{-i}) \cdot f_{\mathbf{X}_{-i}}(\mathbf{x}_{-i}) 
\]
Recognizing that $f_{\mathbf{X}_{-i}}(\mathbf{x}_{-i})$ can be further decomposed into the product of the marginal probability mass functions of the individual variables in $\mathbf{X}_{-i}$ (assuming independence for simplicity), we can write:
\[
f_{X_i}(x_i) = \sum_{\mathbf{x}_{-i}} g_{X_i|\mathbf{X}_{-i}}(x_i|\mathbf{x}_{-i}) \cdot \prod_{j \neq i} f_{X_j}(x_j)
\]
This equation illustrates that the marginal probability mass funcion of $X_i$ is a summation over all possible combinations of values for the variables excluding $X_i$, weighted by the conditional probability of $X_i$ given those values and the joint probabilities of those values.
\end{proof}

The law of total probability is utilized to decompose the probability of an event into a sum of probabilities conditional on a partition of the sample space, making it useful for problems where direct calculation is impractical.

\begin{example}
{\color{red} TODO: Provide an example based on random variables}
\end{example}

Bayes' theorem (see {\color{red} XXX}) provides a way to update our probability estimates for a hypothesis given new evidence. For random vectors, the theorem can be generalized to accommodate the multi-dimensional nature of the variables involved. 

\begin{theorem}
Let $\mathbf{X} = (X_1, X_2, \ldots, X_n)$ and $\mathbf{Y} = (Y_1, Y_2, \ldots, Y_m)$ be two random vectors representing different sets of random variables. Suppose we are interested in the conditional probability distribution of $\mathbf{X}$ given observed values of $\mathbf{Y}$, denoted as $\mathbf{y} = (y_1, y_2, \ldots, y_m)$. The generalized Bayes' theorem for random vectors can be stated as:
\[
P(\mathbf{X}=\mathbf{x}|\mathbf{Y}=\mathbf{y}) = \frac{P(\mathbf{Y}=\mathbf{y}|\mathbf{X}=\mathbf{x})P(\mathbf{X}=\mathbf{x})}{P(\mathbf{Y}=\mathbf{y})}
\]
where $P(\mathbf{X}=\mathbf{x}|\mathbf{Y}=\mathbf{y})$ is the conditional probability of $\mathbf{X}$ given $\mathbf{Y}=\mathbf{y}$, $P(\mathbf{Y}=\mathbf{y}|\mathbf{X}=\mathbf{x})$ is the conditional probability of $\mathbf{Y}$ given $\mathbf{X}=\mathbf{x}$,  $P(\mathbf{X}=\mathbf{x})$ is the prior probability of $\mathbf{X}$, and $P(\mathbf{Y}=\mathbf{y})$ is the marginal probability of $\mathbf{Y}$, which can also be expressed using the law of total probability as:
\[
P(\mathbf{Y}=\mathbf{y}) = \sum_{\mathbf{x}} P(\mathbf{Y}=\mathbf{y}|\mathbf{X}=\mathbf{x})P(\mathbf{X}=\mathbf{x})
\]
for discrete random vectors.
\end{theorem}
\begin{proof}
{\color{red} TODO}
\end{proof}

This generalized form of Bayes' theorem allows us to update our belief about the probability distribution of a set of random variables $\mathbf{X}$ based on new information encapsulated in another set of random variables $\mathbf{Y}$. It emphasizes the interplay between the prior information we have about $\mathbf{X}$, the likelihood of observing $\mathbf{Y}=\mathbf{y}$ given $\mathbf{X}$, and the evidence provided by the actual observation of $\mathbf{Y}=\mathbf{y}$.

\begin{example}
{\color{red} TODO: Provide an example based on random variables}
\end{example}

Building on the familiar concept of independence between random variables (see {\color{red} XXX}), an important extension is the idea of conditional independence. This concept comes into play when the independence of a set of random variables is considered in the context of being conditioned on another set of variables.

\begin{definition}
Let $\mathbf{Z}$ be a random vector with joint probability function $f_\mathbf{Z} \left( \mathbf{z} \right)$. The variables of the random vector $\mathbf{X} = \left( X_{1}, \ldots, X_{n} \right)$ are \emph{conditionally independent}\index{Conditional independence} given $\mathbf{Z}$ if, for all $\mathbf{z}$ such that $f_\mathbf{Z}\left(\mathbf{z}\right)>0$, we have $g_{\mathbf{X} \mid \mathbf{Z}} \left(\mathbf{x}\mid\mathbf{z}\right)=\prod_{i=1}^{n}g_{X_i}\left(x_{i}\mid\mathbf{z}\right)$ where $g_{\mathbf{X} \mid \mathbf{Z}} \left( \mathbf{x} \mid \mathbf{z} \right)$ stands for the conditional probability mass function of $\mathbf{X}$ given $\mathbf{Z}=\mathbf{z}$ and $g_{X_i}\left(x_{i}\mid\mathbf{z}\right)$ stands for the conditional probability mass function of $X_{i}$ given $\mathbf{Z}=\mathbf{z}$.
\end{definition}

Let's consider a practical example involving three discrete random variables $X$, $Y$, and $Z$, where $X$ and $Y$ are not independent, but they become conditionally independent given the third random variable $Z$.

\begin{example}
{\color{red} TODO: Provide a practical example involving three discrete random variables $X$, $Y$ , and $Z$. The domain of $X$, $Y$ and $Z$ must be infinitely countable. $X$ and $Y$ must be dependent, and they become conditionally independent given $Z$.}
\end{example}

\subsection*{Random Samples}

The concept of a random sample is pivotal in the field of statistical learning, as assuming a set of random variables constitutes a random sample greatly simplifies the mathematical underpinnings of inferential methods. Nevertheless, the criteria for a collection of variables to be considered a random sample are not always met in real-world scenarios.

\begin{definition}
Given a probability distribution $f$, and let $X_1, X_2, \ldots, X_n$ be $n$ random variables. These variables $X_1, X_2, \ldots, X_n$ constitute a \emph{random sample} from the distribution $f$ if they are independent and each has the marginal distribution $f$.
\end{definition}

When the random variables $X_1, X_2, \ldots, X_n$ constitute a random sample from the distribution $f$, they are described as \emph{independent and identically distributed (i.i.d.)}. The term \emph{sample size} refers to the number $n$.

The joint distribution $g$ of a random sample is defined as:
\[
g \left( x_1, x_2, \ldots, x_n \right) = f \left( x_1 \right) f \left( x_2 \right) \ldots f \left( x_n \right)
\]
for every point $\left( x_1, x_2, \ldots, x_n \right) \in \mathcal{R}$.

\begin{example}
Imagine a researcher aims to estimate the average height of students at a university. The university has $N$ students, with $N$ very large, making it impractical to measure the height of every student. The researcher decides to select a sample of $n$ students, where $n << N$, ensuring that every student has an equal chance of being included in the sample. The set of height measurements can be represented as a random sample $\{X_1, X_2, \ldots, X_n\}$ from the population distribution of student heights, where each $X_i$ is a random variable representing the height of the $i$-th student in the sample.
\end{example}

%
% Section: Expectation
%

\section{Characterizing Distributions}
\label{sec:probability_expectation}

A \emph{measure of central tendency} is a number derived from a probability distribution, intended as a summary of that distribution. The most common measures of central tendency in use are the mean and the median. Each of these measures provides a different approach to characterize distributions. It is also common to use \emph{metrics of dispersion} to describe the variability of a distribution around the measures of centrality. We will review two metrics of dispersion, the variance and the standard deviation. The metrics of disperson can also be used in case of bivariate distributions, under the names of covariance and correlation, to measure the \emph{statistical relationship} between two random variables. All these measures allow us to summarize and compare distributions.

% Subsection: Measures of Central Tendency

\subsection{Measures of Central Tendency}

The most commonly used measure of central tendency is the \emph{expected value}. The expected value of a collection of outcomes is the weighted average of these outcomes, where the weights are the probabilities of each outcome. The expected value extends the concept of the mean of a population, as discussed in Section \ref{sec:mean}, to continuous cases.

\begin{definition}\label{probability:expectation}
Let $X$ be a discrete random variable whose probability function is $f$. The \emph{mean} of $X$, denoted by $E\left(X\right)$, is defined as:
\[
E\left(X\right)=\sum_{x}xf\left(x\right)
\]
\end{definition}

Of course, Definition \ref{probability:expectation} only makes sense if the summation converges. It is also possible that the expected value is infinite, but in this book we are only interested in finite expected values. We have defined the concept of expected values based on random variables. In this sense, the definition of expected value only takes into account the distribution of the random variables, not the original outcomes. That is, two different random variables with the same distribution will have the same expected values. The name expected value is a little bit misleading, since for the majority of the discrete distributions, the expected value is not one of the possible values of the distribution, i.e., the expected value is not expected at all. For example, if we throw a dice, the expected value would be 3.5. This undesired property of something known as expected value has generated a lot of confusion in scientific research. The expected value, as it was the case of the mean, can be greatly affected by a small change in the probability assigned to a large value of $X$.

It is worth to clarify the difference between mean and expected value. In case of continuous statistical variables, the mean is calculated based on the frequency of each attribute value observed in the population. Typically, we aggregate individuals into class intervals, and calculate the mean using class marks. However, for random variables, the expectation is computed directly from the theoretical probabilities associated with these intervals. Consequently, the mean and the theoretical expected value may differ due to measurement errors, rounding errors of empirical values, or inaccuracies in the theoretical probability model selected. In the context of a discrete probability space $\left( \Omega, \mathcal{A}, P \right)$, which is the type of space we focus on in this book, the concepts of the mean and expected might provide the same value. This equivalence holds as long as the probability space $\left( \Omega, \mathcal{A}, P \right)$ is sufficiently granular, that is, for every $\omega$ in $\Omega$ the singleton set $\{ \omega \}$ belongs to $\mathcal{A}$, and probabilities resembles frequencies. Neither the mean nor the expected value makes sense in the case of categorical variables.

The expectation of the linear combination of $n$ random variables is the linear combination of their expectations.

\begin{proposition}
Let $X_{1}, \ldots, X_{n}$ be $n$ independent discrete random variables with expectations $E\left(X_{i}\right)$, and let $a_1, \ldots, a_n$ and $b$ constants, then
\[
E\left(a_{1}X_{1}+\ldots+a_{n}X_{n}+b\right)=a_{1}E\left(X_{1}\right)+\ldots+a_{n}E\left(X_{n}\right)+b
\]
\end{proposition}
\begin{proof}
We have that
\begin{multline}
E \left(a_1 X_1 + \ldots + a_n X_n +b \right) = 
\sum_{x_1} \ldots \sum_{x_n} \left(a_ 1 x_1 + \ldots + a_n x_n + b  \right) f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} \ldots \sum_{x_n} a_1 x_1 f\left(x_1, \ldots, x_n \right) + \ldots + \sum_{x_1} \ldots \sum_{x_n} a_n x_n f\left(x_1, \ldots, x_n \right) + \sum_{x_1} \ldots \sum_{x_n} b f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} a_1 x_1 f\left(x_1\right) + \ldots + \sum_{x_n} a_n x_n f\left( x_n \right) + b = 
a_1 \sum_{x_1} x_1 f\left(x_1\right) + \ldots + a_n \sum_{x_n} x_n f\left( x_n \right) + b = \\
a_1 E\left(X_1\right) + \ldots + a_n E\left(X_n\right) + b
\end{multline}
\end{proof}

The expectation of the product of $n$ independent random variables is the product of the individual expectations.

\begin{proposition}
Let $X_{1}, \ldots, X_{n}$ be $n$ independent discrete random variables with expectations $E\left(X_{i}\right)$, then:
\[
E\left(\prod_{i=1}^{n}X_{i}\right)=\prod_{i=1}^{n}E\left(X_{i}\right)
\]
\end{proposition}
\begin{proof}
We have that
\begin{multline}
E \left(X_1  \cdot \ldots \cdot X_n  \right) = 
\sum_{x_1} \ldots \sum_{x_n} \left(x_1 \cdot \ldots \cdot x_n  \right) f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} \ldots \sum_{x_n} x_1 f\left(x_1, \ldots, x_n \right) \cdot \ldots \cdot \sum_{x_1} \ldots \sum_{x_n} x_n f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} x_1 f\left(x_1\right) \cdot \ldots \cdot \sum_{x_n} x_n f\left( x_n \right) = 
E \left( X_1 \right) \cdot \ldots \cdot E \left( X_n \right)
\end{multline}
\end{proof}

The expectation of the product of non-independent random variables is not necesarily equal to the product of their individual expectations.

In the area of statistical inference it is also highly convenient to compute the sample mean, as the average of $n$ random variables. In particular, we will compute the sample mean of random samples.

\begin{definition}
Let $X_1, X_2, \ldots, X_n$ be $n$ random variables. The \emph{sample mean}, denoted by $\bar{X}_n$, is defined as:
\[
\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i
\]
\end{definition}

Do not confuse $\frac{1}{n} \left( X_1 + X_2 + \ldots + X_n \right)$, which is a probability distribution, with $E \left( X_1 + X_2 + \ldots + X_n \right)$, which is a real number.

% The Median

\subsubsection*{The Median}

We have seen that the mean of a probability distribution is the center of gravity of that distribution. The actual center of the distribution is called  the \emph{median}.

\begin{definition}
Let $X$ be a discrete random variable. Every number $m$ that satisfy the following properties is called a median of the distribution of $X$:
\[
Pr\left(X\leq m\right)\geq1/2 \quad and \quad Pr\left(X\geq m\right) \geq 1/2
\]
\end{definition}

The median divides a probability distribution in two equal parts. A distribution could have more than one median. And, on the contrary of what happens in case of the expectation, every distribution must have at least one median. An advantage of the median over the mean is that we can move a value $x$ larger to the median to any arbitrary larger value, and the median will be remain the same. 

\subsection{Measures of Dispersion}

Definitions of the Variance and the Standard Deviation

\begin{definition}
Let X be a random variable with finite mean and $\mu=E\left(X\right)$. The variance of X, denoted by $Var\left(X\right)$, is defined as follows: $Var\left(X\right)=E\left[\left(X-\mu^{2}\right)\right]$
\end{definition}

{\color{red} If X has infinite mean or if the mean of X does not exist, we say that $Var\left(X\right)$ does not exist. The standard deviation of X is the nonnegative square root of $Var\left(X\right)$ if the variance exists. It is common to denote the standard deviation by the symbol $\sigma$, and the variance by $\sigma^{2}$. Variance depeds only on the distribution.}

\begin{proposition}
For every random variable X, $Var\left(X\right)=E\left(X^{2}\right)-\left[E\left(X\right)\right]^{2}$.
\end{proposition}
\begin{proof}
\end{proof}

{\color{red} The variance (as well as the standard deviation) of a distribution provides a measure of the spread or dispersion of the distribution around its mean $\mu$. The variance of a distribution, as well as its mean, can be made arbitrarily large by placing even a very small but positive amount of probability far enough from the origin on the real line.}

Properties of the Variance

\begin{proposition}
For constants a and b, let $Y=aX+b$, then $Var\left(Y\right)=a^{2}Var\left(X\right)$
and $\sigma_{Y}=\left|a\right|\sigma_{X}$.
\end{proposition}
\begin{proof}
\end{proof}

{\color{red} If $X_{\text{1}},\ldots,X_{n}$ are independent random variables with finite means, and if $a_{1},\ldots,a_{n}$ and b are arbitrary constants, then $Var\left(a_{1}X_{1}+\ldots+a_{n}X_{n}+b\right)=a_{1}^{2}Var\left(X_{1}\right)+\ldots+a_{n}^{2}Var\left(X_{n}\right)$}

\begin{example}
The Variance of a Binomial Distribution

The variance of a random variable X with a binomial distribution of n samples with probability p is $Var\left(X\right)=np\left(1-p\right)$
\end{example}


\subsection{Measures of Statistical Relationship}



Covariance and correlation are attempst to measure the linear dependence between to random variables.

Covarance

\begin{definition}
Definition 183. Let X and Y be random variables having finite means. Let $E\left(X\right)=\mu_{X}$ and $E\left(Y\right)=\mu_{Y}$. The covariance of X and Y, which is denoted by $Cov\left(X,Y\right)$ is defined as $Cov\left(X,Y\right)=E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right]$
\end{definition}

if the expectation exists.

The covariance between X and Y is intended to measre the degree to which X and Y tend to be large at the same time or the degree to which one tends to be large while the other is small.

\begin{proposition}
For all random variables X and Y such that $\sigma_{X}^{2}<\infty$ and $\sigma_{Y}^{2}<\infty Cov\left(X,Y\right)=E\left(XY\right)-E\left(X\right)E\left(Y\right)$
\end{proposition}
\begin{proof}
\end{proof}

Correlation

Correlation is a measure of association between two random variables that is not driven by arbitrary changes in the scales.

\begin{definition}
Let X and Y be random variables with finite variances $\sigma_{X}^{2}$ and $\sigma_{Y}^{2}$ respectively. Then the correlation of X and Y, which is denoted by $\rho\left(X,Y\right)$, is defined as follows $\rho\left(X,Y\right)=\frac{Cov\left(X,Y\right)}{\sigma_{X}\sigma_{Y}}$
\end{definition}

XX

\begin{definition}
It is said that X and Y are positively correlated if $\rho\left(X,Y\right)>0$, that X and Y are negatively correlated if $\rho\left(X,Y\right)<0$ and that X and Yare uncorrelated if $\rho\left(X,Y\right)=0$.
\end{definition}

Properties of Covariance and Correlation

\begin{proposition}
Moreover $-1 \leq \rho\left(X,Y\right) \leq 1$
\end{proposition}
\begin{proof}
\end{proof}

\begin{proposition}
If X and Y are independent random variables with $0<\text{\ensuremath{\sigma_{X}^{2}}}<\infty$ and $0<\text{\ensuremath{\sigma_{Y}^{2}}}<\infty$ then $Cov\left(X,Y\right)=\rho\left(X,Y\right)=0$
\end{proposition}
\begin{proof}
\end{proof}

The converse is not true as a general rule. Two dependent random variables can be uncorrelated.

\begin{proposition}
Suppose that X is a random variable such that $0<\sigma_{X}^{2}\infty$ and $Y=aX+b$ for some constants a and b, where $a\neq0$. If $a>0$ then $\rho\left(X,Y\right)=1$. If $a<0$, then $\rho\left(X,Y\right)=-1$.
\end{proposition}
\begin{proof}
\end{proof}

The converse is also true, that is, if $\left|\rho\left(X,Y\right)\right|=1$ implies that X and Y are linearly related.

\begin{proposition}
If X and Y are random variables such that $Var\left(X\right)<\infty$ and $Var\left(Y\right)<\infty$, then $Var\left(X+Y\right)=Var\left(X\right)+Var\left(Y\right)+2Cov\left(X,Y\right)$
\end{proposition}
\begin{proof}
\end{proof}

For all constants a and b, it can be shown that $Cov\left(aX,bY\right)=abCov\left(X,Y\right)$.

\begin{proposition}
Let X and Y are random variables such that $Var\left(X\right)<\infty$ and $Var\left(Y\right)<\infty$, and let a, b and c be constants, then $Var\left(aX+bY+c\right)=a^{2}Var\left(X\right)+b^{2}Var\left(Y\right)+2abCov\left(X,Y\right)$
\end{proposition}
\begin{proof}
\end{proof}

A special case is $Var\left(X-Y\right)=Var\left(X\right)+Var\left(Y\right)-2Cov\left(X,Y\right)$

\begin{proposition}
If $X_{1},\ldots,X_{n}$ are random variables such that $Var\left(X_{i}\right)<\infty for i=1,\ldots,n$, then $Var\left(\sum_{i=1}^{n}X_{i}\right)=\sum_{i=1}^{n}Var\left(X_{i}\right)+2\sum\sum Cov\left(X_{i},X_{j}\right)$
\end{proposition}
\begin{proof}
\end{proof}

\begin{proposition}
If $X_{1},\ldots,X_{n}$ are uncorrelated random variables, then $Var\left(\sum_{i=1}^{n}X_{i}\right)=\sum_{i=1}^{n}Var\left(X_{i}\right)$
\end{proposition}
\begin{proof}
\end{proof}


%
% Section: Distribution
%

\section{Common Distributions}
\label{sec:probability_distributions}


\subsection{Uniform Distribution}

\begin{definition}
Let $a \leq b$ be integers. Suppose that the value of a random variable $X$ is equally likely to be each of the integers $a, \ldots, b$. Then we say that $X$ has the uniform distribution on the integers $a, \ldots, b$.
\end{definition}

{\color{red} Introduce the following propostion}

\begin{proposition}
If $X$ has the uniform distribution on the integers $a,\ldots,b$, the p.f. of $X$ is $f\left(x\right)=\begin{cases}
\frac{1}{b-a+1} & for\,x=a,\ldots,b\\
0 & otherwise
\end{cases}$
\end{proposition}
\begin{proof}
\end{proof}

{\color{red} The uniform distribution on the integers $a, \ldots, b$ represents the outcome of an experiment that is often described by saying that one of the integers $a, \ldots, b$ is chosen at random. A uniform distribution cannot be assigned to an infinite sequence of possible values}


\subsection{Bernoulli Distributions}

\begin{example}
A random variable $Z$ that takes only two values $0$ and $1$ with $P\left(Z=1\right)=p$ has the Bernoulli distribution with parameter $p$. We also say that $Z$ is a Bernoulli random variable with parameter $p$.
\end{example}

\subsection{Binomial Distributions}

\begin{example}
Suppose we perform $N$ independent trials where each trial either succeeds or fails with probability of success $p$, and let $X$ the random variable defined by the number of successes. The probability of having exactly $n$ successes $P(X=n)$ follows a \emph{binomial distribution} with parameters $N, p$, defined by:
\[
f(n\mid N, p) = \binom{N}{n} p^n (1-p)^{(N-n)}
\]
\end{example}

\begin{definition}
The discrete distribution represented by the p.f.f $\left(x\right)=\begin{cases}
{n \choose x}p^{x}\left(1-p\right)^{n-x} & for\,x=0,1,\ldots,n\\
0 & otherwise
\end{cases}$
\end{definition}

{\color{red} is called the binomial distribution with parameters $n$ and $p$.}

{\color{red} Consider a general experiment that consists of observing $n$ independent trials with only two possible results for each trial: success and failure. Then the distribution of the number of trials that result in success will be binomial with parameters $n$ and $p$, where $p$ is the probability of success on each trial.} 


%
% Section: Large Random Samples
%

\section{Large Random Samples}
\label{sec:probability_random_samples}

The law of large numbers provides the mathematical foundation to the intuition that the average of a large sample of independent and identically distributed random variables should be close to their mean. The central limit theorem is a practical method that allow us to approximate the probability that the sample average is close to the true mean.

{\color{red} Write a more elaborated introduction to the section.}

% Law of Large Numbers

\subsection{Law of Large Numbers}

The \emph{law of large numbers} is a theorem that states that the average of a large sample of independent and identically distributed random variables should be close to their mean, and that the more variables we add, the closer will be to that value. In practice, the law of large numbers allow us to describe the expected value of performing the same experiment a large number of times.

In this section we are going to prove the weak version of law of large numbers. {\color{red} Explain the difference between the weak and the strong versions of the law.}

Before to prove the law of large numbers we need to prove two other related propostions: Markov's inequality and Chebyshev's inequality. Markov's inequality puts a bound on how much arbitrarily large can be the values of a nonnegative random variable given its mean. 

\begin{proposition}[Markov's Inequality]
Let $X$ be a nonnegative random variable, i.e. $P\left( X \geq 0 \right) = 1$ with mean $\mu$. Then for every real number $t>0$ we have that 
\[
P \left( X \geq t \right) \leq \frac{\mu}{t}
\]
\end{proposition}
\begin{proof}
Let $f$ be the probability mass function of $X$. The mean $\mu$ of $X$ is given by $\mu = \sum_{x} x f \left( x \right)$, but since $X$ is non-negative we have that $\mu = \sum_{x>0} x f \left( x \right)$. Then
% \begin{multline}
\[
\mu = \sum_{x>0} x f \left( x \right) = \sum_{x=0}^{t} x f \left( x \right) + \sum_{x=t}^{\infty} x f \left( x \right) \geq
\sum_{x=t}^{\infty} x f \left( x \right) \geq \sum_{x=t}^{\infty} t f \left( x \right) = t \sum_{x=t}^{\infty} f \left( x \right) =
t P \left( X \geq t \right)
\]
% \end{multline}
that is, $\mu \geq t P \left( X \geq t \right)$.
\end{proof}

Chebyshev's inequality uses the variance to bound on how much arbitrarily large can be the values of the random variable given its mean. Chebyshev's inequality does not require the random variable to be nonnegative.

\begin{corollary}[Chebyshev's inequality]
 Let $X$ be a random variable with mean $\mu$ and variance $\sigma^2$. Then for every real number $t > 0$ we have that
\[
P \left( \left| X - \mu \right| \geq t \right) \leq \frac{\sigma^2}{t^2}
\]
\end{corollary}
\begin{proof}
Applying Markov's inequality we have that
\[
P \left( \left| X - \mu \right| \geq t \right) = P \left( \left( X - \mu \right)^2 \geq t^2 \right) \leq \frac{E \left( \left( X - \mu \right)^2 \right)}{t^2} = \frac{\sigma^2}{t^2}
\]
\end{proof}

The last element we need to formally prove the law of large numbers is to introduce the concept of convergence in probability for random variables. A sequence of random variables converges in probability to a value $b$ if $X_n$ lies in all the intervals around $b$.

\begin{definition}
Let $X_{1}, X_{2}, \ldots$ be a sequence  of random variables. It is said that the sequence $X_{1}, X_{2}, \ldots$ \emph{converges in probability} to $b$ , denoted by $X_{n} \overset{p}{\rightarrow}b$, if for every positive real number $\varepsilon>0$ we have that
\[
\lim_{n \rightarrow \infty} P \left( \left| X_{n} - b \right| < \varepsilon \right) = 1
\]
\end{definition}

Given the above definitions and partial results, we can now formally introduce and proove the law of large numbers.

\begin{theorem}[Law of Large Numbers]
\label{th:law_large_numbers}
Let $X_1, \ldots, X_n$ be a random sample with finite mean $E \left( X_i \right) = \mu <\infty$ for all $i$ and finite variance $Var \left( X_i \right) = \sigma^2 <\infty$ for all $i$, and let $\overline {X}_n = \frac {1}{n} \left( X_1 + \ldots + X_n \right)$ be the sample mean. Then we have that
\[
X_n \overset{p}{\rightarrow} \mu
\]
\end{theorem}
\begin{proof}
Since the variables $X_1, \ldots, X_n$ are independent and identically distributed, we have that the variance of the sample mean is $Var \left( \overline {X}_n \right) = \frac{\sigma^2}{n}$ and that the mean is $E \left( \overline {X}_n \right) = \mu$ (see XX). Applying the Chebyshev's inequality to the random variable $\overline {X}_n$ we have that
\[
P \left( \left| \overline {X}_n- \mu \right| \geq \varepsilon \right) \leq \frac{\sigma ^2}{n \varepsilon^2}.
\]
From there we can obtain
\[
P \left( \left| \overline {X}_n - \mu \right| < \varepsilon \right) = 1 - P \left( \left| \overline {X}_n - \mu \right| \geq \varepsilon \right) \geq 1 - \frac{\sigma ^2}{n \varepsilon^2}.
\]
As n approaches infinity, the above expression approaches 1. Aplying the definition of convergence in probability, we have that
\[
\overline{X}_n \overset{p}{\rightarrow} \mu
\]
\end{proof}

It is very important to note that the law of large numbers only works in case that the random variables are independent and identicaly distributed. Also, that the law is true only in the limit. If we have a finite number of random variables, the sample mean will be close to the distribution mean, but not necesarily equal.

Also it is important to mention that the law refers to the average of the sample mean, that is, $\sum_{i=1}^{n} \frac {X_{i}}{n}$ and it is not necesarily true for other formulas, like for example, the deviation from the theoretical values $\sum_{i=1}^{n} X_{i} - n \times \overline {X}$ which not only it does not convers, but that it inscrases in absolute value as $n$ increases (see Example \ref{ex:gambler's_fallacy}).

\begin{example}
\label{ex:gambler's_fallacy}
If we toss a fair coin, the probability that the outcome will be head is equal to $1/2$. According to the law of large numbers, the proportion of heads in a large number of coin tosses will be close to $1/2$. However, the difference between the number of heads and tails will not be close to zero. In fact, the larger the number of coin tosses, the larger will be this difference. This is a highly conterintuitive fact, since most of the people think that the more we toss the coin, the closer will be the number of heads to the number of tails, which is not true.
\end{example}

% Central Limit Theorem

\subsection{Central Limit Theorem}

Let $X_1, \ldots, X_n$ be a sample of $n$ independent and identically distributed random variables with mean $\mu$ and variance $\sigma^2$. As we saw in the previous section, the law of large numbers states that the sample average $\overline {X}_n$ converges in probability to $\mu$ as $n$ increases. The central limit theorem states that the distribution of the difference between the sample average $\overline {X}_n$ and the population mean $\mu$, when multiplied by the factor $\sqrt {n}$ approximates to the normal distribution with mean $0$ and variance $\sigma^2 / n$. The theorem is true regardless of the shape of the original random vairables.

\begin{theorem}[Central Limit Theorem]
Let $X_{1}, \ldots, X_{n}$ be a random sample of size $n$ from a distribution with mean $\mu$ and a finite variance $\sigma^{2}$. Then  for each fixed number $\varepsilon > 0$ we have that
\[
\lim_{n \rightarrow \infty} Pr \left( \frac{\overline{X}_{n}-\mu}{\sigma/n^{1/2}} \leq \varepsilon \right) = \Phi \left( x \right)
\]
where $\Phi \left( x \right)$ denotes the cumulative distribution function of the standard normal distribution.
\end{theorem}
\begin{proof}
{\color{red} Perhaps is too complex for this book to give a proof of the theorem.}
\end{proof}

{\color{red} TODO: Use a formulation of the theorem that does not uses the cumulative distribution}

The Central Limit Theorem is a fundamental concept in probability theory used in statistical analysis and inference. It allows us to compute the probability that the sample average is close to the distribution mean. It is important to recall the conditons for the central limit theorem to be true: the samples must be independent and identically distributed, the original distribution has to have a finite variance, and the sample size must be sufficiently large.

\begin{example}
{\color{red} Start with a uniform distribution, for example, throwing a dice. Explain and show how the aritmethic mean can be described with a binomial distribution. Explain that going to the limit results in the normal distribution.}
\end{example}

{\color{red} Elaborate in the fact the central limit theorem, and the law of large numbers, do not help too much if our target metric is not the arithmetic mean. Mention the implications, for example, in machine learning.}

%
% Section: References
%
\section*{References}

% There are a few more interpretations of the concept of probability that you might consider including:

% 1. **Propensity Interpretation**: This interpretation, often associated with the philosopher Karl Popper, suggests that probabilities express physical propensities, or tendencies, of situations. It is often applied in quantum mechanics and other fields where repeated experiments under exactly the same conditions give different outcomes.

% 2. **Logical Interpretation**: The logical interpretation, associated with philosophers like John Maynard Keynes and Rudolf Carnap, views probabilities as logical relations between statements or propositions. In this view, the probability of a statement is a measure of the degree to which the available evidence supports the statement.

% 3. **Empirical Interpretation**: This interpretation, often associated with Richard von Mises, takes the long-run frequency view of the frequentist interpretation but applies it more strictly. It states that probabilities can only be assigned to events that can be repeated indefinitely under essentially the same conditions. It emphasizes the role of empirical observation over theoretical considerations.

% 4. **Constructive Interpretation**: This is a view of probability inspired by intuitionistic logic, in which the truth of a statement is regarded as something that can be constructed, rather than something that exists independently of our knowledge. In the constructive interpretation of probability, a probability is a construction rather than a description of reality. This view is less common and more controversial than the others.

% Each of these interpretations has its strengths and weaknesses, and the choice between them often depends on the context and purpose for which probability is being used.

% Here are some academic references for these interpretations:

% 1. **Propensity Interpretation**:
%    - Popper, K. (1959). "The Propensity Interpretation of Probability". British Journal for the Philosophy of Science. 
%    - Miller, David. (1994). "Propensities: Popper and Probability". British Journal for the Philosophy of Science.

% 2. **Logical Interpretation**:
%    - Keynes, J. M. (1921). "A Treatise on Probability". Macmillan and Co. 
%    - Carnap, Rudolf. (1950). "Logical Foundations of Probability". University of Chicago Press.

% 3. **Empirical Interpretation**:
%    - Von Mises, Richard. (1957). "Probability, Statistics and Truth". Dover Publications.
%   - Hájek, Alan. (2012). "Interpretations of Probability". The Stanford Encyclopedia of Philosophy.

% 4. **Constructive Interpretation**:
%    - Per Martin-Löf. (1973). "An intuitionistic theory of types: predicative part". In Logic Colloquium '73.

% Please note that some of these sources are primary, being written by the philosophers who proposed or developed the interpretation in question. Others are secondary, discussing or explaining the interpretation. In the case of the constructive interpretation, the source is not about probability per se, but about the underlying logic that inspires the interpretation. For more detailed and up-to-date discussions of these interpretations, consider checking out the more recent literature in the philosophy of probability.


