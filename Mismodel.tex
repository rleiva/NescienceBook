%
% CHAPTER: Mismodel
%

\chapterimage{Alexander_cuts_the_Gordian_Knot.pdf} % Chapter heading image

\chapter{Mismodel}
\label{chap:Redundancy}

\begin{quote}
\begin{flushright}
\emph{Everything should be made as simple as possible,\\
but not simpler.}\\
Albert Einstein 
\end{flushright}
\end{quote}
\bigskip

{\color{red} TODO: Nice introduction}

Mismodel, being a non-computable quantity, has to be approximated in practice. We propose to approximate the concept of mismodel by splitting it into two complementary subconcepts: inaccuracy and surfeit. Inaccuracy compares the output of our description with the representation selected to encode the entity.

%
% Section: Mismodel
%

\section{Mismodel}

In Section \ref{sec:descriptions_models} we defined the concept of description, or model, of an entity as a computer program that, when executed, recreates one of the representations that encode that entity. More specifically, a description $d$ of a representation $r$ encoding an entity $e$ is a Turing machine that, when interpreted by a universal Turing machine $\delta$, prints out the string $r$. We also saw that the objective of science is to find from the shortest desription $d^\star_r$ from the collection of perfect descriptions $\mathcal{D}^\star_r$ that model the representation $r$. This shortest desription $d^\star_r$ might not be unique.

Since our knowledge about the entity $e$ under study is incomplete, and in particular our knowledge about the representation $r$ of $e$ is incomplete, we will be usually working with a description $d$ that is, hopefully, colose to one of the shortest descriptions $d^\star_r$, but not necessarily equal. We are interested in to quantify the error due to the use of a description $d$ that is not perfect.

We define the mismodel of a description for a representation as the normalized information distance between our description and all possible minimal descriptions of $r$.

\begin{definition} [Mismodel]
\label{def:mismodel}
Let $r \in \mathcal{B}^\ast$ be a representation, and $d \in \mathcal{D}$ be a description. We define the \emph{mismodel} of the description $d$ for the representation $r$, denoted by $\sigma(d, r)$, as:
\[
\sigma (d, r) = { \underset{ d^\star \in d^\star_r } \min} \frac{ \max\{ K \left( d \mid d^\star \right), K \left( d^\star \mid d \right) \} } { \max\{ K \left( d \right), K \left( d^\star \right) \} }
\]
\end{definition}

The description $d$ we are using not only it might not be one of the shortest possible models that print out $r$, but in fact, it might happen that what $d$ prints is not $r$.

We cannot use the more ...

The mismodel of a description is, conveniently, a number between $0$ and $1$, as next proposition shows.

\begin{proposition}
We have that $0 \leq \sigma(d, r) \leq 1$ for all representations $r \in \mathcal{B}^\ast$ and all the descriptions $d \in \mathcal{D}$.
\end{proposition}
\begin{proof}
Given that $0 \leq \frac{ \max\{ K(x \mid y), K(y \mid x) \} } { \max\{ K(x), K(y) \} } \leq 1$ for all $x, y \in \mathcal{B}^\ast$ according to Proposition \ref{prop:ncd_between_zero_and_one}.
\end{proof}

{\color{red} The above proposition holds for all possible descriptions $d$ and all possible representations $r$, even in the case that a description $d$ is not intended as a model for the representation $r$, in which case the inaccuracy would be close to one.}

Mismodel is equal to zero if, and only if, the description $d$ is one of the possible valid descriptions of the representation $r$.

\begin{proposition}
Let $d \in \mathcal{D}$ be a description for a representation $r \in \mathcal{B}^\ast$, we have that $\iota(d, r) = 0$ if, and only if, $d \in \mathcal{D}_r$.
\end{proposition}
\begin{proof}
If $d \in \mathcal{D}_r$ we have that $K \left( r \mid \delta(d) \right) = K \left( \delta(d) \mid r \right) = 0$ and that $\iota(d, r) = 0$. If $\iota(d, r) = 0$ we have that $\max\{ K \left( r \mid \delta(d) \right) = K \left( \delta(d) \mid r \right) \} = 0$, which implies that $K \left( r \mid \delta(d) \right) = K \left( \delta(d) \mid r \right) = 0$ and that $d \in \mathcal{D}_r$.
\end{proof}

%
% Section: Mismodel of Joint Representations
%

\subsection{Mismodel of Joint Representations}

{\color{red}

Given two representations $r$ and $s$, we want to know the inaccuracy of the model $d$ when describing the joint representation $rs$. Since we require that $rs$ must be a valid representation, the formalization of the concept of inaccuracy applied to joint representation is straightforward, and it does not require a new definition:
\[
\iota(d, rs) = \frac{ \max\{ K \left(rs \mid \delta(d) \right), K \left( \delta(d) \mid rs \right) \} } { \max\{ K(rs), K \left(\Gamma(d) \right) \} }
\]
As a direct consequence of Proposition \ref{prop:range_miscoding}, if $r, s \in \mathcal{B}^\ast$ are two arbitrary representations and $d \in \mathcal{D}$ is a description, we have that $0 \leq \iota(d, rs) \leq 1$.

{\color{red} TODO: Recall the properties of NID, and particularize for the case of inaccuracy of joint representations.}

}

%
% Section: Mismodel of Conditional Descriptions
%

\subsection{Conditional Mismodel}

{\color{red}

In this section we are going to extend the concept of inaccuracy from descriptions to conditional descriptions, that is, the inaccuracy of a description assuming the existence of some background knowledge, what we call conditional inaccuracy.

We have to start by defining what we mean when we say that a conditional description is inaccurate.

\begin{definition}
Let $r \in \mathcal{B}^\ast$ be a representation, and $d_{r \mid s} = \langle d, s \rangle$ a conditional description of $r$ given the string $s \in \mathcal{B}^\ast$, with $ d = \langle TM, a \rangle$. If $TM \left(\langle a, s \rangle \right) = r'$, such that $r \neq r'$, we say that $d_{r \mid s}$ is an \emph{inaccurate conditional description} for $r$.
\end{definition}

In the same way we defined the concept of inaccuracy of a description in Definition \ref{def:inaccuracy:inaccuracy:inaccuracy}, we can define the concept of conditional inaccuracy to characterize the error made when using an inaccurate conditional description.

\begin{definition}
Let $r \in \mathcal{B}^\ast$ be a representation, $s \in \mathcal{B}^\ast$ a string, and $d_{r \mid s} = \langle d, s \rangle$ a inaccurate condtional description. We define the \emph{conditional inaccuracy} of the description $d_{r \mid s}$ for the representation $r$ given the string $s$, denoted by $\iota(d_{r \mid s})$, as:
\[
\iota(d_{r \mid s}) = \frac{ \max\{ K \left(r \mid \delta(\langle d, s \rangle) \right), K \left( \delta(\langle d, s \rangle) \mid r \right) \} } { \max\{ K(r), K \left(\delta(\langle d, s \rangle) \right) \} }
\]
\end{definition}

The conditional inaccuracy of a description is a number between $0$ and $1$.

\begin{proposition}
\label{prop:range_conditional_inaccuracy}
Let $r \in \mathcal{B}^\ast$ be a representation, $s \in \mathcal{B}^\ast$ a string, and $d_{r \mid s} = \langle d, s \rangle$ a inaccurate condtional description. We have that $0 \leq \iota(d_{r \mid s}) \leq 1$.
\end{proposition}
\begin{proof}
Given that $0 \leq \frac{ \max\{ K(x \mid y), K(y \mid x) \} } { \max\{ K(x), K(y) \} } \leq 1$ for all $x, y \in \mathcal{B}^\ast$ according to Proposition \ref{prop:ncd_between_zero_and_one}.
\end{proof}

Conditional inaccuracy is equal to zero if, and only if, the description $d$ is one of the possible valid descriptions of the representation $r$.

\begin{proposition}\label{prop:perfect_description}
Let $r \in \mathcal{B}^\ast$ be a representation, $s \in \mathcal{B}^\ast$ a string, and $d_{r \mid s} = \langle d, s \rangle$ a condtional description, with $ d = \langle TM, a \rangle$. We have that $\iota(d_{r \mid s}) = 0$ if, and only if, $TM \left(\langle a, s \rangle \right) = r$.
\end{proposition}
\begin{proof}
If $TM \left(\langle a, s \rangle \right) = r$ we have that $K \left( r \mid \delta(d_{r \mid s}) \right) = K \left( \delta(d_{r \mid s}) \mid r \right) = 0$ and that $\iota(d_{r \mid s}) = 0$. If $\iota(d_{r \mid s}) = 0$ we have that $\max\{ K \left( r \mid \delta(d_{r \mid s}) \right) = K \left( \delta(d_{r \mid s}) \mid r \right) \} = 0$, which implies that $K \left( r \mid \delta(d_{r \mid s}) \right) = K \left( \delta(d_{r \mid s}) \mid r \right) = 0$ and that $TM \left(\langle a, s \rangle \right) = r$.
\end{proof}

Given two representations $r$ and $s$, we want to know the inaccuracy of a conditional model $d$ geniven $t$ when describing the joint representation $rs$. Since we require that $rs$ must be a valid representation, the formalization of the concept of contional inaccuracy applied to joint representation is straightforward, and it does not require a new definition:
\[
\iota(d_{rs \mid t}) = \frac{ \max\{ K \left(r \mid \delta(\langle d, t \rangle) \right), K \left( \delta(\langle d, t \rangle) \mid r \right) \} } { \max\{ K(rs), K \left(\delta(\langle d, t \rangle) \right) \} }
\]
As a direct consequence of Proposition \ref{prop:range_conditional_inaccuracy}, if $r, s \in \mathcal{B}^\ast$ are two arbitrary representations, $d_{r \mid s} = \langle d, s \rangle$ is a condtional description and $t \in \mathcal{B}^\ast$ an arbitrary string, we have that $0 \leq \iota(d_{rs \mid t}) \leq 1$.

}

%
% Section: Reducing Mismodel
%

\section{Reducing Mismodel}

{\color{red}

A valid representation of an entity is a string that contains all the necessary information required by the oracle to reconstruct that entity and only that information. If the representation is non-valid, meaning its miscoding exceeds zero, it may be because some crucial information is missing, some symbols are incorrect, or it contains irrelevant symbols. To reduce the miscoding of a representation, we can add the missing information, or remove the incorrect or irrelevant symbols. We cannot know in advance if any information is missing or some symbols need to be removed. Fortunately, as the next theorem illustrates, it must be one of these two cases.

\begin{theorem}
\label{th:reduce_miscoding}
Let $r \in \mathcal{B}^\ast$ be a representation such that $\mu(r) >0$, then at least one of the following cases is true:
\begin{enumerate}[label=(\roman*)]
\item there exist a $s \in \mathcal{B}^\ast$ such that $\mu(rs) < \mu(r)$ or $\mu(sr) < \mu(r)$,
\item there exists a $s \in \mathcal{B}^\ast$ in the form $r = \alpha s \beta$ with $\alpha, \beta \in \mathcal{B}^\ast$ such that $\mu(r) < \mu(s)$.
\end{enumerate}
\end{theorem}
\begin{proof}
{\color{red} Finish}
Assume that $\mu(r) >0$. We have that
\[
\overset{o}{ \underset{ r^\star_e \in \mathcal{R}^\star_\mathcal{E} } \min} \frac{ \max\{ K \left( r \mid r^\star_e \right), K \left( r^\star_e \mid r \right) \} } { \max\{ K \left( r \right), K \left( r^\star_e \right) \} } > 0
\]
Let $r^\star_e = arg\,min \left( \mu(r) \right)$. We have that $\max\{ K \left( r \mid r^\star_e \right), K \left( r^\star_e \mid r \right) \} > 0$. If $K \left( r \mid r^\star_e \right) > 0$ we have that $r$ contains non-relevant symbols. If $K \left( r^\star_e \mid r \right) > 0$ we have that $r$ is missing some relevant symbols.
\end{proof}

\begin{example}
{\color{red} TODO: Provide a practical example.}
\end{example}

}

%
% Section: Mismodel of Areas
%
\section{Mismodel of Areas}

{\color{red}

The concept of miscoding can be extended to research areas in order to quantitative measure the amount of effort required to fix an inaccurate representation of the area.

\begin{definition}
Let $A \subset \mathcal{E}$ be an area with known subset $\hat{A} = \{e_1, e_2, \ldots, e_n\}$. We define the \emph{miscoding of the area} given the known subset $\hat{A}$ as:
\[
\mu(\hat{A}) = \min_{(r^\star_{e_1}, r^\star_{e_2}, \ldots, r^\star_{e_n}) \in \mathcal{R}^\star_\mathcal{E}}  \frac{K \left( \langle t_{e_1}, t_{e_2}, \ldots, t_{e_n} \rangle \mid \langle t_1, t_2, \ldots, t_n \rangle \right) }{K \left( \langle t_{e_1}, t_{e_2}, \ldots, t_{e_n} \rangle \right)}
\]
\end{definition}

}


%
% Section: Inaccuracy
%
\section{Inaccuracy}

In Section \ref{sec:descriptions_models} we defined the concept of description, or model, of an entity as a computer program that, when executed, recreates one of the representations that encode that entity. More specifically, a description $d$ of a representation $r$ for an entity $e$ is a Turing machine that, when interpreted by a universal Turing machine $\delta$, prints out the string $r$. However, since our knowledge about the entity $e$ under study is usually incomplete, the description $\delta(d)$ will transcribe a different string $r'$, which will be similar to $r$, but not equal. In this chapter we are going to study the error induced by bad models, i.e. how close the string $r'$ is to the original string $r$. We refer to this type of error as the inaccuracy of the description $d$.

Inaccuracy is the second element we will use to characterize how well we understand a research entity. The intuition is that the more accurate our model is, the better we know the entity. From a formal point of view, we compute the inaccuracy of a description $d$ as the normalized information distance between the original representation $r$ and the representation $r'$ produced by our description $d$. That is, inaccuracy is masured as the length of the shortest computer program that can fix the incorrect output of our model.

Inaccuracy compares the output of our description with the representation selected to encode the entity. However, this representation could be at the same time an incorrect one, as we have seen in the previous chapter. Inaccuracy is a concept that deals only with the description $d$, and does not take into account the fact that the representation $r$ might have a positive miscoding. Furthermore, despite not requiring the use of the oracle, inaccuracy is a quantity that it is no computable for the general case, so it must be approximated in practice as we will see in Part III of this book.

In this chapter we will introduce formally the concept of inaccuracy and we will study its properties. We will also review how inaccuracy behaves when we use a conditional description of a representation compared to the unconditional one. And finally, we will extend the concept of inaccuracy from individual entities to research areas.

%
% Section: Inaccuracy
%

\subsection{Inaccuracy}
\label{sec:inaccuracy:inaccuracy}

When studying an entity $e \in \mathcal{E}$ through a representation $r \in \mathcal{R}_e$, it might happen that our candidate description $d$ is not a valid description for $r$, that is, $d \notin \mathcal{D}_r$ (see Definition \ref{def:descriptions_model}). In that case, the universal Turing machine $\delta$, when given as input $d$, will print out a string $r'$ different from the expected string $r$. Intuitively, we can say that $d$ is an inaccurate description of the entity $e$. However, since descriptions describe entities indirectly though representations, our formal definition of the concept of inaccuracy has to be given with respect to the representations in use, not with respect to the original entities, and we should take into account that representations might be themselves wrong (something that has been already addressed with the concept of miscoding). Given the above considerations, we propose the following definition of the concept of inaccurate description.

\begin{definition}
Let $r \in \mathcal{B}^\ast$ be a representation, and $d \in \mathcal{D}$ a description, with $ d = \langle TM, a \rangle$. If $TM(a) = r'$, such that $r \neq r'$, we say that $d$ is an \emph{inaccurate} description for $r$.
\end{definition}

Our candidate description $d$ might not belong to the set of valid descriptions $\mathcal{D}_r$ of $r$ (positive inaccuracy), and the representation $r$ might not belong to the set of valid $\mathcal{R}^\star_e$ representations of $e$ (positive miscoding).

If our description is inaccurate, we would like to have a quantitative measure of how far we are from the right description. In terms of machines, a natural way to define this measure would be by means of computing how difficult is to transform the wrong representation $r'$ produced by our description into the original representation $r$, that is, to compute the normalized information distance between $r'$ and $r$ .

\begin{definition} [Inaccuracy]
\label{def:inaccuracy:inaccuracy:inaccuracy}
Let $r \in \mathcal{B}^\ast$ be a representation, and $d \in \mathcal{D}$ a description, with $d = \langle TM, a \rangle$. We define the \emph{inaccuracy} of the description $d$ for the representation $r$, denoted by $\iota(d, r)$, as:
\[
\iota(d, r) = \frac{ \max\{ K \left(r \mid \delta(d) \right), K \left( \delta(d) \mid r \right) \} } { \max\{ K(r), K \left(\delta(d) \right) \} }
\]
\end{definition}

Having a relative measure of inaccuracy instead of an absolute one allow us to compare the inaccuracy of different descriptions for the same representation, and the inaccuracy of different descriptions for different representations.

Inaccuracy, as it was the case of miscoding (see Definition \ref{def:miscoding}), is computed using a two-way approach: we compute the length of the shortest computer program that can print the correct representation $r$ given the wrong one $r'$, and the other way around, that is, to compute the length of the shortest computer program that can print $r'$ given the string $r$. That is, the representation generated by a valid description has to include all the information required to reconstruct an entity, but it cannot include wrong, or irrelevant, information. 

\begin{example}
Inaccuracy is about how difficult is to fix the output of a description, i.e. the output of computable model, not how difficult is to fix the description itself. If we have a dataset produced by system that can be perfectly described by a quadratic function, and we use as description a linear function, inaccuracy will compare the original quadratic dataset with the linear dataset predicted. Inaccuracy is not about how difficult is to tranform the wrong linear model into the right quadratic one. In this sense, if the orginal dataset has 10 points, a ten degrees perfectly fitted polinomial would have also an inaccuracy of zero. Which model is the best between the zero inaccuracy quadratic and zero accuracy ten degrees polynomial is the subject of the surfeit metric (see Chapter \ref{chap:Redundancy}).
\end{example}

Being based in the concept of Kolmogorov complexity, inaccuracy is a quantity that cannot be computed in practice for the general case, and so, it must be approximated. How to approximate the concept of inaccuracy is something that depends on the characteristics of the entities under study and their representations.

The inaccuracy of a description is, conveniently, a number between $0$ and $1$, as next proposition shows.

\begin{proposition}
\label{prop:inaccuracy:inaccuracy:range}
We have that $0 \leq \iota(d, r) \leq 1$ for all representations $r \in \mathcal{B}^\ast$ and all the descriptions $d \in \mathcal{D}$.
\end{proposition}
\begin{proof}
Given that $0 \leq \frac{ \max\{ K(x \mid y), K(y \mid x) \} } { \max\{ K(x), K(y) \} } \leq 1$ for all $x, y \in \mathcal{B}^\ast$ according to Proposition \ref{prop:ncd_between_zero_and_one}.
\end{proof}

The above proposition holds for all possible descriptions $d$ and all possible representations $r$, even in the case that a description $d$ is not intended as a model for the representation $r$, in which case the inaccuracy would be close to one.

Inaccuracy is equal to zero if, and only if, the description $d$ is one of the possible valid descriptions of the representation $r$.

\begin{proposition}\label{prop:perfect_description}
Let $d \in \mathcal{D}$ be a description for a representation $r \in \mathcal{B}^\ast$, we have that $\iota(d, r) = 0$ if, and only if, $d \in \mathcal{D}_r$.
\end{proposition}
\begin{proof}
If $d \in \mathcal{D}_r$ we have that $K \left( r \mid \delta(d) \right) = K \left( \delta(d) \mid r \right) = 0$ and that $\iota(d, r) = 0$. If $\iota(d, r) = 0$ we have that $\max\{ K \left( r \mid \delta(d) \right) = K \left( \delta(d) \mid r \right) \} = 0$, which implies that $K \left( r \mid \delta(d) \right) = K \left( \delta(d) \mid r \right) = 0$ and that $d \in \mathcal{D}_r$.
\end{proof}

%
% Section: Inaccuracy of Joint Representations
%

\subsection{Inaccuracy of Joint Representations}

Given two representations $r$ and $s$, we want to know the inaccuracy of the model $d$ when describing the joint representation $rs$. Since we require that $rs$ must be a valid representation, the formalization of the concept of inaccuracy applied to joint representation is straightforward, and it does not require a new definition:
\[
\iota(d, rs) = \frac{ \max\{ K \left(rs \mid \delta(d) \right), K \left( \delta(d) \mid rs \right) \} } { \max\{ K(rs), K \left(\Gamma(d) \right) \} }
\]
As a direct consequence of Proposition \ref{prop:range_miscoding}, if $r, s \in \mathcal{B}^\ast$ are two arbitrary representations and $d \in \mathcal{D}$ is a description, we have that $0 \leq \iota(d, rs) \leq 1$.

{\color{red} TODO: Recall the properties of NID, and particularize for the case of inaccuracy of joint representations.}

%
% Section: Inaccuracy of Conditional Descriptions
%

\subsection{Conditional Inaccuracy}

In this section we are going to extend the concept of inaccuracy from descriptions to conditional descriptions, that is, the inaccuracy of a description assuming the existence of some background knowledge, what we call conditional inaccuracy.

We have to start by defining what we mean when we say that a conditional description is inaccurate.

\begin{definition}
Let $r \in \mathcal{B}^\ast$ be a representation, and $d_{r \mid s} = \langle d, s \rangle$ a conditional description of $r$ given the string $s \in \mathcal{B}^\ast$, with $ d = \langle TM, a \rangle$. If $TM \left(\langle a, s \rangle \right) = r'$, such that $r \neq r'$, we say that $d_{r \mid s}$ is an \emph{inaccurate conditional description} for $r$.
\end{definition}

In the same way we defined the concept of inaccuracy of a description in Definition \ref{def:inaccuracy:inaccuracy:inaccuracy}, we can define the concept of conditional inaccuracy to characterize the error made when using an inaccurate conditional description.

\begin{definition}
Let $r \in \mathcal{B}^\ast$ be a representation, $s \in \mathcal{B}^\ast$ a string, and $d_{r \mid s} = \langle d, s \rangle$ a inaccurate condtional description. We define the \emph{conditional inaccuracy} of the description $d_{r \mid s}$ for the representation $r$ given the string $s$, denoted by $\iota(d_{r \mid s})$, as:
\[
\iota(d_{r \mid s}) = \frac{ \max\{ K \left(r \mid \delta(\langle d, s \rangle) \right), K \left( \delta(\langle d, s \rangle) \mid r \right) \} } { \max\{ K(r), K \left(\delta(\langle d, s \rangle) \right) \} }
\]
\end{definition}

The conditional inaccuracy of a description is a number between $0$ and $1$.

\begin{proposition}
\label{prop:range_conditional_inaccuracy}
Let $r \in \mathcal{B}^\ast$ be a representation, $s \in \mathcal{B}^\ast$ a string, and $d_{r \mid s} = \langle d, s \rangle$ a inaccurate condtional description. We have that $0 \leq \iota(d_{r \mid s}) \leq 1$.
\end{proposition}
\begin{proof}
Given that $0 \leq \frac{ \max\{ K(x \mid y), K(y \mid x) \} } { \max\{ K(x), K(y) \} } \leq 1$ for all $x, y \in \mathcal{B}^\ast$ according to Proposition \ref{prop:ncd_between_zero_and_one}.
\end{proof}

Conditional inaccuracy is equal to zero if, and only if, the description $d$ is one of the possible valid descriptions of the representation $r$.

\begin{proposition}\label{prop:perfect_description}
Let $r \in \mathcal{B}^\ast$ be a representation, $s \in \mathcal{B}^\ast$ a string, and $d_{r \mid s} = \langle d, s \rangle$ a condtional description, with $ d = \langle TM, a \rangle$. We have that $\iota(d_{r \mid s}) = 0$ if, and only if, $TM \left(\langle a, s \rangle \right) = r$.
\end{proposition}
\begin{proof}
If $TM \left(\langle a, s \rangle \right) = r$ we have that $K \left( r \mid \delta(d_{r \mid s}) \right) = K \left( \delta(d_{r \mid s}) \mid r \right) = 0$ and that $\iota(d_{r \mid s}) = 0$. If $\iota(d_{r \mid s}) = 0$ we have that $\max\{ K \left( r \mid \delta(d_{r \mid s}) \right) = K \left( \delta(d_{r \mid s}) \mid r \right) \} = 0$, which implies that $K \left( r \mid \delta(d_{r \mid s}) \right) = K \left( \delta(d_{r \mid s}) \mid r \right) = 0$ and that $TM \left(\langle a, s \rangle \right) = r$.
\end{proof}

Given two representations $r$ and $s$, we want to know the inaccuracy of a conditional model $d$ geniven $t$ when describing the joint representation $rs$. Since we require that $rs$ must be a valid representation, the formalization of the concept of contional inaccuracy applied to joint representation is straightforward, and it does not require a new definition:
\[
\iota(d_{rs \mid t}) = \frac{ \max\{ K \left(r \mid \delta(\langle d, t \rangle) \right), K \left( \delta(\langle d, t \rangle) \mid r \right) \} } { \max\{ K(rs), K \left(\delta(\langle d, t \rangle) \right) \} }
\]
As a direct consequence of Proposition \ref{prop:range_conditional_inaccuracy}, if $r, s \in \mathcal{B}^\ast$ are two arbitrary representations, $d_{r \mid s} = \langle d, s \rangle$ is a condtional description and $t \in \mathcal{B}^\ast$ an arbitrary string, we have that $0 \leq \iota(d_{rs \mid t}) \leq 1$.

%
% Section: Inaccuracy of Areas
%

\subsection{Inaccuracy of Areas}

The concept of conditional inaccuracy can be extended to research areas in order to quantitative measure the amount of effort required to fix an inaccurate description of the area assuming some already existing background knowledge.

{\color{red} TODO: review notation}

\begin{definition}
Let $\mathcal{A} \subset \mathcal{E}$ be an area with known subset $\hat{\mathcal{A}} = \{r_1, r_2, \ldots, r_n\}$, $s \in \mathcal{B}^\ast$ a string, and $d_{\hat{\mathcal{A}} \mid s}$ a condtional description. We define the \emph{inaccuracy of the area} $d_{\hat{\mathcal{A}}}$ as:
\[
\iota(d_{\hat{\mathcal{A}} \mid s}) = \frac{ \max\{ K \left( \langle r_1, r_2, \ldots, r_n \rangle \mid \delta(\langle d, t \rangle) \right), K \left( \delta(\langle d, t \rangle) \mid \langle r_1, r_2, \ldots, r_n \rangle \right) \} } { \max\{ K(\langle r_1, r_2, \ldots, r_n \rangle), K \left(\delta(\langle d, t \rangle) \right) \} }
\]
\end{definition}

{\color{red} TODO: Recall the properties of areas, and particularize for the case of inaccuracy.}

%
% Surfeit
%
\section{Surfeit}

Surfeit is a quantity that measures how redundant is the model we are using to understand a research topic. Intuitively, the more ignorant we are about the topic, the longer will be our current best model. Long models usually contain elements that are wrong or that are not needed, and a better understanding of the topic should allow us to remove all those unnecessary symbols. 

We define the surfeit of a topic's model as the difference between the length of this model and the length of the best possible model for that topic. In the theory of nescience we assume that the theoretical limit of what can be known about a topic, that is, its perfect model, is the shortest possible description that allows us to fully reconstruct the topic (a representation of the entity). Of course, perfection is conditional on the representation being valid and the model accurate.

The length of the shortest possible model of a topic is given by the Kolmogorov complexity of that topic. As we have seen, this quantity is not computable for the general case. Moreover, in practice, and given that our knowledge about entities is in general incomplete, we do not know the shortest possible model either. Surfeit is a quantity that has to be approximated in practice.

If we were able to come up with a perfect description of an entity, that description must be a random string, otherwise it would contain redundant elements that can be removed. According to the theory of nescience, perfect knowledge implies randomness. Randomness imposes a limit on how much we can know about a particular research topic. Far from being a handicap, the proper understanding of this limitation opens new opportunities in science and technology. For example, by means of computing how far our current model is from being a random string we can estimate how far we are from having a perfect model.

In this chapter we will introduce formally the concept of surfeit, and study its properties. We will see ...

%
% Section: Surfeit
%

\subsection{Surfeit}
\label{sec:Definition_redundancy}

Given the length of a description of a representation for an entity, and the length of its shortest possible description, we can introduce a relative measure of how much unneeded effort we are using to explain the entity when using that description. We call this quantity \emph{surfeit}, and it will be part of our definition of nescience, that is, how much we do not know about that research entity.

\begin{definition}[Surfeit]
Given a representation $r \in \mathcal{B}^\ast$, and $d \in \mathcal{D}$ a description for $r$, we define the \emph{surfeit of the description $d$ for the representation $r$}, denoted by $\sigma(d, r)$, as
\[
\sigma (d, r) = \frac{ | l(d) - K(r) |}{l(d)}
\]
\end{definition}

For the majority of the descriptions it will be the case that the length of the description $l(d)$ for $r$ is greater that the length of its shortest possible description $K(r)$. Intuitively, the more ignorant we are about an entity, the longer will be our description, since a better understanding of that entity means that we should be able to remove all the redundant elements from its description. However, it migth also be the case that the description is shorter than the optimal description. In that case, we are oversimplifying the problem, which is also a bad thing. This is why we use the absolute value $| l(d) - K(r) |$ instead of symply $l(d) - K(r)$. Of course, the description in use could be inaccurate, and the representation could be non-valid, but these issues have been already addressed by the metrics of inaccuracy and miscoding.

In our definition of surfeit we have used a relative measure instead of an absolute one (i.e., $| l(d) - K(r) |$), because beside to compare the surfeit of different models for the same entity, we are also interested in to compare the surfeit of different entities. We prefer to use $K(r)$ instead of the equivalent $l \left( r^\star \right)$ in order to be consistent with the definition of inaccuracy provided in Section \ref{sec:inaccuracy:inaccuracy}.

\begin{example}
{\color{red} TODO: Provide an example to clarify the concept.}
\end{example}

The surfeit of a description is a number between $0$ and $1$.

\begin{proposition}
\label{prop:range_redundancy}
Let $r \in \mathcal{B}^\ast$ be a representation , and $d \in \mathcal{D}^\star_r$ one of its valid descriptions, then we have that $0 \leq \sigma(d, r) \leq 1$.
\end{proposition}
\begin{proof}
Given that $l\left( d \right)>0$ and that $K\left( r \right)>0$, since they are the lengths of non-empty strings, and that $l\left( d \right) \geq K\left( r \right)$ since we do not consider pleonastic descriptions.
\end{proof}

{\color{red} Explain when it is the case that surfait is zero, and that there could be more than one description that makes surfeit zero for the same representation.}

Our definition of surfeit compares the length of a description with the Kolmogorov complexity of the representation, not with the Kolmogorov complexity of the description itself (i.e., $K\left( d \right)$). That is, surfeit is not a measure of the redundancy of a description. It might happen that we come up with an incompressible description (no redundant elements to remove), that it is not the shortest possible one that describes the representation (see Example \ref{ex:description_neural}). Such a description would not be redundant in the traditional sense, but it still will present some surfeit in the sense of the theory of nescience. Moreover,it migth happen that the description $d$ we are considering does not describe the representation $r$, that is, $d \in \mathcal{D}_r$. In practice it is highly convenient to introduce the following alternative (we could say weaker) characterization of the concept of redundant model:

\begin{definition}[Redundancy]
Given a description $d \in \mathcal{D}$, we define the \emph{redundancy} of the description $d$, denoted by $\rho(d)$, as
\[
\rho(d) = 1 - \frac{K(d)}{l(d)}
\]
\end{definition}

The redundancy of a description $d$ is a quantity related to the description itself, and it does not depend on the representation $r$ being described.

\begin{example}
{\color{red} TODO: Provide an example to clarify the concept.}
\end{example}

We have that the redundancy of a description is  a number between $0$ and $1$.

\begin{proposition}
We have that $0 \leq \rho(d) \leq 1$ for all $d \in \mathcal{D}$.
\end{proposition}
\begin{proof}
Apply Proposition \ref{prop:kolmogorov_length}.
\end{proof}

{\color{red} Clarify when the redundancy is zero}

Finally, next proposition formalizes our intuition that the surfeit of a description is greater or equal than its redundancy.

\begin{proposition}
\label{prop:surfeit_comparison}
Let $r \in \mathcal{B}^\ast$ be a representation , and $d \in \mathcal{D}^\star_r$ one of its valid descriptions, then we have that $\rho(d) \leq \sigma(d, r)$.
\end{proposition}
\begin{proof}
Proving that $\rho(d) \leq \sigma(d, r)$ is equivalent to prove that $K(d) \geq K(r)$ for all $d$. Lets assume that there exist a $d$ such that $K(d) < K(r)$, that would mean there exists a Turing machine $\langle TM, a \rangle$ such that $TM(a)=r$ but $l(<TM, a>) < K(r)$. That is a contradiction with the fact that $K(r)$ is the length of the shortest possible Turing machine that prints $r$.
\end{proof}

It would be very nice if Proposition \ref{prop:surfeit_comparison} applies to all possible description. Unfortunately, the proposition is true only when we deal with valid descriptions (from $\mathcal{D}^\star_r$), as Example \ref{ex:surfeit_non_comparison} shows.

\begin{example}
\label{ex:surfeit_non_comparison}
{\color{red} TODO: Provide an example.}
\end{example}


%
% Section: Conditional Surfeit
%

\subsection{Conditional Surfeit}

We are interested into study how the surfeit of a description for a representation is affected when some background knowledge is assumed. That is, we want to know the surfeit of a conditional description for a representation, what we call \emph{conditional surfeit}.

\begin{definition}
Let $r \in \mathcal{B}^\ast$ be a representation, $s \in \mathcal{B}^\ast$ be a string, and $d \in \mathcal{D}$ be a description of $r$ given $s$. We define the \emph{conditional surfeit} of the conditional description $d_{r \mid s}$, denoted by $\sigma(d_{r \mid s})$, as: 
\[
\sigma(d_{r \mid s}) = 1 - \frac{K\left( r \mid s \right)}{l \left( d_{r \mid s} \right)}
\]
\end{definition}

This definition is required mostly for practical purposes, since given that our knowledge of $s$ is perfect, we can focus on studying what it is new in topic $t$, that is, in that part not covered by the assumed (perfect) background knowledge.

Conditional surfeit, being a relative measure, is a number between $0$ and $1$.

\begin{proposition}
Let $r \in \mathcal{B}^\ast$ be a representation, $s \in \mathcal{B}^\ast$ be a string, and $d \in \mathcal{D}$ be a description of $r$ given $s$. We have that $0 \leq \sigma(d_{r \mid s}) \leq 1$.
\end{proposition}
\begin{proof}
Given that $l \left( d_{r \mid s} \right) > 0$ and that $K\left( r \mid s \right) > 0$, since they are the lengths of non-empty strings, {\color{red} and that $l\left( d \right) \geq K\left( r \right)$ since we do not consider pleonastic descriptions}.

\end{proof}

Intuition tell us that the surfeit of a description could only decrease if we assume the background knowledge given by the description of another topic. This is because we require that this background knowledge must be a perfect description (it presents no surfeit). However, as it was the case of joint surfeit, we have to wait until Chapter \ref{chap:Nescience} to formalize this intuition.

In the same way we introduced the concept of redundancy of a description as a weaker version of the concept of surfeit, we can also introduce the concept of conditional redundancy as a weaker version of the concept of conditional surfeit.

\begin{definition}
Let $s \in \mathcal{B}^\ast$ be a string, and $d \in \mathcal{D}$ be a conditional description given $s$. We define the \emph{conditional surfeit} of the conditional description $d_{t \mid s^\star}$, denoted by $\sigma(d_{t \mid s^\star})$, as: 
\[
\rho(d_{r \mid s}) = 1 - \frac{K \left( d_{t \mid s^\star} \right)}{l \left( d_{t \mid s^\star} \right)}
\]
\end{definition}

Conditional surfeit is a relative measure, and so, a number between $0$ and $1$.

\begin{proposition}
We have that $0 \leq \rho(d_{t \mid s^\star}) \leq 1$ for all $t,s$ and all $d_{t,s}$.
\end{proposition}
\begin{proof}
Given that $K(d_{t \mid s^\star}) \leq l(d_{t \mid s^\star})$ we have that $\frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} \leq 1$ and so, $1 - \frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} \geq 0$. Also, since $\frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} > 0$ (both quantities are positive integers), we have that $1 - \frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} \leq 1$.
\end{proof}

Finally, we can extend our concepts of conditional surfeit and conditional redundancy to multiple, but fine, number of topics.

\begin{definition}
Let $t, s_1, s_2, \ldots, s_n \in \mathcal{T}$ be a finite collection of topics, and let $d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star}$ any conditional description of $t$ given $s_1, s_2, \ldots, s_n$. We define the \emph{conditional surfeit} of the description $d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star}$, denoted by $\sigma(d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star})$, as: 
\[
\sigma(d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star}) = 1 - \frac{K\left( t \mid s_1^\star, s_2^\star, \ldots,s_n^\star \right)}{l \left( d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star} \right)}
\]
And the \emph{conditional redundancy} of the description $d_{t_1, t_2, \ldots, t_n}$, denoted by $\rho(d_{t_1, t_2, \ldots, t_n})$, as:
\[
\rho(d_{t_1, t_2, \ldots, t_n}) = 1 - \frac{K(d_{t_1, t_2, \ldots, t_n})}{l \left( d_{t_1, t_2, \ldots, t_n} \right)}
\]
\end{definition}

It is easy to show that the properties of conditional surfeit and conditional redundancy apply to the case of multiple topics as well.

%
% Section: Surfeit of Areas
%

\subsection{Surfeit of Areas}

{\color{red} Review this section}

The concept of surfeit can be extended to research areas, to quantitative measure the amount of extra effort we are using to describe the topics of the area.

\begin{definition}
Let $A \subset \mathcal{T}$ be an area with known subset $\hat{A} = \{t_1, t_2, \ldots, t_n\}$, and let $d_{\hat{A}}$ be a description. We define the \emph{surfeit of the description} $d_{\hat{A}}$ as:
\[
\sigma \left( d_{\hat{A}} \right) = 1  - \frac{K( \langle t_1, t_2, \ldots, t_n \rangle )}{l \left( d_{\hat{A}} \right)}
\]
\end{definition}

As it was the case of the concept of redundancy, in general we do not know the complexity of the area $K(\hat{A})$, and so, in practice, it must be approximated by the complexity of the descriptions themselves $K(\hat{d}_{\hat{A}})$. However, in the particular case of areas, we could have also problems with the quantity $\hat{d}_{\hat{A}}$, since it requires to study the conditional descriptions of the topics included in the area.

\begin{definition}
Let $A \subset \mathcal{T}$ be an area with known subset $\hat{A} = \{t_1, t_2, \ldots, t_n\}$, and let $d_{\hat{A}}$ be a description. We define the \emph{weak redundancy of the description} $d_{\hat{A}}$ as:
\[
\rho(d_{\hat{A}}) =  1  - \frac{K \left( d_{\hat{A}} \right)}{l \left( d_{\hat{A}} \right)}
\]
\end{definition}

%
% Section: References
%

\section*{References}

A good introduction to the study of uncertaintines (error analysis in models) in science, and in particular in physics, chemistry, and engineering, is the best-selling text \cite{taylor2022introduction}, which also features the same image of a crashed train than in the introduction to this chapter.

The concept of redundancy has been also investigated in the context of information theory, since we are interested on using codes with low redundancy (see for example \cite{abramson1963information}).

{\color{red} TODO: Add more references.}
