%
% CHAPTER: Probability Theory
%

\chapterimage{Koenigsberg_Map_by_Bering_1613.pdf} % Chapter heading image

\chapter{Discrete Probability}
\label{chap:Probability Theory}

\begin{quote}
\begin{flushright}
\emph{Mathematics may be defined as the subject in which\\
we never know what we are talking about,\\
nor whether what we are saying is true.}\\
Bertrand Russell
\end{flushright}
\end{quote}
\bigskip

%
% Section: Foundations
%

\section{Foundations}
\label{sec:probability_foundations}

\emph{Probability} is a very difficult concept to grasp. Imagine we throw a dice and we want to calculate the probability that the number that appears is even. The dice has six possible outcomes, and since there are three even numbers, we say that the probability of having an even number is $3/6$ or $1/2$. This what the \emph{classical interpretation}\index{Classical interpretation of probability} of probability proposes: if we have an experiment in which all the possible outcomes are equally likely to happen, the probability of an event is the number of favourables cases divided by the total number of cases. The problem with this interpretations is that "equally likely" is essentially the same thing than "having the same probability", and so, it is a circular definition. An alternative approach to assign probabilities would be to apply the \emph{principle of indiference}\index{Principle of indiference}: in absense of any relevant evidence, all possible outcomes should have the same probability. The problem with this principle araises when there is evidence that not all cases are equal. For example, what happens if we know that the dice is loaded?, how do we assign probabilities when not all the sides are equally likely?

The \emph{frequentist interpretation}\index{Frequentist interpretation of probability} of probability proposes throwing the dice multiple times and calculating the releative frequency of even numbers with respect to the total number of throws. The general idea is to repeat the experiment a sufficiently large number of times under similar conditions and assing the releative frequency of each outcome as its probability. This interpretation has two main limitations. First of all, it is not clear how we should repeat an experiment under "similar conditions", since if we use exactly the same conditions the results of all the trials would be the same. The second is that it is not defined what a "large number of times" means (technically speaking, we should repeat the experiment an infinite number of times). From a practical point of view it is very difficult to apply the frequentist interpretation: some experiments cannot be repeated a large number of times, for example, what it is the probability that a candidate wins an election?; probability is defined in terms of a succession of experiments, so we cannot compute the probability of an individual outcome; and we require that the relative frequency limit exists, that is not always the case, for example, in finacial time series.

A third intepreation of the concept of probability, called \emph{subjective interpretation}\index{Subjective interpretation of probability}, proposes to assign to each event a probability based on our degree of belief: the more we belive that an event is true, the higher its probability. Of course, not all possible combinations of probabilities are valid, it is required that some rules of coherence must be satisfied. For example, if we are betting on the result of the dice thrown, an assigment of probabilities that guarantees that we will loose all of our money (what it is called a \emph{dutch book}\index{Dutch book}) do not satify the conditions of the subjective interpretation. It turns out that the conditions neccesary and sufficient to guarantee a fair bet are the same conditions required by the axioms of probability introduced below. In this sense, we can assign to events whatever probabilities we want, as long as they are consistant with the axioms of probability. The problem with the subjective interpretation is that people have different degrees of belief. A solution to this problem is proposed by the \emph{Bayesian interpretation}\index{Bayesian interpretation of probability} of probability: we start with a tentative assigment of probabilities, and as we get futher evidence, we modify our degree of belief, or probability, accordingly; as we get more evidence, estimated probabilities will converge to the true probabilities (see {\color{red} XXX} for more information about Bayes theorem). In any case, assigning probabilities to an infinite number of events is not, in general, human attainable. 

Currently, the concept of probabilty is defined axiomatically (\emph{axiomatic interpretation}\index{Axiomatic interpretation of probability}), which means we give up in trying to define what a probability is, and instead we assume as true some of its properties. Intuitively, a probability should be a number between $0$ and $1$, where an event that cannot happen has a probability of zero, and an event that for sure will happen has a probability of one. That fact that probability must be a number between $0$ and $1$ is more a social convention that a real mathematical requirement, since other ranges of numbers are equally good. We require some additional properties of probabilities. For example, if two events $A$ and $B$, with probabilities $P \left( A \right)$ and $P \left( B \right)$ respectively, cannot happen at the same time, the probabilty that either $A$ or $B$ occurs should be $P \left( A \right) + P \left( B \right)$. If $A$ and $B$ can happen at the same time, but they are not related in any way, that is, they are independent events (whatever that means), we expect that the probability of the two events happening at the same time shold be $P \left( A \right) \dot P \left( B \right)$. Finally, we are also interested in the pobabily of $A$ happeing assuming that $B$ has already happend, that sould be the the fraction of probability $A$ that intersects with $B$. Probability, then, would be anything that satisfies these properties. The problem with the axiomatic interpretation is that too many things satisfy those properties, for example, phisical quantities like normalized mass or normalized volume.

Probability theory is about assigning a number to some events from a sample space. The word "event" is a little bit misleading in this context because, as we will see in Section {\color{red} XX}, it suggest that something happened, which is not always the case. However, to avoid confusion, we will keep calling events to what are essentially subsets.

\begin{definition}
Let $\left( \Omega, \mathcal{A} \right)$ be a field over a non-empty discrete set. We call $\Omega$ the \emph{sample space}\index{Sample space}, its elements \emph{outcomes}\index{Outcome}, and the elements of $\mathcal{A}$ events. In particular, we call $\Omega$ the \emph{certain event}\index{Certain event} and $\varnothing$ the \emph{impossible event}\index{Impossible event}.
\end{definition}

As we saw in Section \ref{sec:sets}, given that $\left( \Omega, \mathcal{A} \right)$ is a field, we have that $\Omega \in \mathcal{A}$ and that $\varnothing \in \mathcal{A}$. Moreover, the union of a finite collection of events is an event $A_1 \cup A_2 \cup \ldots \cup A_n \in \mathcal{A}$, and that the intersection of a fine collection of events is also an event $A_1 \cap A_2 \cap \ldots \cap A_n \in \mathcal{A}$.

As we have said in the previous chapter, our main interest is in discrete mathematics, and so, we will mostly working with probabilities over discrete (finite or countably infinite) sets. An extension of the concept of probability to continuous sets requires the use of $\sigma$-algebras\index{$\sigma$-algebra} of sets instead of fields, and the use of some advanced concepts of measure theory, and so, it is beyond the scope of this book.

The standard axiomaitization used in probability theory are the \emph{Kolmogorov axioms}\index{Kolmogorov axioms}.

\begin{definition}
A \emph{probability}\index{Probability} is a number $P(A) \in \mathbb{R}$ assigned to each event $A \in \mathcal{A}$ of the field $\left( \Omega, \mathcal{A} \right)$, that satisfy the following axioms:

\medskip

\begin{description}
\item [Axiom 1] $P(A) \geq 0$.
\item [Axiom 2] $P(\Omega) = 1$.
\item [Axiom 3] For every finite sequence of disjoint events $A_1, A_2, \ldots, A_n$ we have that $P(\cup_{i=1}^n) = \sum_{i=1}^n P(A_i)$.
\end{description}
\end{definition}

No information is contained in the axioms about how probabilites can be assigned to the events.

\begin{example}
Let $\Omega$ a sample space containing $n$ elements equally probable. If $A \subset \Omega$ is an event with $d(A) = m$, the we have that $P(A) = m/n$.
\end{example}

Let's prove some basic results of probabilities, starting by calculating the probability of the complement of an event, that is, the probability that this event does not happen.

\begin{proposition}
For every event $A$, $P \left( A^{c} \right) = 1 - P \left( A \right)$
\end{proposition}
\begin{proof}
The sets $A$ and $A^c$ are disjoint and $A \cup A^c = \Omega$. Given Axiom 3 we have that $P \left( A \cup A^c \right) = P \left( A \right) + P \left( A^c \right)$, and given Axiom 2 we have that $P \left( A \cup A^c \right) = P(\Omega) = 1$, and so, $P \left( A \right) + P \left( A^c \right) = 1$.
\end{proof}

As a direct consequence of previous proposition, we can derive the probability of the impossible event.

\begin{proposition}
The probability of the impossible event is zero, that is, $P \left( \varnothing \right) = 0$
\end{proposition}
\begin{proof}
Given that $P \left( \varnothing \right) = 1 - P \left( \Omega \right) = 0$
\end{proof}

As expected, sub-events have smaller probabilities than events.

\begin{proposition}
If $A\subset B$ then $P \left( A \right) \leq P \left( B \right)$
\end{proposition}
\begin{proof}
The event $B$ can be decomposed as the union of two disjoint events $A$ and $A^c \cap B$, so we have that $P \left( B \right) = P \left( A \right) + P \left( A^c \cap B \right)$, that combined with the fact that  $P \left( A^c \cap B \right) \geq 0$ proves the proposition.
\end{proof}

Now we have all the elements we need to prove that probabilities are numbers between zero and one.

\begin{proposition}
For every event $A$ we have that $0 \leq P \left( A \right) \leq 1$.
\end{proposition}
\begin{proof}
Based on Axiom 1 and given that $A \subset \Omega$ and so $P \left( A \right) \leq P \left( \Omega \right) = 1$
\end{proof}

Axiom 3 allows us to compute the probability of the union of disjoint events. Next proposition allows us to compute the probability of the union of events that are not disjoint.

\begin{proposition}
For every two events $A$ and $B$, $Pr\left(A\cup B\right)=Pr\left(A\right)+Pr\left(B\right)-Pr\left(A\cap B\right)$.
\end{proposition}
\begin{proof}
The union of sets $A$ and $B$ can be decomposed as the union of the two disjoint sets $A \cup B = B \cup \left( A \cap B^c \right)$, so given Axiom 3 we have that
\[
P \left( A \cup B \right) = P \left( B \right) + P \left( A \cap B^c \right)
\]
In the same way, the set $A$ can be decomposed as the unition of the disjoint sets $A = \left( A \cap B \right) \cup \left( A \cap B^c \right)$, so that
\[
P \left( A \cup B^c \right) = P \left( A \right) - P \left( A \cap B \right)
\]
Combining boths expression we get the desired result.
\end{proof}

A similar formula can be derived for the case of $n$ events.

%
% Section: Conditional Probability
%

\section{Conditional Probability}
\label{sec:probability_conditional}

The concept of conditional probability plays a fundamental role in the area of statistical learning. Traditionally, the conditional probability of event $A$ given event $B$ has been seen as the updated probability of $A$ after we have learnt that $B$ has occurred. However, this interpretation suggests that there is a causal relationshipt between events $B$ and $A$, which is not necessarly true.

Based on Kolmogorov's axioms, conditional probability is defined as a quotient.

\begin{definition}
Let $A$ and $B$ two events such that $P \left( B \right) \neq 0$. The \emph{conditional probability} of $A$ given $B$, denoted by $P \left( A \mid B \right)$, is defined as
\[
P\left(A\mid B\right) = \frac{P\left(A\cap B\right)}{P\left(B\right)}
\]
\end{definition}

Conditional probability is itself a probability, since it satisfy the axioms. Conditional probability $P\left(A\mid B\right)$ is not defined if $P\left(B\right)=0$.

The probability that two events will happen toghether (although not necessarily at the same time), given their conditional probabilities, is $P \left( A \cap B \right) = P \left( A \mid B \right) P \left( B \right)$ or $P \left( A \cap B \right) = P \left( A \mid B \right) P \left( B \right)$. The formula $P \left( A \cap B \right) = P \left( A \mid B \right) P \left( B \right)$ provides a more intuitive interpretation of the concept of conditional probability. There are even some authors that have created alternative axiomatizations of probability in which this property is one of the assumed axioms.

The generalation of this formula for the case of $n$ events, called \emph{multiplication rule}, is $P \left( A_{1} \cap A_{2} \cap \ldots \cap A_{n} \right) = P \left( A_{1} \right) P \left( A_{2} \mid A_{1}\right) \ldots  P \left( A_{n} \mid A_{1}\cap A_{2} \cap \ldots \cap A_{n-1} \right)$.

Independence plays a very important role in probability theory and statistical learning. 

\begin{definition}
Two events $A$ and $B$ are said to be \emph{independent} if $P \left( A \cap B \right) = P \left( A \right) P \left(B \right)$
\end{definition}

From an intuitive point of view, the events $A$ and $B$ are independent if observing that B has occurred does not alter the probability of A. This property can be derived from the definition of independence.

\begin{proposition}
Let $A$ and $B$ two events shuch that $P \left( A \right) > 0$ and $P \left( B \right)>0$, then $A$ and $B$ are independent if and only if $P \left( A \mid B\right) = P \left( A \right)$ and $P \left( B \mid  A \right) = P \left( B \right)$.
\end{proposition}
\begin{proof}
Assume that $A$ and $B$ are independent, that is $P \left( A \cap B \right) = P \left( A \right) P \left(B \right)$, then
\[
P \left( A \mid B \right) = \frac{P\left(A\cap B\right)}{P\left(B\right)} = \frac{P \left( A \right) P \left(B \right)}{P\left(B\right)} = P \left( A \right)
\]
Now let's assume that $P \left( A \mid B \right) = P \left( A \right)$. Given the multiplication rule we have that,
\[
P \left( A \cap B \right) =  P \left( A \mid B \right) P \left( B \right) = P \left( A \right) P \left( B \right)
\] 
The same applies if we interchange $A$ and $B$. 
\end{proof}

As it was the case of conditional probability, some authors claim that independence, being a fundamental concept in probability theory, should be assumed as an axiom, not as a definition.

The concept of independence can be generalized to $n$ events as ... The $k$ events $A_{1},\ldots,A_{k}$ are independents (or mutually independent) if for every subset $A_{i_{1}},\ldots,A_{i_{j}}$ of $j$ of these events $\left(j=2,3,\ldots,k\right)P\left(A_{i_{1}}\cap\ldots\cap A_{i_{j}}\right)=P\left(A_{i_{1}}\right)\ldots P\left(A_{i_{j}}\right)$

\begin{example}
There is some confussion about the difference between mutually exclusive, or disjoint, events and independent events. If $A$ and $B$ are two mutually exclusive events, it does not make too much sense to compute the probability that $A$ will happen given $B$, since if $B$ happens, $A$ cannot happen; in the same way that it does not make too much sense to talk about the conditional probability that $A$ will happen given $B$ if the probability of $B$ is zero.
\end{example}

A particular interesting case is when events $A$ and $B$ are not independent but they become independent if we know that some other even $C$ has happened. 

\begin{definition}
Let's $A$, $B$ and $C$ events such that $P\left( B \cap C \right)>0$. $A$ and $B$ are \emph{conditionally independent} given $C$ if $P\left(A \mid B \cap C \right) = P\left( A \mid C \right)$.
\end{definition}

Baye's theorem provides the foundations of Bayesian inference (see Section XX), a very important technique in the area of statistical learning.

\begin{theorem} (Bayes' Theorem) Let's $A$ and $B$ two events such that $P\left( B \right) \neq 0$. Then we have that
\[
P \left( A \mid B \right) = \frac{P \left( B \mid A \right) P \left( A \right)}{P \left( B \right)}
\]
\end{theorem}
\begin{proof}
\end{proof}

\begin{example}
Let $E$ a disease that affects to one of every one million persons, $P(E) = 1 \times 10^{-6}$, and let $+$ a test designed to detect the disease that fails once every one thousand applications, $P(+ \mid E) = 999/1000$. We are interested in knowing the probability of having the disease if the test is positive $P(E \mid +)$. Applying Bayes' theorem we have that:
\[
P(E \mid +) = \frac{P(+ \mid E) P(E)}{P(+)} = \frac{P(+ \mid E) P(E)}{P(+ \mid E) P(E) + P(+ \mid E^c) P(E^c)} = 0.001
\]
That is, although the test only fails once per thousand applications, it is still very unlikely we have the disease in the case of a positive result. This counterintuitive result is explained because the probability of failure of the test $10^{-3}$ is much higher than the probability of having the disease $10^{-6}$. In practice we solve this problem by applying a second test to those who got a positive result, since the probability of having the disease after two positives is $0.5$ (assuming that the successive repetitions of the test are independent).
\end{example}

The probability $P\left( B \right)$ is called \emph{prior probability}, and $P\left( B\mid A \right)$ is called \emph{posterior probability}.

%
% Section: Random Variables
%

\section{Random Variables}
\label{sec:probability_random_variables}

{\color{red} In particular, if $B$ is a continuous event, the conditional probability is not defined by any individual point. Borel-Kolmogorovo paradox}


A \emph{random variable}\index{Random variable} is a mapping between outcomes and real numbers. Formally, a random variable is a function $X : \Omega \rightarrow \mathbb{R}$, where the probability of having the value $x \in \mathbb{R}$, denoted by $P(X=x)$, is given by $P(\{ \omega \in \Omega : X(\omega) = x\})$. A \emph{parametric random variable} $X$, denoted by $f\left(X \mid \theta \right)$, is a distribution that belongs to a family of functions parameterized by $\theta$, where $\theta$ can be a single parameter, or a vector of several parameters. The \emph{probability mass function}\index{Probability mass function} of a discrete random variable $X$ with range $\{ x_1, x_2, \ldots, x_i, \ldots \}$ is defined as the function $f$ such that $f(x_i) = P(X=x_i)$.

\begin{example}
Suppose we perform $N$ independent trials where each trial either succeeds or fails with probability of success $p$, and let $X$ the random variable defined by the number of successes. The probability of having exactly $n$ successes $P(X=n)$ follows a \emph{binomial distribution} with parameters $N, p$, defined by:
\[
f(n\mid N, p) = \binom{N}{n} p^n (1-p)^{(N-n)}
\]
\end{example}

The \emph{expectation}\index{Expectation} of $X$, denoted by $E(X)$ or $\mu$, is defined as $E(X) = \sum_{x \in \Omega} x f(x)$, and the \emph{standard deviation} as $\sigma = \sqrt{E \left( (X - \mu)^2 \right)}$.

\begin{example}
The expectation is $Np$, and the standard deviation $\sqrt{Np(1-p)}$.
\end{example}

%
% Section: Expectation
%

\section{Expectation}
\label{sec:probability_expectation}


%
% Section: Distribution
%

\section{Distributions}
\label{sec:probability_distributions}


%
% Section: Large Random Samples
%

\section{Large Random Samples}
\label{sec:probability_random_samples}

