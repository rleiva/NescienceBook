%
% CHAPTER: Miscoding
%

\chapterimage{Escher.pdf} % Chapter heading image

\chapter{Miscoding}
\label{chap:Miscoding}

\begin{quote}
    \begin{flushright}
        \emph{All great work is the fruit of patience and perseverance,\\
            combined with tenacious concentration on a subject\\
            over a period of months or years.}\\
        Santiago Ramón y Cajal
    \end{flushright}
\end{quote}
\bigskip

In many areas of science, the entities we aim to study (the set $\mathcal{E}$) often include abstract concepts or complex phenomena that are difficult to investigate directly. These may be ideas or processes that resist straightforward description or modeling. To gain insight into such entities, we must often rely on indirect methods. As discussed in the previous chapter, the theory of nescience suggests using representations, that is, sequences of symbols, rather than attempting to interact directly with the entities themselves.

However, this approach introduces its own challenges. Because our understanding of the elements in $\mathcal{E}$ is incomplete (otherwise, further research would be unnecessary), the representations we construct are typically imperfect, they lack the completeness and precision we would ideally desire. These imperfections can lead to significant problems: inaccuracies in representation can propagate into the models derived from them, potentially distorting our interpretation and understanding. It is therefore essential to recognize this source of error and to carefully consider its implications.

To address this issue, we introduce the concept of \emph{miscoding}, a measure used to quantify the error that arises from misrepresenting or inaccurately encoding entities. Miscoding is defined in terms of the length of the shortest computer program capable of transforming an incorrect representation into a correct one. In essence, it measures the amount of effort, as reflected by the program's length, required to correct a faulty representation.

However, there is a significant complication: applying this idea in practice is not straightforward, since the perfect representations of entities are unknown. As a result, we cannot directly measure the gap between a given representation and an ideal one. Nevertheless, we propose a theoretical framework based on the oracle machine (an abstract construct) used to define the set $\mathcal{E}$. This oracle machine is assumed to be capable of recognizing valid representations for all entities in the set. It is important to note, however, the limitations of this approach; for instance, we cannot query the oracle about a specific entity unless we already possess a valid representation of it.

Getting a better grasp on scientific representation and the challenges in reducing miscoding could really push forward scientific research, helping us create better, more thorough models of the natural world. In this chapter, we are going to properly introduce the idea of miscoding and look into its various characteristics. We will examine how miscoding behaves when it comes to combined representations and discuss methods to reduce miscoding. We will delve into the idea that there are strings that do not represent any entity. And we will dig into why miscoding is important in different research areas and talk about its potential to open up new lines of inquiry.

Gaining a deeper understanding of scientific representation and the difficulties involved in reducing miscoding can significantly advance scientific research, enabling the development of more accurate and comprehensive models of the natural world. In this chapter, we formally introduce the concept of \emph{miscoding} and explore its key properties. We will analyze how miscoding behaves in the context of combined representations and consider strategies for reducing it. We will also examine the notion that some strings fail to represent any entity and investigate the implications of this idea. Finally, we will highlight the relevance of miscoding across various research domains and discuss its potential to inspire new lines of inquiry.

%
% Section: Miscoding
%
\section{Miscoding}
\label{sec:miscoding}

In an ideal scenario, we would ask the oracle to determine how far a particular string $r$ is from perfectly encoding an entity $e$. This, however, presents a complication: we can only refer to entity $e$ through a perfect representation from the set $\mathcal{R}^\star_e$. Unfortunately, we typically do not have access to these perfect representations and, therefore, cannot use them in our query.

To overcome this limitation, we propose an alternative approach. Rather than asking the oracle to assess the discrepancy between our current representation $r$ and a valid representation of the specific entity $e$, we instead we identify the smallest distance between $r$ and all valid representations of all entities in the set $\mathcal{E}$, that is, the elements of $\mathcal{R}^\star_\mathcal{E}$. 

\begin{definition} [Miscoding]\index{Miscoding}
\label{def:miscoding}
Let $r \in \mathcal{B}^\ast$ be a representation. We define the \emph{miscoding} of $r$, denoted by $\mu(r)$, as:
\[
\mu(r) = \overset{o}{ \underset{ r^\star_e \in \mathcal{R}^\star_\mathcal{E} } \min} \frac{ \max\{ K \left( r \mid r^\star_e \right), K \left( r^\star_e \mid r \right) \} } { \max\{ K \left( r \right), K \left( r^\star_e \right) \} }
\]
where $\overset{o}{\min}$ indicates that the minimum is to be computed by an oracle.
\end{definition}

Intuitively, our lack of knowledge about an entity is reflected in a higher miscoding value of its current representation. Conversely, a deeper understanding of the entity should lead to an encoding that is closer to a valid representation, and thus exhibits lower miscoding.

Miscoding is computed using a bidirectional approach: we require the oracle to determine both the length of the shortest computer program that can generate the valid representation $r^\star_e$ from our current representation $r$ (i.e., $K \left( r^\star_e \mid r \right)$), and the length of the shortest program that can generate $r$ from $r^\star_e$ (i.e., $K \left( r \mid r^\star_e \right)$). This bidirectionality captures the maximum difficulty of transforming one representation into the other. A high-quality representation should allow for an easy reconstruction of the correct representation and vice versa, implying that both of these conditional complexities are low. In other words, an ideal representation contains all the information needed to recover the correct encoding of the entity, without introducing any erroneous or irrelevant symbols.

While miscoding concerns whether the symbols in a representation are relevant, that is, whether they correctly refer to the intended entity, it is not concerned with how many symbols are used. In contrast, surfeit (as discussed in Chapter \ref{chap:Redundancy}) focuses on whether the symbols are essential, aiming to minimize the number of unnecessary or redundant symbols. In this sense, miscoding relates to the correctness of the content, while surfeit relates to the efficiency of its expression. Although both concepts address flaws in representation, they capture fundamentally different types of representational inefficiency.

Our definition of miscoding is formulated as a relative measure rather than an absolute one. Instead of providing an isolated score for a single representation, it quantifies how far a representation is from the nearest valid representation, normalized by their respective lengths. This relative formulation allows us to meaningfully compare the miscoding of different representations of the same entity, and also to compare the miscoding values across representations of different entities, regardless of their size or complexity.

The miscoding value of a representation $r$ always lies within the interval $[0, 1]$.

\begin{proposition}
\label{prop:range_miscoding}
For all $r \in \mathcal{B}^\ast$, it holds that $0 \leq \mu(r) \leq 1$.
\end{proposition}
\begin{proof}
This follows directly from the inequality $0 \leq \frac{ \max\{ K(x \mid y), K(y \mid x) \} }{ \max\{ K(x), K(y) \} } \leq 1$ which holds for all $x, y \in \mathcal{B}^\ast$, as stated in Proposition \ref{prop:ncd_between_zero_and_one}.
\end{proof}

Miscoding is zero if, and only if, the representation $r$ is a valid representation of some entity $e$.

\begin{proposition}
\label{prop:perfect_encoding}
Given a representation $r \in \mathcal{B}^\ast$, we have $\mu(r) = 0$ if, and only if, $r \in \mathcal{R}^\star_\mathcal{E}$.
\end{proposition}
\begin{proof}
If $r \in \mathcal{R}^\star_\mathcal{E}$, then there exists an entity $e \in \mathcal{E}$ such that $r = r^\star_e$. In this case, $K \left( r \mid r^\star_e \right) = 0$, and thus $\mu(r) = 0$.

Conversely, if $\mu(r) = 0$, then there must exist some $r^\star_e \in \mathcal{R}^\star_\mathcal{E}$ such that $K \left( r \mid r^\star_e \right) = 0$ and $K \left( r^\star_e \mid r \right) = 0$, which implies that $r = r^\star_e$, and hence $r \in \mathcal{R}^\star_\mathcal{E}$.
\end{proof}

According to Proposition \ref{prop:perfect_encoding}, a miscoding value of zero implies that $r$ perfectly encodes some entity $e$. However, identifying the exact entity that $r$ represents remains a challenge. While scientific intuition may offer plausible hypotheses about the encoded entity, such guesses cannot be confirmed mathematically. Moreover, as scientific understanding evolves, our interpretation of what $r$ encodes may also change over time (see Example \ref{ex:polywater}).

It has been observed that an entity $e \in \mathcal{E}$ may possess multiple valid representations, as captured by the set $\mathcal{R}^\star_e$. Fortunately, the value of miscoding is invariant under the choice of valid representation (see the discussion on style in Section \ref{sec:scientific_representation}).

\begin{proposition}
For any valid representation $r^\ast_e \in \mathcal{R}^\star_\mathcal{E}$, it holds that $\mu\left( r^\star_e \right) \leq \mu\left( r \right)$ for all $r \in \mathcal{B}^\star$.
\end{proposition}
\begin{proof}
This follows from the fact that $\mu\left( r^\star_e \right) = 0$ and $\mu\left( r \right) \geq 0$ for all $r \in \mathcal{B}^\star$.
\end{proof}

For a given entity $e$, all valid representations in the set $\mathcal{R}^\star_e$ are equally adequate with respect to miscoding, as each yields a miscoding value of $0$. From a practical standpoint, however, the most suitable representation is the one that best facilitates the discovery of new knowledge about the entity—specifically, the representation that most effectively supports the construction of explanatory models.

%
% Section: Miscoding of Joint Representations
%

\section{Joint Miscoding}
\label{sec:joint_miscoding}

Section \ref{sec:descriptions_joint_topic} introduced the notion of a \emph{joint representation}\index{Joint representation}, which arises when two representations $s, t \in \mathcal{B}^\ast$ are concatenated to form a new string $st$. This section explores the properties of miscoding as they apply to such joint representations.

Since the concatenated string $st$ is itself a valid representation, it is not necessary to introduce a separate
definition of miscoding for joint representations. The miscoding of the joint representation is given by:
\[
\mu(st) = \overset{o}{ \underset{ r^\star_e \in \mathcal{R}^\star_\mathcal{E} } \min} \frac{ \max\{ K \left( st \mid r^\star_e \right), K \left( r^\star_e \mid st \right) \} } { \max\{ K \left( st \right), K \left( r^\star_e \right) \} }
\]
Consistent with Proposition \ref{prop:range_miscoding}, the miscoding of a joint representation is a value between $0$ and $1$, i.e., $0 \leq \mu(st) \leq 1$.

It is important to note that supplementing an incomplete representation, that is, one with a positive miscoding value, with additional symbols does not necessarily lead to a reduction in miscoding. This is because the added symbols may introduce irrelevant or incorrect information. Conversely, miscoding does not necessarily increase either, since incorporating relevant and accurate symbols can reduce the miscoding of an incomplete representation.

From a formal perspective, given two arbitrary representations $s, t \in \mathcal{B}^\ast$, there is no guarantee that any of the following inequalities will always hold: $\mu(ts) \geq \mu(t)$, $\mu(ts) \leq \mu(t)$, $\mu(ts) \geq \mu(s)$, or $\mu(ts) \leq \mu(s)$.

\begin{example}
Consider the text $r$ of a biology research article that succinctly and accurately describes a newly discovered specimen, resulting in a low miscoding value $\mu(r)$. However, when additional text $s$, taken from a completely unrelated article about a second specimen from a different species, is appended, the resulting concatenated representation $rs$ muddles the original focus. This inclusion of unrelated content increases the miscoding value to $\mu(rs)$, illustrating a case in which $\mu(rs) \geq \mu(r)$. Although the added text is scientifically valid on its own, it dilutes the clarity and precision of the original article, thereby increasing its miscoding.

Conversely, in a second scenario, consider another research article $r'$ that exhibits a relatively high miscoding value $\mu(r')$, due to an incomplete description of the specimen. If this article is supplemented with additional text $s'$, containing essential information previously omitted, the resulting concatenated representation $r's'$ becomes significantly more informative and coherent. As a result, the miscoding value decreases, demonstrating a case where $\mu(r's') \leq \mu(r')$. This highlights that enriching an incomplete representation with relevant and focused content can yield a more accurate and lower-miscoding representation.
\end{example}

Furthermore, it should not be assumed that the miscoding of a joint representation is less than or equal to the sum of the miscodings of the individual representations, i.e., $\mu(st) \leq \mu(s) + \mu(t)$. This inequality does not necessarily hold, even when both $s$ and $t$ encode the same entity. The reason is that the joint representation \$st\$ may end up encoding an entirely different entity from those represented by $s$ and $t$ individually.

\begin{example}
In a medical research context, representation $r$ describes a new drug compound $A$ for treating a specific type of cancer, while representation $s$ outlines a genetic mutation $B$ associated with that cancer type. Both $r$ and $s$ have low miscoding values, denoting accurate, focused content. However, when concatenated, the combined representation $rs$ unintentionally suggests a third entity $C$, implying a causal relationship between $A$ and $B$. This unintended implication stems from the mixture of distinct information, leading to a higher miscoding value. In this instance, each of $r$, $s$, and $rs$ essentially represents different research entities. $r$ and $s$ are clear in their individual contexts, but $rs$ is ambiguous, exemplifying a case where concatenated information can inadvertently create a representation of an entirely different, unintended entity, thus increasing miscoding.
\end{example}

Given the non-commutative nature of joining two representations, it is not guaranteed that $\mu(ts) = \mu(st)$. This is because the composite strings $ts$ and $st$ may encode entirely different entities. This property holds true even when we restrict our attention to valid representations of a specific entity $e$. For example, if $r^\star_e, s^\star_e \in \mathcal{R}^\star_{e}$ are two valid representations of the entity $e$, there is no assurance that the concatenated string $r^\star_e s^\star_e$ will itself be a valid representation of $e$.

\begin{example}
In environmental science, representation $r$ thoroughly examines microplastics, their nature and harmful impacts on marine ecosystems, while representation $s$ focuses on specific technologies and methodologies for detecting these pollutants in water. The concatenation $rs$ presents a logical progression: first describing the pollutant and its effects, then outlining detection methods. This creates a coherent representation of a broader entity, one concerned with the pollutant, its impact, and how to detect it, an entity which may not yet be fully understood, hence yielding a higher miscoding value.

Conversely, the concatenation $sr$ begins with technical methodologies for detection and then provides the contextual background on microplastics and their environmental effects. This ordering may be more familiar within a methodological framework and results in a more coherent and well-understood entity, yielding a lower miscoding value.

In this particular case, we observe that $\mu(rs) > \mu(sr)$.
\end{example}

The concept of miscoding can be naturally extended to joint representations formed from any finite collection of representations. Let $r_1, r_2, \ldots, r_n \in \mathcal{B}^\ast$ be a finite collection of representations. The miscoding of their concatenation, denoted by $r_1 r_2 \ldots r_n$, is defined as:
\[
\mu(r_1 r_2 \ldots r_n) = \overset{o}{ \underset{ r^\star_e \in \mathcal{R}^\star_\mathcal{E} } \min} \frac{ \max\{ K \left( r_1 r_2 \ldots r_n \mid r^\star_e \right), K \left( r^\star_e \mid r_1 r_2 \ldots r_n \right) \} } { \max\{ K \left( r_1 r_2 \ldots r_n \right), K \left( r^\star_e \right) \} }
\]
As in the case of concatenating two representations, the miscoding of a joint representation composed of \$n\$ elements remains bounded within the unit interval $0 \leq \mu(r_1 r_2 \ldots r_n) \leq 1$. However, several caveats must be noted. There is no guarantee that $\mu(r_1 r_2 \ldots r_n) \leq \mu(r_i) \quad \text{or} \quad \mu(r_1 r_2 \ldots r_n) \geq \mu(r_i)$ for any $i = 1, \ldots, n$. Moreover, the miscoding of the joint representation is not necessarily less than or equal to the sum of the individual miscodings $\mu(r_1 r_2 \ldots r_n) \nleq \mu(r_1) + \mu(r_2) + \cdots + \mu(r_n)$. Finally, no general conclusions can be drawn regarding how the miscoding of a joint representation is affected by permutations of its components. That is, reordering the representations may increase, decrease, or leave unchanged the overall miscoding.

%
% Section: Decreasing Miscoding
%

\section{Decreasing Miscoding}

A valid representation of an entity is a string that contains all the essential information required by the oracle to reproduce the entity, no more, no less. In contrast, an invalid representation, indicated by a miscoding value greater than zero, may result from missing critical information, the inclusion of incorrect symbols, or the presence of irrelevant symbols. To reduce the miscoding of a representation, one may attempt to supply the missing information or remove erroneous or non-pertinent symbols. However, it is generally not possible to determine in advance which of these actions will be effective. Nevertheless, as the following theorem shows, at least one of these approaches must be applicable.

\begin{theorem}
\label{th:reduce_miscoding}
Let $r \in \mathcal{B}^\ast$ be a representation such that $\mu(r) >0$, then one or both of the following conditions must hold:
\begin{enumerate}[label=(\roman*)]
\item There exists a $s \in \mathcal{B}^\ast$ such that $\mu(rs) < \mu(r)$ or $\mu(sr) < \mu(r)$,
\item There exists an $s \in \mathcal{B}^\ast$ in the form $r = \alpha s \beta$ with $\alpha, \beta \in \mathcal{B}^\ast$ such that $\mu(s) < \mu(r)$.
\end{enumerate}
\end{theorem}
\begin{proof}
Let $r\in\mathcal B^{\ast}$ be a representation with
\[
\mu(r)=
\min_{r_{e}^{\star}\in\mathcal R^{\star}_{\mathcal E}}
\frac{\max\{K(r\mid r_{e}^{\star}),\,K(r_{e}^{\star}\mid r)\}}
     {\max\{K(r),\,K(r_{e}^{\star})\}}
>0 .
\]
Fix one valid representation
$r^{\star}\in\mathcal R^{\star}_{\mathcal E}$ at which the minimum is attained, and write
\[
A:=K(r\mid r^{\star}),\qquad
B:=K(r^{\star}\mid r),\qquad
L:=\max\{K(r),K(r^{\star})\},
\]
so that $\mu(r)=\frac{\max\{A,B\}}{L}$.

We distinguish two (exhaustive) cases. Case i).  $B>0$, that is, some information is missing in $r$. Because $B>0$, a shortest self-delimiting program $p$ of length $B$ exists that, given $r$, outputs $r^{\star}$.
Set $s:=p$ and consider the concatenation $r':=rs$. Because $s$ already is the program that converts $r$ into $r^{\star}$,
\[
K(r^{\star}\mid r')=O(1),\qquad
K(r'\mid r^{\star})\le A+B+O(1).
\]
Hence
\[
\max\{K(r^{\star}\mid r'),K(r'\mid r^{\star})\}\le A+B+O(1)<A+L .
\]
The description length of $r'$ satisfies $K(r')\le K(r)+B+O(\log B)$, so
\[
\max\{K(r'),K(r^{\star})\}\ge
\max\{K(r),K(r^{\star})\}=L .
\]
Thus
\[
\mu(r')\;=\;
\frac{\max\{K(r^{\star}\mid r'),\,K(r'\mid r^{\star})\}}
     {\max\{K(r'),\,K(r^{\star})\}}
\;<\;
\frac{A+B+O(1)}{L}
\;\le\;
\frac{\max\{A,B\}}{L}
=\mu(r),
\]
so either $\mu(rs)<\mu(r)$ (if we append) or, by symmetric reasoning, $\mu(sr)<\mu(r)$ (if we prepend).
Hence condition (i) is satisfied.

Case ii). $B=0$, that is, all information needed to produce $r^{\star}$ is contained in $r$; extra symbols remain. Now $A>0$ because $\mu(r)>0$. Since $K(r^{\star}\mid r)=0$, a deterministic algorithm can scan $r$ from left to right and halt as soon as it has seen a **shortest prefix** that already allows it to output $r^{\star}$.
Let $s$ be that prefix and write $r=\alpha s\beta$ with $\beta\ne\lambda$ (otherwise $r=s$ and $A=0$, contradicting $\mu(r)>0$).
\[
K(r^{\star}\mid s)=0,\qquad
K(s\mid r^{\star})\le K(r\mid r^{\star})=A .
\]
Because $s$ is a proper prefix of $r$, $K(s)\le K(r)$ (up to an $O(\log K(r))$ term). Therefore
\[
\mu(s)\;=\;
\frac{\max\{K(s\mid r^{\star}),K(r^{\star}\mid s)\}}
     {\max\{K(s),K(r^{\star})\}}
\;\le\;
\frac{A}{\max\{K(s),K(r^{\star})\}}
<
\frac{\max\{A,B\}}{L}
=\mu(r),
\]
so condition (ii) holds with that substring $s$.
\end{proof}

Refer to Example \ref{ex:lung_cancer} for a case in which appending additional information to a representation improves its quality, that is, reduces its miscoding. Also, the following example illustrates the opposite situation: a case in which removing certain symbols from a representation can lead to a decrease in miscoding.

\begin{example}
A historian compiles a representation $r$ intended to narrate the events surrounding the signing of a pivotal treaty. However, the inclusion of speculative statements and personal interpretations about the intentions of the involved parties, statements not supported by primary sources, results in a high miscoding value. Although the document is rich in content, it is contaminated by erroneous or unverifiable information, rendering it an invalid representation of the historical event.

By removing these speculative and unsubstantiated segments, a revised version $r'$ is produced. This refined representation presents a concise and factual account based solely on verifiable data and primary sources. The removal of misleading content reduces the miscoding value, making $r'$ a more valid representation that more accurately reflects the historical event without the noise of uncorroborated interpretations.
\end{example}

%
% Section: Targetless Representations
%

\section{Targetless Representations}
\label{sec:targetless_representations}

In the most extreme scenario, the miscoding value can reach its upper bound of 1. This occurs when the current representation $r$ does not contain a single symbol that contributes to the encoding of any entity in the set $\mathcal{E}$. In such a case, $r$ is said to be a targetless representation, an abstract string with no concrete or meaningful interpretation.

\begin{definition}
Let $r \in \mathcal{B}^\ast$ be a representation. If $\mu(r) = 1$ we say that $r$ is a \emph{targetless}\index{Targetless representation} representation.
\end{definition} 

Although one could, in principle, assign a targetless representation to an arbitrary entity, doing so would violate the essential requirement of surrogative reasoning\index{Surrogative reasoning} (see Section \ref{sec:scientific_representation}). The following proposition formalizes this idea.

\begin{proposition}
\label{prop:miscoding_upper_bound}
Given a representation $r \in \mathcal{B}^\ast$, the miscoding $\mu(r)$ reaches its maximum value of 1 if and only if no symbol in $r$ contributes to the encoding of any entity in $\mathcal{E}$.
\end{proposition}
\begin{proof}
Assume that $r$ contains no symbol that contributes to the encoding of any entity $e \in \mathcal{E}$. Let $r^\star_e$ be a valid representation of some $e \in \mathcal{E}$. Then, the shortest program capable of generating $r^\star_e$ from $r$ must encode the entirety of $r^\star_e$ without reuse of any symbols from $r$. Thus, $K(r^\star_e \mid r) = K(r^\star_e)$. Similarly, since no symbol in $r^\star_e$ contributes to the generation of $r$, we have $K(r \mid r^\star_e) = K(r)$. Substituting into the definition of miscoding:
\[
\mu(r) = \frac{ \max\{ K(r \mid r^\star_e), K(r^\star_e \mid r) \} }{ \max\{ K(r), K(r^\star_e) \} } = \frac{ \max\{ K(r), K(r^\star_e) \} }{ \max\{ K(r), K(r^\star_e) \} } = 1
\]
assuming $K(r) \neq 0$ and $K(r^\star_e) \neq 0$. Conversely, suppose that $\mu(r) = 1$. Then:
\[
\max\{ K(r \mid r^\star_e), K(r^\star_e \mid r) \} = \max\{ K(r), K(r^\star_e) \}
\]
which implies that neither $r$ can help generate $r^\star_e$ nor $r^\star_e$ can help generate $r$. Therefore, there is no overlap in information, and $r$ contains no symbol that contributes to the encoding of any entity in $\mathcal{E}$.
\end{proof}

For every finite or countably infinite set of entities $\mathcal{E}$, there exists an infinite number of targetless representations. We will prove this result constructively by showing that at least one targetless representation exists and then defining a method to generate infinitely many more from it.

\begin{proposition}
For every finite or countably infinite set of entities $\mathcal{E}$, there exists an uncountably infinite set of targetless representations.
\end{proposition}
\begin{proof}
Let $\mathcal{R}$ denote the set of all valid representations of entities in $\mathcal{E}$, such that for each entity $e \in \mathcal{E}$, there exists a representation $r \in \mathcal{R}$ and vice versa. Since $\mathcal{E}$ is finite or countably infinite, $\mathcal{R}$ is also countable.

Now consider the set $\mathcal{B}^\ast$ of all finite binary strings. This set is countably infinite, as it includes all possible finite sequences over the alphabet ${0,1}$. Because $\mathcal{R}$ is a subset of $\mathcal{B}^\ast$ and is countable, and $\mathcal{B}^\ast \setminus \mathcal{R}$ is non-empty and infinite, there must exist binary strings that are not valid representations of any entity in $\mathcal{E}$.

Let $r_0 \in \mathcal{B}^\ast \setminus \mathcal{R}$ be any such string. By definition, $r_0$ is a targetless representation.

Given any targetless representation $r_i$, we can construct a new targetless representation $r_{i+1}$ by appending a symbol (either '0' or '1') to $r_i$. Since $r_i$ does not encode any entity in $\mathcal{E}$, and $\mathcal{R}$ is prefix-free with respect to targetless extensions (i.e., extending a non-representation cannot turn it into a valid one), the new string $r_{i+1}$ also does not correspond to any entity. Therefore, $r_{i+1}$ is also targetless.

By iterating this process, we generate an infinite sequence of distinct targetless representations ${r_0, r_1, r_2, \ldots} \subset \mathcal{B}^\ast \setminus \mathcal{R}$. This proves that there exists a countably infinite set of targetless representations corresponding to any finite or countably infinite set of entities $\mathcal{E}$.
\end{proof}

What constitutes a targetless representation for one oracle may not necessarily be targetless for another, as the following example illustrates.

\begin{example}
Imagine two machines, A and B, each controlling a robotic arm capable of manufacturing nuts and bolts. Machine A operates using a low-level assembly language, whereas Machine B uses a more sophisticated high-level programming language. As a result, a particular set of instructions that fails to produce a valid output on Machine A—thus being considered a targetless representation—could be perfectly interpretable by Machine B, successfully yielding a finished bolt.
\end{example}

The normalized compression distance between a targetless representation and the closest valid (non-targetless) representation may be less than one. This indicates that the targetless representation shares some information with a valid one. However, this is not sufficient to ensure scientific progress, as the computational effort, reflected in the size or complexity of the Turing machine required to extract useful knowledge, may be larger.

%
% Section: Miscoding of Areas
%
\section{Miscoding of Areas}
\label{sec:miscoding_areas}

The concept of miscoding can be extended to research areas to quantitatively measure the amount of effort required to correct an inaccurate representation of an area. Unfortunately, as discussed in Section \ref{sec:areas}, there is no reliable way to verify whether the strings included in an $n$-fold representation actually correspond to entities within that area. Moreover, it is not possible to prevent cases in which some of these strings represent the same entity.

Consider the strings $r_1, r_2, \ldots, r_n$, where each $r_i \in \mathcal{B}^\ast$ for $i = 1, 2, \ldots, n$. Recall that the expression $r_1 r_2 \ldots r_n$ refers to the concatenation of these strings into a single binary string. This operation may merge the individual components in a way that makes them inseparable. In contrast, the notation $\langle r_1, r_2, \ldots, r_n \rangle$ denotes a re-encoding of the individual strings into a single string that preserves the ability to recover each original component.

Furthermore, the joint representation $r_1 r_2 \ldots r_n$ is assumed to encode a single entity, while the $n$-fold representation $\langle r_1, r_2, \ldots, r_n \rangle$ may encode up to $n$ distinct entities.

The following definition extends the concept of miscoding to $n$-fold representations.

\begin{definition}
Let $R = \left( r_1, r_2, \ldots, r_n \right) \in \mathcal{B}^\ast \times \mathcal{B}^\ast \times \cdots \times \mathcal{B}^\ast$ be an $n$-fold representation. We define the \emph{miscoding} of $R$, denoted by $\mu(R)$, as:
\begin{multline*}
\mu(R) = \overset{o}{\underset{(r^\star_{e_1}, \ldots, r^\star_{e_n}) \in \mathcal{R}^\star_\mathcal{E} \times \cdots \times \mathcal{R}^\star_\mathcal{E}}{\min}} \\
\frac{
\max\left\{
K\left( \langle r_1, \ldots, r_n \rangle \mid \langle r^\star_{e_1}, \ldots, r^\star_{e_n} \rangle \right),
K\left( \langle r^\star_{e_1}, \ldots, r^\star_{e_n} \rangle \mid \langle r_1, \ldots, r_n \rangle \right)
\right\}
}{
\max\left\{
K\left( \langle r_1, \ldots, r_n \rangle \right),
K\left( \langle r^\star_{e_1}, \ldots, r^\star_{e_n} \rangle \right)
\right\}
}
\end{multline*}
where $\overset{o}{\min}$ indicates that the minimum is to be computed by an oracle.
\end{definition}

The miscoding of the representation of an area falls within the range $[0, 1]$, as demonstrated by the following proposition.

\begin{proposition}
\label{prop:miscoding:area:range}
For all known subsets $R = \left( r_1, r_2, \ldots, r_n \right) \in \mathcal{B}^\ast \times \mathcal{B}^\ast \times \cdots \times \mathcal{B}^\ast$, it holds that $0 \leq \mu(R) \leq 1$.
\end{proposition}
\begin{proof}
Since $\langle r_1, r_2, \ldots, r_n \rangle$ is a string in $\mathcal{B}^\ast$, we can apply Proposition \ref{prop:ncd_between_zero_and_one}, which guarantees that the normalized compression-based distance lies between 0 and 1.
\end{proof}

By extending the concept of miscoding to cover research areas, we gain a quantitative means of evaluating the quality of representations for specific subsets of entities. This mathematical framework provides a rigorous tool to assess and correct inaccuracies, both at the level of individual entities and across broader scientific domains.

%
% Section: References
%

\section*{References}

Misrepresentation or inaccuracies in scientific representation have significant implications for scientific discovery, technological progress, policymaking, and other domains. However, no book or paper explicitly addresses the topic of "incorrect representations in science" from the perspective adopted in this work. Here, we decompose the problem of scientific representation into two complementary subproblems: the representation of entities and the description of those representations.

\cite{suppes2002representation} provides a formal foundation for scientific representation and emphasizes the role of structure-preserving mappings; his analysis is crucial for understanding when and how representations fail to capture relevant features of phenomena. \cite{van1980scientific} introduces the concept of "constructive empiricism" and highlights the model-dependent nature of scientific representation. \cite{latour2013laboratory} offers an ethnographic study of how scientific representations are socially constructed—and, at times, distorted—in scientific practice.

Although the specific topic of incorrect representations has not been directly examined in this way, various researchers have addressed related concerns indirectly. Their discussions often focus on issues such as scientific fraud, the replication crisis, and the use of incorrect or misleading models. For example, \cite{ioannidis2005most} presents a widely cited empirical and philosophical analysis of how methodological biases, poor model design, and selective reporting contribute to unreliable or misleading scientific results.






