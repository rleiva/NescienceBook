%
% CHAPTER: Probability Theory
%

\chapterimage{Koenigsberg_Map_by_Bering_1613.pdf} % Chapter heading image

\chapter{Probability Theory}
\label{chap:Probability Theory}

\begin{quote}
\begin{flushright}
\emph{Mathematics may be defined as the subject in which\\
we never know what we are talking about,\\
nor whether what we are saying is true.}\\
Bertrand Russell
\end{flushright}
\end{quote}
\bigskip

%
% Section: Foundations
%

\section{Foundations}
\label{sec:probability_foundations}

\emph{Probability} is a very difficult concept to grasp. Imagine we throw a dice and we want to calculate the probability that the number that appears is even. The dice has six possible outcomes, and since there are three even numbers, we say that the probability of having an even number is $3/6$ or $1/2$. This what the classical interpretation of probability proposes: if we have an experiment in which all the possible outcomes are equally likely to happen, the probability of an event is the number of favourables cases divided by the total number of cases. The problem with this interpretations is that "equally likely" is essentially the same than "having the same probability", and so, it is a circular definition. 

The classical interpretation of probability is based on the principle of indiference. The principle of indiferences states that ...

A second problem is what happens if te dice is loaded, that is, not all the numbers have the same probability.

The frequentist interpretation of probability proposes throwing the dice multiple times and calculating the releative frequency of the even numbers with respect to the total number of throws. The general idea is to repeat the experiment a sufficiently large number of times under similar conditions and assing the releative frequency of each outcome as its probability. This interpretation has two main limitations. First of all, it is not clear how we should repeat an experiment under "similar conditions", since if we use exactly the same conditions the results of all the trials would be the same. The second is that it is not defined what a "large number of times" means (technically speaking, we should repeat the experiment an infinite number of times). From a practical point of view, it is very difficult to apply the frequentist interpretation: some experiments cannot be repeated a large number of times, for example, what it is the probability that a candidate wins an election?; probability is defined in terms of a succession of experiments, so we cannot compute the probability of an individual outcome; finally, we require that the relative frequency limit exists, and that is not always the case, for example in finacial time series.

A third intepreatin of the concept of probability says we can assign to each event the probability we want (subjective interpretation), as long as we follow some rules of consitency.

dutch book

Currently, the concept of probabilty is defined axiomatically, which means we give up in trying to define what a probability is, and instead we assume as true some of its properties. In this sense, probability would be anything that satisfies the axioms.

Intuitively, a probability should be a number between $0$ and $1$, where an event that cannot happen has a probability of zero, and an event that for sure will happen has a probability of one. That fact that probability must be a number between $0$ and $1$ is more a social convention that a real mathematical requirement, since other ranges of numbers are equally good. We require some additional properties of probabilities. For example, if two events $A$ and $B$, with probabilities $P_A$ and $P_B$ respectively, cannot happen at the same time, the probabilty that either $A$ or $B$ occurs should be $P_A + P_B$. If $A$ and $B$ can happen at the same time, but they are not related in any way, that is, they are independent events (whatever that means), we expect that the probability of the two events happening at the same time shold be $P_A \dot P_B$. Finally, we are also interested in the pobabily of $A$ happeing assuming that $B$ has already happend, that sould be the the fraction of probability $A$ that intersects with $B$.

The standard axiomaitzation used for probability are the Kolmogorov axioms.


Let $\Omega$ be a set, called \emph{sample space}\index{Sample space}, whose elements are called \emph{outcomes}\index{Outcome}. An \emph{event}\index{Event} $A$ is any subset of $\Omega$. We call $\Omega$ the \emph{certain event} and $\varnothing$ the \emph{impossible event}. We are only interested those collections of events $\mathcal{A} \subseteq \mathcal{P}\left( \Omega \right)$ that satisfy the following conditions:

\begin{enumerate}[label=(\roman*)]
\item $\Omega \in \mathcal{A}$, 
\item if $A \in \mathcal{A}$ then $A^c \in \mathcal{A}$,
\item if $A_1, A_2, \ldots$ be a countable collection of events of $\mathcal{A}$, then $\cup_{i=1}^\infty A_i \in \mathcal{A}$.
\end{enumerate}

Given these conditions we have that the impossible event is an event $\varnothing \in \mathcal{A}$, the union of a finite collection of events $A_1, A_2, \ldots, A_n \in \mathcal{A}$ is an event $A_1 \cup A_2 \cup \ldots \cup A_n \in \mathcal{A}$, and by applying De Morgan's laws, that the intersection of a fine or countable collection of sets of $\mathcal{A}$ is also in $\mathcal{A}$.

If $\Omega$ is a finite or countable set, the collection of all subsets of $\Omega$ satisfies the above conditions. As we have said in the introduction of this chapter, our main interest is in discrete mathematics, and so, we will mostly working with probabilities over discrete (finite or countable) sets.

A \emph{probability}\index{Probability} is a number $P(A) \in \mathbb{R}$ assigned to each event $A \in \mathcal{A}$, that satisfy the following axioms:

\medskip

\begin{description}
\item [Axiom 1] $P(A) \geq 0$.
\item [Axiom 2] $P(\Omega) = 1$.
\item [Axiom 3] For every infinite sequence of disjoint events $A_1, A_2, \ldots$ we have that $P(\cup_{i=1}^\infty A_i) = \sum_{i=1}^\infty P(A_i)$.
\end{description}

\medskip

Probability is a number between $0$ and $1$. The probability of the impossible event $\varnothing$ is $0$\footnote{Note that in case of a continuous sample space, if an event has probability $0$ it does not mean necessarily that the event is impossible.}. The probability of the complement event is $P(A^c) = 1 - P(A)$. If $A \subseteq B$ then we have that $P(A) \leq P(B)$. If $A$ and $B$ are not mutually exclusive then we have that $P(A \cup B) = P(A) + P(B) - P(A \cap B)$. The probability of a finite sequence of disjoint events $A_1, A_2, \ldots, A_n$ is $P(\cup_{i=1}^n A_i) = \sum_{i=1}^n P(A_i)$.

\begin{example}
Let $\Omega$ a sample space containing $n$ elements equally probable. If $A \subset \Omega$ is an event with $d(A) = m$, the we have that $P(A) = m/n$.
\end{example}


%
% Section: Conditional Probability
%

\section{Conditional Probability}
\label{sec:probability_conditional}

The concept of conditional probability plays a fundamental role in the area of statistical learning. Traditionally, the conditional probability of event $A$ given event $B$ has been seen as the updated probability of $A$ after we learn that $B$ has occurred. However, this interpretation suggests that there is a causal relationshipt between events $B$ and $A$, which is not necessarly true.

From an axiomatical point of view, conditional probability is defined as a quotient.

\begin{definition}
Let $A$ and $B$ two events such that $P \left( B \right) \neq 0$. The \emph{conditional probability} of $A$ given $B$, denoted by $P\left(A\mid B\right)$, is defined as
\[
P\left(A\mid B\right) = \frac{P\left(A\cap B\right)}{P\left(B\right)}
\]
\end{definition}

The conditional probability $P\left(A\mid B\right)$ is not defined if $P\left(B\right)=0$.

The following \emph{multiplication rule} allows us to compute the probability that a collection of events will happen toghether (although not necessarily at the same time), given their conditional probabilities.

\begin{proposition}
Let $A_{1}, A_{2},\ldots,A_{n}$ be events such that $P\left(A_{1} \cap A_{2} \cap \ldots \cap A_{n-1}\right)>0$, then
\[
Pr\left(A_{1}\cap A_{2}\cap\ldots\cap A_{n}\right)=Pr\left(A_{1}\right)Pr\left(A_{2}\mid A_{1}\right)Pr\left(A_{3}\mid A_{1}\cap A_{2}\right)\ldots Pr\left(A_{n}\mid A_{1}\cap A_{2}\cap\ldots\cap A_{n-1}\right)
\]
\end{proposition}
\begin{proof}
{\color{red} TBD}
\end{proof}

In particular, $P\left(A\cap B\right) = Pr\left(B\right)Pr\left(A\mid B\right)=Pr\left(A\right)Pr\left(B\mid A\right)$. The formula $P\left(A\cap B\right) = P\left(B\right)Pr\left(A\mid B\right)$ provides a more intuitive interpretation of the concept of conditional probability. Some authors even suggest that it should be considered as an axiom of probability.

Law of total probability?

Augmented Experiment?

{\color{red} If learning that B has occurred does not change the probability of A, then we say that A and B are independent. There are many cases in which events A and B are not independent, but they would be independent if we learned that some other event C had occurred. In this case, A and B are conditionally independent given C.}

\begin{definition}
Definition 52. Two events A and B are independent ifP\left(A\cap B\right)=P\left(A\right)P\left(B\right)
\end{definition}

Assuming that P\left(A\right)>0 and P\left(B\right)>0 then A and B are independent if and only if P\left(A\mid B\right)=P\left(A\right) and P\left(B\mid A\right)=P\left(B\right).

Theorem 53. If two events A and B are independents, then the events A and B^{c} are also independent.

The same happens to A^{c} and B, and A^{c} and B^{c}.

Definition 54. The k events A_{1},\ldots,A_{k} are independents (or mutually independent) if for every subset A_{i_{1}},\ldots,A_{i_{j}} of j of these events \left(j=2,3,\ldots,k\right)P\left(A_{i_{1}}\cap\ldots\cap A_{i_{j}}\right)=P\left(A_{i_{1}}\right)\ldots P\left(A_{i_{j}}\right)

The k events A_{1},\ldots,A_{k} are pairwise independent if P\left(A_{i}\cap A_{j}\right)=P\left(A_{i}\right)P\left(A_{j}\right) for all i,j. A collection of pairwise independent events is not necessarily independent.

Theorem 55. Let A_{1},\ldots,A_{k} be events such that P\left(A_{1}\cap\ldots\cap A_{k}\right)>0. Then A_{1},\ldots,A_{k} are independent if and only if, for every two disjoint subsets \left\{ i_{1},\ldots,i_{m}\right\}  and \left\{ j_{1},\ldots,j_{l}\right\}  of \left\{ 1,\ldots,k\right\} , we haveP\left(A_{i_{1}}\cap\ldots\cap A_{i_{m}}\mid A_{j_{1}}\cap\ldots\cap A_{j_{k}}\right)=P\left(A_{i_{1}}\cap\ldots\cap A_{i_{m}}\right)



Theorem 56. Let n>1 and let A_{1},\ldots,A_{n} be events that are mutually exclusive. The events are also mutual independent if and only if all the events except possibly one of them has probability 0.

Conditionally Independent Events

Definition 57. We say that events A_{1},\ldots,A_{k} are conditionally independent given B if, for every subcollection A_{i_{1}},\ldots,A_{i_{j}} of j of these events \left(j=2,3,\ldots k\right)P\left(A_{i_{1}}\cap\ldots\cap A_{i_{j}}\mid B\right)=P\left(A_{i_{1}}\mid B\right)\ldots P\left(A_{i_{j}}\mid B\right)

Theorem 58. Suppose that A_{1}, A_{2} and B are events such that P\left(A_{1}\cap B\right)>0. Then A_{1} and A_{2} are conditionally independent given B if and only if P\left(A_{2}\mid A_{1}\cap B\right)=P\left(A_{2}\mid B\right).

2.3 Bayes' Theorem

Suppose that we are interested in which of several disjoint events B_{1},\ldots,B_{k} will occur and that we will get to observe some other event A. If P\left(A\mid B_{i}\right) is available for each i, then Bayes' theorem is a useful formula for computing the conditional probabilities of the B_{i} events given A.

Statement, Proof and Examples of Bayes' Theorem

Theorem 59. (Bayes' Theorem) Let the events B_{1},\ldots,B_{k} form a partition of the space S such that P\left(B_{j}\right)>0 for j=1,\ldots,k and let A be an event such that P\left(A\right)>0. Then, for i=1,\ldots,k,P\left(B_{i}\mid A\right)=\frac{P\left(B_{i}\right)P\left(A\mid B_{i}\right)}{\sum_{j=1}^{k}P\left(B_{j}\right)P\left(A\mid B_{j}\right)}

There is also a version of Bayes' theorem conditional on an event C:P\left(B_{i}\mid A\cap C\right)=\frac{P\left(B_{i}\mid C\right)P\left(A\mid B_{i}\cap C\right)}{\sum_{j=1}^{k}P\left(B_{j}\mid C\right)P\left(A\mid B_{j}\cap C\right)}

Prior and Posterior Probabilities

We call P\left(B\right) the prior probability, and P\left(B\mid A\right) the posterior probability. If an experiment is carried out in more than one stage, then the posterior probability of every event can also be calculated in more than one stage. After each stage has been carried out, the posterior probability calculated for the event after that stage serves as the prior probability for the next stage.

\begin{example}
Let $E$ a disease that affects to one of every one million persons, $P(E) = 1 \times 10^{-6}$, and let $+$ a test designed to detect the disease that fails once every one thousand applications, $P(+ \mid E) = 999/1000$. We are interested in knowing the probability of having the disease if the test is positive $P(E \mid +)$. Applying Bayes' theorem we have that:
\[
P(E \mid +) = \frac{P(+ \mid E) P(E)}{P(+)} = \frac{P(+ \mid E) P(E)}{P(+ \mid E) P(E) + P(+ \mid E^c) P(E^c)} = 0.001
\]
That is, although the test only fails once per thousand applications, it is still very unlikely we have the disease in the case of a positive result. This counterintuitive result is explained because the probability of failure of the test $10^{-3}$ is much higher than the probability of having the disease $10^{-6}$. In practice we solve this problem by applying a second test to those who got a positive result, since the probability of having the disease after two positives is $0.5$ (assuming that the successive repetitions of the test are independent).

https://en.wikipedia.org/wiki/Base_rate_fallacy
https://en.wikipedia.org/wiki/Extension_neglect

\end{example}




%
% Section: Random Variables
%

\section{Random Variables}
\label{sec:probability_random_variables}

{\color{red} In particular, if $B$ is a continuous event, the conditional probability is not defined by any individual point. Borel-Kolmogorovo paradox}


A \emph{random variable}\index{Random variable} is a mapping between outcomes and real numbers. Formally, a random variable is a function $X : \Omega \rightarrow \mathbb{R}$, where the probability of having the value $x \in \mathbb{R}$, denoted by $P(X=x)$, is given by $P(\{ \omega \in \Omega : X(\omega) = x\})$. A \emph{parametric random variable} $X$, denoted by $f\left(X \mid \theta \right)$, is a distribution that belongs to a family of functions parameterized by $\theta$, where $\theta$ can be a single parameter, or a vector of several parameters. The \emph{probability mass function}\index{Probability mass function} of a discrete random variable $X$ with range $\{ x_1, x_2, \ldots, x_i, \ldots \}$ is defined as the function $f$ such that $f(x_i) = P(X=x_i)$.

\begin{example}
Suppose we perform $N$ independent trials where each trial either succeeds or fails with probability of success $p$, and let $X$ the random variable defined by the number of successes. The probability of having exactly $n$ successes $P(X=n)$ follows a \emph{binomial distribution} with parameters $N, p$, defined by:
\[
f(n\mid N, p) = \binom{N}{n} p^n (1-p)^{(N-n)}
\]
\end{example}

The \emph{expectation}\index{Expectation} of $X$, denoted by $E(X)$ or $\mu$, is defined as $E(X) = \sum_{x \in \Omega} x f(x)$, and the \emph{standard deviation} as $\sigma = \sqrt{E \left( (X - \mu)^2 \right)}$.

\begin{example}
The expectation is $Np$, and the standard deviation $\sqrt{Np(1-p)}$.
\end{example}

%
% Section: Expectation
%

\section{Expectation}
\label{sec:probability_expectation}


%
% Section: Distribution
%

\section{Distributions}
\label{sec:probability_distributions}


%
% Section: Large Random Samples
%

\section{Large Random Samples}
\label{sec:probability_random_samples}

