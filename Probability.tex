%
% CHAPTER: Probability Theory
%

\chapterimage{Galton_box} % Chapter heading image

\chapter{Discrete Probability}
\label{chap:Probability Theory}

\begin{quote}
\begin{flushright}
\emph{The purpose of models is not to fit the data\\
but to sharpen the questions.}\\
Samuel Karlin
\end{flushright}
\end{quote}
\bigskip

Probability theory is the branch of mathematics that studies random experiments and random phenomena. Probability assigns a numerical description to all the possible outcomes of an experiment based on how likely these outcomes are to occur. Even if the outcome of an experiment cannot be determined in advance, we can study its properties with probability theory and draw relevant results and conclusions. For example, while we cannot predict the next number that will appear in a lottery game, probability theory can help us understand why it is not a good strategy to spend all our savings on lottery tickets with the goal of becoming rich.

The importance of probability theory extends far beyond games of chance. It provides the mathematical foundation for statistical inference, enabling us to draw conclusions from data, and for machine learning, where it underpins algorithms used for classification and forecasting based on large datasets. Additionally, probability theory plays a crucial role in fields such as finance, risk management, and the natural sciences, where understanding uncertainty and variability is essential.

In this chapter, we are going to focus on the area of discrete probability. In this version of the theory, the possible outcomes of an event are finite or, at most, countably infinite. We are interested in discrete probability, first because of its practical applications in the area of learning from data, and second, because discrete probability has some very interesting connections to theories used in this book: the length of optimal codes, the probability that a random machine will halt, and the derivation of universal distributions based on Kolmogorov complexity. All of these connections are relevant to our theory of nescience.

{\color{red} On a philosophical level, it might be argued that there can only be countable sample spaces, since measurements cannot be made with infinite accuracy. [...] While in practice this is true, probabilistic and statistical methods associated with uncountable sample spaces are, in general, less cumbersome than those for countable spaces, and privde a close approximation to the true (countable) situation.}

We will cover only the most important concepts and results of probability theory. The contents have been selected based on their applicability to the theory of nescience. For example, moment-generating functions are not covered. For a more comprehensive introduction to probability theory refer to the references section of this chapter.

We are going to study probability theory from a formal, axiomatic point of view. We will start by formulating a basic collection of fundamental axioms and then derive the major results and properties from them. Axioms are crucial in mathematical theory because they provide the foundation for constructing a consistent, universal, and rigorous framework. In probability theory, they allow us to define and manipulate the elusive concept of probability in a way that is both precise and broadly applicable.

%
% Section: Foundations of Probability Theory
%

\section{Foundations of Probability Theory}
\label{sec:probability_foundations}

The concept of \emph{probability} represents a profound intellectual challenge. Let us consider an instance where a dice is rolled and the objective is to compute the probability of an even number being the outcome. The dice comprises six distinct outcomes, and given that half of these are even numbers, we posit that the probability of yielding an even number is $3/6$ or equivalently $1/2$. This embodies the \emph{classical interpretation}\index{Classical interpretation of probability} of probability which asserts that in an experiment where all finite potential outcomes possess an equal likelihood of occurrence, the probability of an event equates to the count of favorable instances over the total number of instances. This interpretation, however, confronts the issue of circularity in its definition as "equally likely" essentially amounts to "possessing the same probability". An alternative method for probability assignment might be the implementation of the \emph{principle of indifference}\index{Principle of indifference}, which postulates that in the absence of any relevant evidence, all potential outcomes should possess identical probability. This principle, however, encounters a predicament when there exists evidence that contradicts the presumption of equivalence amongst all outcomes. For instance, how should probability be assigned when knowledge of a loaded dice suggests that not all sides possess an equal likelihood of occurrence?

The \emph{frequentist interpretation}\index{Frequentist interpretation of probability} of probability posits that one should roll the dice multiple times and contrast the frequency of even numbers with the total number of rolls. The fundamental notion is to execute the experiments repeatedly under similar conditions and assign the relative frequency of each outcome as its probability. This interpretation, however, faces two primary limitations. Firstly, the definition of repeating an experiment under "similar conditions" remains vague; if the conditions were truly identical, the results across all trials would invariably be the same. Secondly, the notion of a "large number of times" is undefined (technically, the experiment should be executed an infinite number of times). From a practical standpoint, implementing the frequentist interpretation is fraught with challenges. Some experiments, such as predicting the probability of a candidate winning an election, cannot be repeated multiple times. Furthermore, probability is defined in the context of a sequence of experiments, hence precluding the computation of the probability of a solitary outcome. Lastly, this interpretation necessitates the existence of a relative frequency limit, a condition which is not always satisfied, as illustrated by financial time series.

The \emph{subjective interpretation}\index{Subjective interpretation of probability}, representing a third approach to the concept of probability, suggests assigning probabilities to each event that reflect our degree of belief: the higher our conviction in the event's occurrence, the greater its assigned probability. However, it's important to note that not all potential probability allocations are viable; certain coherence rules need to be satisfied. For instance, when placing bets on the outcome of a dice roll, an assignment of probabilities that ensures a complete loss of money - a scenario known as a \emph{dutch book}\index{Dutch book} - would contravene the conditions of the subjective interpretation. It transpires that the conditions both necessary and sufficient to ensure a fair bet align with the axioms of probability introduced subsequently. Hence, we are free to assign any probabilities we desire to events, provided they remain consistent with the axioms of probability. A key drawback of the subjective interpretation is the inherent variability in individuals' degrees of belief. The \emph{Bayesian interpretation}\index{Bayesian interpretation of probability} of probability offers a solution: we commence with a provisional assignment of probabilities and, upon accruing further evidence, adjust our degree of belief or probability accordingly. With the accumulation of more evidence, estimated probabilities will converge to the true probabilities. Regardless, the task of assigning probabilities to an infinite number of events is typically unattainable for humans in general.

{\color{red} Probability theory builds upon set theory}

{\color{red} We do not define probabilities in terms of frequencies but instead take the mathematically simpler axiomatic approach. [...] The axiomatic approach is not concerned with the interpretation of probabilities, but is concerned only that the probabilities are defined by a function satisfying the axioms.}

Currently, the notion of probability is defined axiomatically via the \emph{axiomatic interpretation}\index{Axiomatic interpretation of probability}. This implies that we abandon attempts to explicitly define the concept of probability and instead accept some of its properties as inherently true. Intuitively, a probability should be a value between $0$ and $1$, wherein an event with a zero probability is deemed impossible, and an event with a probability of one is certain to occur. Additional properties are required of probabilities. For instance, should two events $A$ and $B$ with probabilities $P \left( A \right)$ and $P \left( B \right)$ respectively, be disjoint, the probability of either $A$ or $B$ occurring should be $P \left( A \right) + P \left( B \right)$. If $A$ and $B$ could occur simultaneously and are independent (however that is defined), the probability of both events occurring concurrently should be $P \left( A \right) P \left( B \right)$. Moreover, the probability of $A$ occurring given that $B$ has already happened should be the fraction of the probability of $A$ that intersects with $B$. Anything that satisfies these properties could be considered a probability.

Probability theory is fundamentally concerned with the task of assigning a numerical value to specific events drawn from a sample space. The term "event" in this context may be somewhat counterintuitive, as it suggest the occurrence of something, which is not always applicable. For instance, consider the sample space of all possible outcomes when tossing a fair coin. A subset of this sample space could be the empty set, which represents no coin toss happening at all. In the conventional understanding of an "event", this scenario may confuse people who arenâ€™t familiar with the mathematical meaning, as it does not correspond to something "happening". Nonetheless, for the sake of clarity, we will continue to employ the term "events" to designate what essentially are subsets.

\begin{definition}
Given $\left( \Omega, \mathcal{A} \right)$ as a field over a non-empty discrete set, $\Omega$ is referred to as the \emph{sample space}\index{Sample space}, its constituents are termed \emph{outcomes}\index{Outcome}, and the components of $\mathcal{A}$ are referred to as \emph{events}\index{Event}. Specifically, $\Omega$ is designated the \emph{certain event}\index{Certain event}, while the empty set $\varnothing$ is deemed the \emph{impossible event}\index{Impossible event}.
\end{definition}

As previously discussed in Section \ref{sec:sets}, given that $\left( \Omega, \mathcal{A} \right)$ is a field, we can deduce that $\Omega \in \mathcal{A}$ and that $\varnothing \in \mathcal{A}$. Additionally, the union of a finite collection of events constitutes an event $A_1 \cup A_2 \cup \ldots \cup A_n \in \mathcal{A}$, and the intersection of a finite collection of events is likewise deemed an event $A_1 \cap A_2 \cap \ldots \cap A_n \in \mathcal{A}$.

As alluded to in the introduction of this chapter, our principal interest lies in discrete mathematics, and hence, we will largely focus on probabilities pertaining to discrete sets (be they finite or countably infinite). An extension of the concept of probability to continuous sets would necessitate the utilization of $\sigma$-algebras\index{$\sigma$-algebra} of sets instead of fields and the application of measure theory. For example, consider a scenario where we measure the electric current in a circuit, which can take any value between -5 and 5 volts. Unlike a discrete set of outcomes, such as specific whole numbers, here we have infinitely many possible values within this interval. In this case, it doesn't make sense to talk about the probability of the current being exactly 2.5 volts (or any other specific real number), as there are infinitely many possible outcomes. Instead, we would talk about the probability of the current falling within a certain range, such as between 2 and 3 volts.

The prevailing axiomatization utilized in the realm of probability theory is encapsulated within the framework of the \emph{Kolmogorov axioms}\footnote{In discrete probability theory, the sample space consists of a finite or countably infinite set of distinct outcomes, which means events are typically composed of individual, separable outcomes. Since probabilities are assigned directly to these discrete events, only finite unions of disjoint events need to be considered in Axiom 3 to cover all practical cases.}.

\begin{definition} (\emph{Kolmogorov's Axioms})\index{Kolmogorov's axioms}\label{Kolmogorov_axioms}
A \emph{probability}\index{Probability} is a real number $P(A) \in \mathbb{R}$ allocated to each event $A \in \mathcal{A}$ in the field $\left( \Omega, \mathcal{A} \right)$. This allocation adheres to the following axioms:

\medskip

\begin{description}
\item [Axiom 1] Each probability is nonnegative: $P(A) \geq 0$.
\item [Axiom 2] The probability of the certain event is one: $P(\Omega) = 1$.
\item [Axiom 3] For any finite sequence of disjoint events $A_1, A_2, \ldots, A_n$, the probability of the union of these events is the sum of their probabilities: $P(\cup_{i=1}^n A_i) = \sum_{i=1}^n P(A_i)$.
\end{description}

The triplet $\left( \Omega, \mathcal{A}, P \right)$ constitutes what is known as a \emph{probability space}\index{Probability space}.
\end{definition}

Despite their significance, the Kolmogorov axioms encounter certain complexities. While they provide the foundational principles that probabilities must conform to, such as non-negativity, normalization, and additivity, they do not offer explicit guidance on how to assign probabilities to specific events. Essentially, the axioms define the rules that probability must follow but leave the determination of those probabilities open. This stems from the fact that Kolmogorov's axioms are highly general and abstract, designed to apply to any measure-theoretic structure. This level of generality means they can accommodate a wide range of constructs beyond probability, such as normalized mass, volume, or other physical quantities. Finally, the connection between model theory (as discussed in Appendix \ref{apx:foundations_mathematics}) and probability theory through the Kolmogorov axioms is not straightforward. Model theory primarily deals with first-order logic, which can express basic properties like non-negativity and finite additivity of sets. However, when it comes to formally defining a continuous quantity, such as probability as a real number, it fails because first-order logic cannot fully capture the complexity of real numbers and continuous structures.

\begin{example}
\label{ex:discrete_sample_space}
Consider a sample space $\Omega$ composed of $n$ equally probable elements. If we have an event $A \subset \Omega$ comprised of $d(A) = m$ elements, then the probability of event $A$ can be represented as $P(A) = m/n$.
\end{example}

{\color{red}

We need general methods of defining probability functions that we know will always satisfy Kolmogorov's Axioms.

\begin{proposition}
Let $\mathcal{S} = \{ s_1, \ldots, s_n \}$ be a finite set. Let $\mathcal{B}$ be any sigma algebra of subsets of $\mathcal{S}$. Let $p_1, \ldots, p_n$ be nonnegative numbers that sum to 1. For any $A \in \mathcal{B}$, define $P(A)$ by:
\[
P(A) = \sum_\{ i : s_i \in A \} p_i
\]
(The sum over an empty set is defined to be 0.) Then $P$ is a probability function on $\mathcal{B}$. This remains true if $\mathcal{S} = \{ s_1, s_2, \ldots \}$ is a countable set.
\end{proposition}
\begin{proof}
\end{proof}

}

We now venture to establish certain fundamental theorems concerning probabilities, beginning with the calculation of the complement of an event, which represents the probability of an event not occurring.

\begin{proposition}
For each event $A$, it holds true that $P \left( A^{c} \right) = 1 - P \left( A \right)$.
\end{proposition}
\begin{proof}
Sets $A$ and $A^c$ are disjoint, and their union $A \cup A^c$ equals $\Omega$. By applying Axiom 3, we infer that $P \left( A \cup A^c \right) = P \left( A \right) + P \left( A^c \right)$, and by applying Axiom 2, we conclude that $P \left( A \cup A^c \right) = P(\Omega) = 1$. Therefore, we can assert that $P \left( A \right) + P \left( A^c \right) = 1$.
\end{proof}

As a direct consequence of the aforementioned proposition, we can deduce the probability of the impossible event.

\begin{proposition}
The probability of the impossible event equals zero, that is, $P \left( \varnothing \right) = 0$.
\end{proposition}
\begin{proof}
Since $P \left( \varnothing \right) = 1 - P \left( \Omega \right) = 0$.
\end{proof}

As anticipated, sub-events (subsets) are associated with smaller probabilities than their corresponding events.

\begin{proposition}
Given that $A\subset B$, it follows that $P \left( A \right) \leq P \left( B \right)$.
\end{proposition}
\begin{proof}
The event $B$ can be dissected into the union of two disjoint events $A$ and $A^c \cap B$. Consequently, $P \left( B \right) = P \left( A \right) + P \left( A^c \cap B \right)$, which combined with the notion that  $P \left( A^c \cap B \right) \geq 0$, substantiates the proposition.
\end{proof}

With these fundamental elements established, we can demonstrate that probabilities range between zero and one.

\begin{proposition}
For each event $A$, $0 \leq P \left( A \right) \leq 1$.
\end{proposition}
\begin{proof}
By virtue of Axiom 1 and the consideration that $A \subset \Omega$ and consequently $P \left( A \right) \leq P \left( \Omega \right) = 1$.
\end{proof}

Axiom 3 provides the means to compute the probability of the union of disjoint events, however, it does not extend to scenarios involving non-disjoint events. The succeeding proposition illustrates the method for computing the probability of the union of non-disjoint events.

\begin{proposition}
For any two events $A$ and $B$, it follows that $P\left(A\cup B\right)=Pr\left(A\right)+Pr\left(B\right)-Pr\left(A\cap B\right)$.
\end{proposition}
\begin{proof}
The union of sets $A$ and $B$ can be represented as the union of two disjoint sets $A \cup B = B \cup \left( A \cap B^c \right)$. Given Axiom 3, we ascertain that
\[
P \left( A \cup B \right) = P \left( B \right) + P \left( A \cap B^c \right)
\]
Similarly, the set $A$ can be deconstructed as the union of the disjoint sets $A = \left( A \cap B \right) \cup \left( A \cap B^c \right)$. As a result, we get
\[
P \left( A \cup B^c \right) = P \left( A \right) - P \left( A \cap B \right)
\]
The combination of both expressions yields the desired result.
\end{proof}

The following equation extends to the scenario of $n$ events $A_1, \ldots, A_n$, employing the principle of inclusion-exclusion\index{Inclusion-exclusion principle} (see Section \ref{sec:counting}):
\begin{equation*}
\begin{split}
P \left( \bigcup_{i=1}^n A_i \right) & = \sum_{i=1}^n P \left( A_i \right) - \sum_{i<j} P \left( A_i \cap A_j \right) + \sum_{i<j<k} P \left( A_i \cap A_j \cap A_k \right) - \\
&  - \sum_{i<j<k<l} P \left( A_i \cap A_j \cap A_k \cap A_l \right) + \ldots +  (-1)^{n+1} P \left( A_1 \cap A_2 \cap \ldots \cap A_n \right) 
\end{split}
\end{equation*}

A probability mass function is characterized as a function that assigns to every possible event within a sample space its corresponding probability.

\begin{definition}
\label{def:probability_function}
Let $\left( \Omega, \mathcal{A} , P \right)$ be a discrete probability space. A \emph{probability mass function}\index{Probability mass function} is a real-valued function $f : \mathcal{A} \rightarrow [0, 1]$ that it assigns $f \left( A \right) = P \left( A \right)$ to every $A \in \mathcal{A}$.
\end{definition}

In Example \ref{ex:discrete_sample_space}, we introduced a discrete probability space $\left( \Omega, \mathcal{A}, P \right)$ comprising $n$ equally probable elements. The probability mass function associated with this experiment is defined as $f : \mathcal{A} \rightarrow [0, 1]$, such that $f \left( A \right) = d\left( A \right)/n$ for all $A \in \mathcal{A}$.

%
% Section: Conditional Probability
%

\section{Conditional Probability}
\label{sec:probability_conditional}

The principle of conditional probability is a cornerstone within the discipline of statistical learning. Conditional probability allow us refine the probability of an event based on new information or conditions. Under the axiomatization prescribed by Kolmogorov, conditional probability is introduced as a definition.

\begin{definition}
Let $A$ and $B$ be two events such that $P \left( B \right) > 0$. The \emph{conditional probability}\index{Conditional probability} of $A$ given $B$, denoted by $P \left( A \mid B \right)$, is defined as:
\[
P\left(A\mid B\right) = \frac{P\left(A\cap B\right)}{P\left(B\right)}
\]
\end{definition}

By virtue of satisfying the axioms, a conditional probability is, in itself, a probability. The conditional probability $P\left(A\mid B\right)$ is undefined in instances where $P\left(B\right)=0$.

Certain scholars posit that, given its pivotal role within probability theory, conditional probability ought to be an attribute that is logically deduced from the foundational axioms rather to be a definition. This perspective, naturally, necessitates an augmentation of Definition \ref{Kolmogorov_axioms}\index{Kolmogorov's axioms} with supplementary properties. Regrettably, there exists no agreed-upon method among mathematicians and philosophers regarding the manner in which this augmentation should be conducted.

The conventional interpretation of conditional probability posits it as the recalibrated probability of event $A$ following the occurrence of event $B$. This perspective, however, potentially implies a sequential or even causative linkage between events $B$ and $A$, a suggestion which may not necessarily hold validity.

\begin{example}
\label{ex:concurrent_events}
Suppose we are playing a game with a standard deck of 52 cards, and we draw two cards. Let event A be "drawing at least one heart" and event B be "drawing at least one queen". These two events are dependent since the occurrence of event B affects the probability of event A. However, these two events are not temporally related because the draw of the card happens at the same time - one event does not occur before the other. This example showcases the essence of dependency in probability theory without any temporal association between the events involved.
\end{example}

The probability of two events transpiring concurrently (although not necessarily contemporaneously, as previously discussed in Example \ref{ex:concurrent_events}), given their respective conditional probabilities, is encapsulated by the formula $P \left( A \cap B \right) = P \left( A \mid B \right) P \left( B \right)$. This equation offers perhaps a more intuitive comprehension of the conditional probability concept. Indeed, there exist a number of authors who advocate for this interpretation to form the basis of the definition of conditional probability, as opposed to the quotient method.

The extension of this formula to accommodate $n$ events, termed the \emph{multiplication rule}\index{Multiplication rule}, is expressed as follows:
\begin{equation}\label{eq:multiplication_rule}
P \left( A_{1} \cap A_{2} \cap \ldots \cap A_{n} \right) = P \left( A_{1} \right) P \left( A_{2} \mid A_{1}\right) \ldots  P \left( A_{n} \mid A_{1}\cap A_{2} \cap \ldots \cap A_{n-1} \right)
\end{equation}

The notion of event independence holds significant importance in the realm of probability theory and statistical learning. 

\begin{definition}\label{independent_events}\index{Independent events}
Two events $A$ and $B$ are declared to be \emph{independent} if $P \left( A \cap B \right) = P \left( A \right) P \left(B \right)$.
\end{definition}

From an intuitive perspective, the events $A$ and $B$ are considered independent if witnessing the occurrence of event B does not influence the probability of event A. This characteristic can be logically inferred from the definition of independence.

{\color{red} I think it is better start with this intuition, and then provide the defintion of independence. See Casella.}

\begin{proposition}
Given two events $A$ and $B$ such that $P \left( A \right) > 0$ and $P \left( B \right)>0$, $A$ and $B$ are independent if and only if $P \left( A \mid B\right) = P \left( A \right)$.
\end{proposition}
\begin{proof}
Assume $A$ and $B$ are independent, implying that $P \left( A \cap B \right) = P \left( A \right) P \left(B \right)$. Then,
\[
P \left( A \mid B \right) = \frac{P\left(A\cap B\right)}{P\left(B\right)} = \frac{P \left( A \right) P \left(B \right)}{P\left(B\right)} = P \left( A \right)
\]
Proceeding from the assumption that $P \left( A \mid B \right) = P \left( A \right)$, and utilizing the multiplication rule, it follows that
\[
P \left( A \cap B \right) =  P \left( A \mid B \right) P \left( B \right) = P \left( A \right) P \left( B \right)
\]
\end{proof}

If $P \left( A \mid B\right) = P \left( A \right)$ we have also that $P \left( B \mid  A \right) = P \left( B \right)$ which follow from the fact that the joint probability $P \left( A \cap B \right)$ can be expressed as $P \left( A \right) P \left(B \right)$.

Similar to the case of conditional probability, certain authors posit that independence, as a foundational concept in probability theory, ought to be a logical extension of the axioms, rather than being imposed as a definition.

The principle of independence can be expanded to accommodate multiple events: the events $A_{1}, \ldots, A_{n}$ are deemed to be independent (or mutually independent) if for every subset $A_{i_1}, \ldots, A_{i_j}$ comprising $j$ events $\left( j = 2, 3, \ldots, n \right)$, it holds true that $P \left( A_{i_1} \cap \ldots \cap A_{i_j} \right) = P \left( A_{i_1} \right) \ldots P \left( A_{i_j}\right)$.

{\color{red} Perhaps mention that it is not sufficient with pairwise independence.}

\begin{example}
A degree of confusion often arises regarding the distinction between mutually exclusive (or disjoint) events and independent events. For two mutually exclusive events $A$ and $B$, the computation of the probability that $A$ will transpire given $B$ is somewhat nonsensical, since if $B$ occurs, $A$ is inherently impossible. Analogously, discussing the conditional probability that $A$ will occur given $B$ when the probability of $B$ is zero is likewise flawed. However, as Definition \ref{independent_events} does not explicitly exclude the instance of $A$ and $B$ being mutually exclusive, we are compelled to conclude that two mutually exclusive events are independent if, and only if, the probability of at least one (or both) of them is zero.
\end{example}

An intriguing scenario arises when events $A$ and $B$ are not independent, yet attain independence contingent on the occurrence of another event $C$.

\begin{definition}
Consider $A$, $B$ and $C$ as events such that $P\left( B \cap C \right)>0$. $A$ and $B$ are considered \emph{conditionally independent}\index{Conditional independence} given $C$ if $P\left(A \mid B \cap C \right) = P\left( A \mid C \right)$.
\end{definition}

\begin{example}
Consider the act of rolling two dice; it is reasonable to assert that the outcomes of the two dice are independent from each other. That is, observing the outcome of one die provides no insight into the outcome of the other die. However, suppose the first die results in a four, and a third event is introduced - that the sum of the outcomes is an odd number - then this additional piece of information narrows the potential outcomes for the second die to only odd numbers. This illustrates the point that two events can be independent, yet fail to maintain conditional independence.
\end{example}

The following theorem presents Bayes' rule, a fundamental principle underpinning a significant statistical learning technique known as Bayesian inference (see Section \ref{sec:bayesian_inference}). 

\begin{theorem}[Bayes' Theorem]\index{Bayes' theorem}\label{th:Bayes_theorem} Let $A$ and $B$ be two events such that $P\left( B \right) \neq 0$. Then we have that
\[
P \left( A \mid B \right) = \frac{P \left( B \mid A \right) P \left( A \right)}{P \left( B \right)}
\]
In this context, $P\left( A \right)$ is referred to as the \emph{prior probability}\index{Prior probability}, while $P\left( A \mid B \right)$ is deemed the \emph{posterior probability}\index{Posterior probability}.
\end{theorem}
\begin{proof}
As per the definition of conditional probability, $P \left( A \mid B \right) = P \left( A \cap B \right) / P \left( B \right)$ (given $P \left( B \right) \neq 0$) and $P \left( B \mid A \right) = P \left( A \cap B \right) / P \left( A \right)$ (provided $P \left( A \right) \neq 0$). By solving for $P(A\cap B)$ and substituting into the previous expressions for $P(A\mid B)$, we arrive at the theorem.
\end{proof}

As shown in the proof, Bayes' theorem is directly derived from the definition of conditional probability. However, despite its practical usefulness, this raises important philosophical concerns about its foundational basis. If conditional probability is accepted as a definition rather than something derived from a collection of axioms, Bayes' theorem inherits this uncertainty. It functions within the limits of this definition, underscoring the idea that it is a tool built on a potentially arbitrary foundation rather than one grounded in more fundamental principles.

Bayesian inference facilitates the computation of how our degree of certainty about event $A$ (the prior probability $P\left( A \right)$) evolves when we acquire supplementary evidence via the occurrence of event $B$ (transforming into the posterior probability $P\left( A \mid B \right)$).

\begin{example}
Consider $E$ to be a disease affecting one in every million people, $P(E) = 1 \times 10^{-6}$, and let $+$ represent a test devised to detect the disease, with a failure rate of one in every thousand applications, $P(+ \mid E) = 999/1000$. We aim to determine the probability of disease presence if the test is positive $P(E \mid +)$. Upon employing Bayes' theorem, we find that:
\[
P(E \mid +) = \frac{P(+ \mid E) P(E)}{P(+)} = \frac{P(+ \mid E) P(E)}{P(+ \mid E) P(E) + P(+ \mid E^c) P(E^c)} = 0.001
\]
This implies that despite the test only failing once per thousand applications, it remains highly improbable that we have the disease following a positive result. This paradoxical outcome can be attributed to the higher probability of test failure $10^{-3}$ compared to the likelihood of disease occurrence $10^{-6}$. Practically, this issue is circumvented by applying a second test to individuals who received a positive result, as the probability of disease presence following two positive results is $0.5$ (under the assumption that the successive test repetitions are independent).
\end{example}

Bayes' theorem is most useful when the events involved are dependent and when new information about one event can update our understanding of the other event's probability.

\begin{example}
Suppose you're drawing a single card from a standard deck of 52 playing cards. Let Event $A$ be "drawing a red card" and Event $B$ be "drawing a queen". In this context, the use of Bayes' theorem to compute $P(A \mid B)$, the probability of drawing a red card given that a queen has been drawn, would not yield a meaningful result because the event $B$ provides no new information that would affect the probability of event $A$.
\end{example}

Bayes' theorem can indeed be extended to accommodate multiple events. Consider a set of events $A_{1}, \ldots, A_{k}$ such that $P\left( A_{j} \right)>0$ for all $j$ in the range of 1 to $k$. Suppose these events constitute a partition of the sample space $\Omega$. Now, let $B$ denote an event with the property that $P\left(B\right)>0$. In such a context, it can be deduced for each $i$ in the range of 1 to $k$ that the conditional probability $P\left(A_{i}\mid B\right)$ is given by the formula
\[
P\left(A_{i}\mid B\right)=\frac{P\left(B\mid A_{i}\right) P\left(A_{i}\right)}{\sum_{j=1}^{k} P\left(B \mid A_{j}\right) P\left(A_{j}\right)}
\]
This illustrates the capacity of Bayes' theorem to apply to a broader set of scenarios involving multiple events.

%
% Section: Random Variables
%

\section{Random Variables}
\label{sec:probability_random_variables}

A random variable is a function that assigns a real number to each possible outcome of an experiment, providing a quantitative representation of the results. Unlike the framework defined by Kolmogorov axioms, where probabilities are assigned to events, random variables simplify the process by directly associating numerical values with outcomes. This alleviates the limitations of assigning probabilities to events, offering a more intuitive and manageable approach, and facilitating the discovery of their analytical properties. Such is the efficacy of random variables that a majority of statisticians primarily consider their investigations within the framework of random variables as opposed to probability spaces.

\begin{definition}
Let $\left( \Omega, \mathcal{A} , P \right)$ be a discrete probability space. A \emph{random variable}\index{Random variable} is a function $X : \Omega \rightarrow \mathbb{R}$ mapping from the set of outcomes $\Omega$ to the real numbers $\mathbb{R}$. A random variable is discrete \index{Discrete random variable} if its range $\{ x_1, x_2, \ldots, x_i, \ldots \}$ is finite or countably infinite.
\end{definition}

Our primary focus here is on discrete random variables in discrete probability spaces. While it's possible to define a discrete random variable on a non-discrete probability space by giving it a discrete range, this particular case is not covered in this book.

The terminology "random variable" might potentially lead to some confusion. Firstly, these are not variables in the conventional algebraic sense, but rather, they are functions. Secondly, they are not inherently random; it is the experiment that they represent which possesses randomness. Despite these points of potential confusion, we adhere to the established terminology.

Random variables are more useful when they represent characteristics of the experiment. For instance, if the sample space consists of a school's student body, a random variable could associate each student with their respective height. Random variables also enable us to redistribute outcomes of the sample space into new events. For example, if two dice are rolled, a random variable could represent the sum of the dice's outcomes.

It is crucial to remember that we possess the liberty to assign a random variable to any sample space, even when the assignment might not seem intuitively meaningful. As an illustration, one could assign a numerical value to each possible color in a deck of cards, draw two cards randomly, and sum the assigned numbers of these two cards. Although such a setup may not yield a significant interpretation, it is nonetheless possible to calculate probabilities based on this setup.

\begin{definition}
Let $X : \Omega \rightarrow \mathbb{R}$ be a discrete random variable, and let $C \subset \mathbb{R}$ be a subset such that the set $\{ \omega \in \Omega \,:\, X \left( \omega \right) \in C\}$ constitutes an event. The probability of $X$ belonging to $C$, expressed as $P\left(X \in C \right)$, is given by $P\left( X \in C \right)=P \left( \left\{ \omega \in \Omega \,:\, X \left( \omega \right) \in C\right\} \right)$.
\end{definition}

The probability of a random variable $X$ essentially configures a probability space over the line of real numbers, specifically over the range of $X$. 

\begin{example}
\label{ex:probability_distribution_real_line}
Let $\Omega = \{1, 2, 3, 4, 5, 6\}$ be the sample space of tossing a die, and $P$ a probability that assigns $1/6$ to each single outcome in $\Omega$. Let $X: \Omega \rightarrow \mathbb{R}$ be a discrete random variable defined as:
    \[
    X(\omega) = 
    \begin{cases} 
      0 & \text{if } \omega \text{ is even (2, 4, 6)}, \\
      1 & \text{if } \omega \text{ is odd (1, 3, 5)}.
    \end{cases}
    \]
This discrete random variable maps the outcomes of the die toss to either 0 (if the outcome is even) or 1 (if the outcome is odd). For $C = \{0\}$, the probability $P(X \in C) = P(X = 0) = P(\{2, 4, 6\}) = 1/2$. For $C = \{1\}$ the probability $P(X \in C) = P(X = 1) = P(\{1, 3, 5\}) = 1/2$. Through this transformation, the original discrete probability space has been mapped to the real numbers using the discrete random variable $X$, establishing a new probability over $X$'s range.
\end{example}

Unless we say the contrary, we will assume that $\{ \omega \in \Omega \,:\, X \left( \omega \right) = x_i\}$ is an event for all the points that compose the range of $X$.

\begin{example}
$\left( \Omega, \mathcal{A} , P \right)$ be a discrete probability space, where $\Omega = \{1, 2\}$, $\mathcal{A} = \{\varnothing, \Omega\}$, and $P(\varnothing)=0$ and $P(\Omega)=1$. And let $X : \Omega \rightarrow \mathbb{R}$ be a discrete random variable defined as $X(1) = 1$ and $X(2) = 2$. We are interested in the probability $P(X=1)$, but since $\{ \omega \in \Omega \,:\, X \left( \omega \right) = 1 \} = \{1\}$ is not an event, such probability cannot be computed.
\end{example}

Definition \ref{def:probability_function} introduced the concept of probability mass function for probability spaces based on the probabilities of the events. Next definition extends the concept of probability mass function to random variables.

\begin{definition}
Let $X$ be a discrete random variable over a discrete probability space, and let $\{ x_1, x_2, \ldots \}$ be the range of $X$. The \emph{probability mass function}\index{Probability mass function} of the discrete random variable $X$, abbreviated as p.f., is defined as the function $f : range \left( X \right) \rightarrow [0, 1]$ such that $f \left( x_i \right) = P \left( X = x_i \right)$.
\end{definition}

The set of points for which the probability mass function is greater than zero, that is $\left\{ x \, : \, f \left( x \right) > 0 \right\}$, is called the \emph{support} of the distribution of $X$.

It is possible for two random variables to have identical probability mass functions but to differ in significant ways.

\begin{example}
Let $\Omega = \{H, T\}$ be the sample space of tossing a coin, and $P$ a probability that assigns $1/2$ to each outcome in $\Omega$. Let $X: \Omega \rightarrow \mathbb{R}$ be a discrete random variable defined as: $X = 1$ if the coin shows Head and $X = 0$ if coin shows Tail. The distribution of $X$ is $P(X = 1) = P(Y = 1) = 0.5$. The discrete random variables of this example and Example \ref{ex:probability_distribution_real_line} have same probability distribution even if they are different discrete random variables.
\end{example}

Given the probability mass function of a random variable, we can derive the probabilty of any subset of the real line.

\begin{proposition}
Let $X$ be a discrete random variable with probability mass function $f$. The probability of each subset $C$ of the real line can be determined from the relation $P\left(X\in C\right)=\sum_{x_{i}\in C}f\left(x_{i}\right)$
\end{proposition}
\begin{proof}
Considering that each outcome in the sample space is associated with exactly one value in the range $\{ x_1, x_2, \ldots, x_i, \ldots \}$ of $X$, we have that:
\[
P\left(X\in C\right) = P \left( \left\{ \omega \in \Omega \,:\, X \left( \omega \right) \in C\right\} \right) = \sum_{x_{i}\in C} P\left( X = x_i \right) = \sum_{x_{i}\in C}f\left(x_{i}\right)
\]
\end{proof}

Next proposition outlines a fundamental property, that the total sum of probabilities across all possible outcomes of a random variable is 1.

\begin{proposition}
Let $X$ be a discrete random variable with probability mass function $f$. If $\{ x_1, x_2, \ldots, \}$ is the range of $X$, then $\sum_{i=1}^{\infty}f\left(x_{i}\right)=1$.
\end{proposition}
\begin{proof}
Considering that $X$ is a total function, that each outcome in the sample space is associated with exactly one value in the range $\{ x_1, x_2, \ldots \}$, and given the axiomatic definition of probability we have that:
\begin{equation*}
\begin{split}
\sum_{i=1}^{\infty} f(x_i) & = f(x_1) + f(x_2) + \ldots = P\left( X = x_1 \right) + P\left( X = x_2 \right) + \ldots = \\
&  = P \left( \left\{ \omega \in \Omega \,:\, X \left( \omega \right) = x_1 \right\} \right) + P \left( \left\{ \omega \in \Omega \,:\, X \left( \omega \right) = x_2 \right\} \right) + \ldots = 1 
\end{split}
\end{equation*}
\end{proof}

{\color{red}
\begin{proposition}
A function $f(x)$ is a probability mass function of a random variable $X$ if, and only if, i) $f(x) \geq 0$ for all $x$, ii) $\sum_{i=1}^{\infty} f(x_i) = 1$.
\end{proposition}
\begin{proof}
TODO
\end{proof}
}

The cumulative distribution function represents the probability that a random variable takes on a value less than or equal to a specific point.

\begin{definition}
The \emph{cumulative distribution function} (abbreviated c.d.f.) $F$ of a discrete random variable $X$ is the function $F(x)=Pr(X\leq x)$ for all $-\infty < x < \infty$
\end{definition}

If $X$ follows a distribution characterized by the probability mass function $f(x)$, its cumulative distribution function $F(x)$ will exhibit the following behavior: at each distinct value $x_i$ of $X$, $F(x)$ will display a jump equal to $f(x_i)$; between these distinct values, $F(x)$ remains unchanged.

The cumulative distribution function allows us to see how probabilities accumulate over the range of a random variable, offering insights into the overall distribution of the data.

\begin{example}
Let's $X$ be a discrete random variable that represents the grades of students in a class. Each grade is between 0 and 10. The probability that a student receives a grade of $x$ is given by the probability mass function $p(x)$. The cumulative distribution function represents the probability that a randomly selected student scores $x$ or less. For example, a value of $F(7) =
0.6$ would mean that there's a 60\% chance a student picked at random scored 7 or below.
\end{example}

The cumulative distribution function of a random variable is non-decreasing. 

\begin{proposition}
Let $F(X)$ be the cumulative distribution function of a discrete random variable $X$. Then, if $x_{1}<x_{2}$ we have that $F\left(x_{1}\right)\leq F\left(x_{2}\right)$.
\end{proposition}
\begin{proof}
Given two values $x_1$ and $x_2$ where $x_1 < x_2$, the set of outcomes where $X \leq x_1$ is a subset of the outcomes where $X \leq x_2$.  Therefore, the probability of $X$ taking a value less than or equal to $x_1$ will be less than or equal to the probability of $X$ taking a value less than or equal to $x_2$. Then $F(x_1) \leq F(x_2)$.
\end{proof}

Next proposition delineates the asymptotic properties of the cumulative distribution function of a random variable, showcasing its bounds as we approach negative and positive infinity.

\begin{proposition}
Let $F(X)$ be the cumulative distribution function of a discrete random variable $X$. Then, we have that $\lim_{x\rightarrow-\infty}F\left(x\right)=0$ and that $\lim_{x\rightarrow\infty}F\left(x\right)=1$.
\end{proposition}
\begin{proof}
As $x$ tends to negative infinity, the probability that the discrete random variable $X$ takes on a value less than or equal to this increasingly smaller $x$ tends to zero. This is because there are fewer and fewer values (or none, depending on the specifics of the distribution) that $X$ can assume which are less than this increasingly negative $x$. Therefore:
\[
\lim_{x \rightarrow -\infty} F(x) = \lim_{x \rightarrow -\infty} P(X \leq x) = 0 
\]
As $x$ tends to positive infinity, the probability that the discrete random variable $X$ takes on a value less than or equal to this increasingly larger $x$ approaches 1. This is because, given the unbounded increase of $x$, it encapsulates all possible values that $X$ can take on. Therefore:
\[
\lim_{x \rightarrow \infty} F(x) = \lim_{x \rightarrow \infty} P(X \leq x) = 1
\]
\end{proof}

The probability of $X$ exceeding $x$ is given by the complement of the cumulative distribution function at that point.

\begin{proposition}
Let $F(X)$ be the cumulative distribution function of a discrete random variable $X$. Then, for every $x \in X$ we have that $P\left(X>x\right)=1-F\left(x\right)$.
\end{proposition}
\begin{proof}
The probability that $X$ takes on a value greater than $x$ plus the probability that $X$ takes on a value less than or equal to $x$ should sum up to 1. Given this, the probability that $X$ takes a value greater than $x$ is:
\[
P(X > x) = 1 - P(X \leq x)
\]
Using the definition of the cumulative distribution function, we get:
\[
P(X > x) = 1 - F(x) 
\]
\end{proof}

The following proposition establishes a relationship between the probabilities of a random variable \( X \) falling between two specific values and the corresponding differences in its cumulative distribution function values at those points.

\begin{proposition}
Let $F(X)$ be the cumulative distribution function of a discrete random variable $X$. Then, for all values $x_1, x_2 \in X$ such that $x_1 < x_2$ we have that $P\left(x_1 < X \leq x_2 \right) = F\left(x_2\right) - F\left(x_1\right)$
\end{proposition}
 \begin{proof}
The probability that $X$ is less than or equal to $x_2$ is $F(x_2)$. From this, if we subtract the probability that $X$ is less than or equal to $x_1$, which is $F(x_1)$, we'll get the probability that $X$ falls strictly between $x_1$ and $x_2$:
\[
P(x_1 < X \leq x_2) = F(x_2) - F(x_1)
\]
\end{proof}

% Multivariate Distributions

\subsection{Multivariate Distributions}

A multivariate probability distribution extends the concept of a probability distribution across multiple random variables, each with its own set of possible outcomes. Unlike univariate distributions that describe phenomena with a single random variable, multivariate distributions capture the relationships and dependencies between two or more variables. This allows for the exploration of complex phenomena where the outcome of interest is influenced by multiple factors simultaneously, providing insights into how these variables interact and impact the probability of various outcomes.

\begin{definition}
Let $X_1, X_2, \ldots, X_n$ be $n$ discrete random variables, where $X_i : \Omega_i \rightarrow \mathbb{R}$ for $i=1, \ldots, n$. The \emph{joint probability distribution}\index{Joint probability distribution} of $X_1, X_2, \ldots, X_n$ is defined as the collection of all probabilities of the form $P\left( \left( X_1, X_2, \ldots, X_n \right) \in C \right)$ for all sets $C \subset \mathbb{R}^n$ of real numbers such that $\left\{ \left( \omega_1, \omega_2, \ldots, \omega_n \right) \in \Omega_1 \times \ldots \times \Omega_n : \left( X_1 \left( \omega_1 \right), X_2 \left( \omega_2 \right), \ldots, X_n \left( \omega_n \right) \right) \in C \right\}$ is an event.
\end{definition}

The joint probability distribution of the random variables $X_1, X_2, \ldots, X_n$ defines a probability space in $\mathbb{R}^n$. If the random variables $X_1, X_2, \ldots, X_n$ each have a discrete distribution, then the joint distribution is also a discrete distribution.

\begin{definition}
Let $X_1, X_2, \ldots, X_n$ be $n$ discrete random variables over discrete probability spaces. The \emph{joint probability mass function}\index{Joint probability mass function} of the discrete random variables $X_1, X_2, \ldots, X_n$ is defined as the function $f : \text{range} \left( X_1 \right) \times \ldots \times \text{range} \left( X_n \right) \rightarrow [0, 1]$ such that $f \left( x_1, \ldots, x_n \right) = P \left( X_1 = x_1, \ldots, X_n = x_n \right)$.
\end{definition}

\begin{example}
A classic example of a bivariate discrete joint distribution involves rolling two six-sided dice. Let's define two discrete random variables: $X_1$ is the outcome of the first die, and $X_2$ is the outcome of the second die. Both $X_1$ and $X_2$ have a discrete uniform distribution over the set $\{1, 2, 3, 4, 5, 6\}$. The joint distribution of $X_1$ and $X_2$ describes the probability of each possible pair of outcomes when the two dice are rolled. The joint probability mass function $f(x_1, x_2)$ for $X_1$ and $X_2$ can be expressed as:
\[
f(x_1, x_2) = P(X_1 = x_1, X_2 = x_2) = \frac{1}{36}, \quad \text{for} \ x_1, x_2 \in \{1, 2, 3, 4, 5, 6\}
\]
The joint distribution allows us to analyze the relationship between $X_1$ and $X_2$. For instance, we can compute the probability that the sum of the two dice is equal to a certain number, or that one die shows a higher number than the other.
\end{example}

Before exploring the properties of multivariate random variables, we will introduce the concept of random vector. This concept simplifies notation and enhances clarity by grouping multiple random variables into a single entity.

\begin{definition}
A \emph{discrete random vector}\index{Discrete random vector} $\mathbf{X}$ is an ordered collection of $n$ discrete random variables $X_1, X_2, \ldots, X_n$, where $X_i : \Omega_i \rightarrow \mathbb{R}$ for all $i=1, \ldots, n$.
\end{definition}

Given the joint probability mass function of a vector of random variables, we can derive the probability of any subset within the multidimensional real space.

\begin{proposition}
Let $\mathbf{X}=\left(X_{1}, \ldots, X_{n}\right)$ be a discrete random vector with a joint probability mass function $f$. The probability of each subset $C$ of the $n$-dimensional real space can be determined from the relation $P\left(\mathbf{X} \in C\right)=\sum_{\mathbf{x} \in C}f\left(\mathbf{x}\right)$, where $\mathbf{x} = (x_{1}, \ldots, x_{n})$.
\end{proposition}
\begin{proof}
Considering that each outcome in the sample space is associated with exactly one value in the range of $\mathbf{X}$, we have that:
\[
P\left(\mathbf{X} \in C\right) = P \left( \left\{ \omega \in \Omega \,:\, \mathbf{X}(\omega) \in C\right\} \right) = \sum_{\mathbf{x}\in C} P\left( \mathbf{X} = \mathbf{x} \right) = \sum_{\mathbf{x}\in C}f\left(\mathbf{x}\right)
\]
\end{proof}

The next proposition outlines a fundamental property, that the total sum of probabilities across all possible outcomes of a vector of random variables is 1.

\begin{proposition}
Let $\mathbf{X}=\left(X_{1}, \ldots, X_{n}\right)$ be a discrete random vector with a joint probability mass function $f$. If the range of $\mathbf{X}$ is represented as a set of vectors $\mathbf{x} = (x_{1}, \ldots, x_{n})$, then $\sum_{\mathbf{x}}f\left(\mathbf{x}\right)=1$.
\end{proposition}
\begin{proof}
Considering that $\mathbf{X}$ is a total function, that each outcome in the sample space is associated with exactly one value in the range of $\mathbf{X}$, and given the axiomatic definition of probability we have that:
\[
\sum_{\mathbf{x}} f(\mathbf{x}) = \sum_{\mathbf{x}} P\left( \mathbf{X} = \mathbf{x} \right) = P \left( \left\{ \omega \in \Omega_1 \,:\, \mathbf{X}_1(\omega) = \mathbf{x_1} \right\} \right) + P \left( \left\{ \omega \in \Omega_2 \,:\, \mathbf{X}_2(\omega) = \mathbf{x_2} \right\} \right) + \ldots = 1 
\]
\end{proof}

A particular interesting case of multivariate distribution is given by the sum of $n$ random variables. This is a little bit confusing scenario, since we are not adding $n$ probability distributions, as the notation $X_1 + \ldots + X_n$ might suggest. Instead, we are defining a new random variable over the cartesian product of the original sample spaces.

\begin{definition}
Let $X_1, X_2, \ldots, X_n$ be $n$ discrete random variables, where $X_i : \Omega_i \rightarrow \mathbb{R}$ for all $i=1, \ldots, n$. The sum distribution of $X_1, X_2, \ldots, X_n$, denoted by $X_1 + \ldots + X_n$, is defined as the discrete random variable $X + \ldots + X_n : \Omega_1 \times \ldots \times \Omega_n \rightarrow \mathbb{R}$ that assigns to each $\left( \omega_1, \omega_2, \ldots, \omega_n \right) \in \Omega_1 \times \ldots \Omega_n$ the number $X_1 \left( \omega_1 \right) + X_2 \left( \omega_2 \right) + \ldots + X_n \left( \omega_n \right)$.
\end{definition}

The concept of sum of $n$ randon variables will be applied in the law of large numbers (see Theorem \ref{th:law_large_numbers}), one of the most important theorems in probability theory.

\begin{example}
Let $X: \Omega \rightarrow \mathbb{R}$ be the discrete random variable representing the outcome of rolling a six-sided dice. The sum distribution corresponding to rolling three times a dice, denoted by $S = X + X + X$, is defined as the discrete random variable $S: \Omega \times \Omega \times \Omega \rightarrow \mathbb{R}$ that assigns to each $\left( \omega_1, \omega_2, \omega_3 \right) \in \Omega \times \Omega \times \Omega$ the number $X \left( \omega_1 \right) + X \left( \omega_2 \right) + X \left( \omega_3 \right)$.
\end{example}

% Marginal Distribution

\subsection{Marginal Probability Mass Function}

%% Popular Science

Given a multivariate discrete probability mass function, the marginal probability mass function of a subset of variables is derived by summing the joint probability mass function over all possible values of the remaining variables. This process essentially "marginalizes" out the variables not of interest, allowing focus on the probability mass function of a single variable or a subset of variables within the multivariate context.

\begin{definition}
Let $\mathbf{X} = (X_{1}, X_{2}, \ldots, X_{n})$ be an $n$-dimensional random vector with joint probability mass function $f_{\mathbf{X}}(x_{1}, x_{2}, \ldots, x_{n})$, partition $\mathbf{X}$ into two subvectors: $\mathbf{Y} = (Y_{1}, Y_{2}, \ldots, Y_{k})$, a $k$-dimensional random vector consisting of $k$ discrete random variables selected from $\mathbf{X}$, and $\mathbf{Z} = (Z_{1}, Z_{2}, \ldots, Z_{n-k})$, the remaining $(n - k)$ discrete random variables of $\mathbf{X}$. The \emph{marginal probability mass function}\index{Marginal probability mass function} $f_{\mathbf{Y}}$ of the random vector $\mathbf{Y}$ is obtained by summing $f_{\mathbf{X}}$ over all possible values of the variables in $\mathbf{Z}$. That is, for any specific values $(y_{1}, y_{2}, \ldots, y_{k})$ of $\mathbf{Y}$,
\[
f_{\mathbf{Y}}(y_{1}, y_{2}, \ldots, y_{k}) = \sum_{z_{1}} \sum_{z_{2}} \ldots \sum_{z_{n-k}} f_{\mathbf{X}}(x_{1}, x_{2}, \ldots, x_{n}),
\]
where in each term of the sum, $x_{i} = y_{i}$ for $i$ corresponding to variables in $\mathbf{Y}$, and $x_{j} = z_{j}$ for $j$ corresponding to variables in $\mathbf{Z}$.
\end{definition}

This definition underscores the process of marginalization in a discrete setting, which is key to understanding and analyzing the behavior of specific variables within a larger multivariate framework.

\begin{example}
Consider two discrete random variables $X$ and $Y$, each taking values in $\{0, 1\}$. Suppose their joint probability mass function is given by:
\[
\begin{array}{c|cc}
\hline
X \backslash Y & 0 & 1 \\
\hline
0 & 0.1 & 0.3 \\
1 & 0.2 & 0.4 \\
\hline
\end{array}
\]
To find the marginal probability mass function of $X$, we sum over all possible values of $Y$:
\[
\begin{aligned}
f_{X}(0) &= f_{X,Y}(0,0) + f_{X,Y}(0,1) = 0.1 + 0.3 = 0.4, \\
f_{X}(1) &= f_{X,Y}(1,0) + f_{X,Y}(1,1) = 0.2 + 0.4 = 0.6.
\end{aligned}
\]
\end{example}

While the marginal probability mass functions of the discrete random variables $X_{1}, \ldots, X_{n}$ can be obtained from their joint probability mass function by summing over the range of the other variables, the reverse process is not straightforward. Specifically, reconstructing the joint probability mass function of $X_{1}, \ldots, X_{n}$ from their marginal probability mass functions alone is not feasible without extra information about the dependence between $X_{1}, \ldots, X_{n}$. This limitation arises because marginal probability mass functions encapsulate only the individual behavior of each variable, omitting details about how the variables interact or are related.

\begin{example}
Let $X$ and $Y$ be discrete random variables, each taking values in $\{0, 1\}$, with marginal probability mass functions:
\[
\begin{aligned}
f_{X}(0) &= 0.5, & f_{X}(1) &= 0.5, \\
f_{Y}(0) &= 0.5, & f_{Y}(1) &= 0.5.
\end{aligned}
\]
Without additional information about the dependence structure between $X$ and $Y$, the joint probability mass function cannot be reconstructed from the marginals alone.
\end{example}

A random vector $\mathbf{X}$ is considered independent if the occurrence of an event associated with any of the random variables of $\mathbf{X}$ does not influence the probability of an event associated with any other variable in $\mathbf{X}$. Independence among these variables indicates that there is no association or correlation among them, implying that knowing the outcome of one provides no information about the outcomes of the others.

\begin{definition}\label{def:independent_random_variables}
An $n$-dimensional random vector $\mathbf{X} = (X_{1}, X_{2}, \ldots, X_{n})$ is said to consist of \emph{independent random variables}\index{Independent random variables} if for every choice of subsets $A_{1}, A_{2}, \ldots, A_{n}$ of $\mathbb{R}$ such that $\{ X_{i} \in A_{i} \}$ is an event for $i = 1, 2, \ldots, n$, the joint probability of these events can be expressed as the product of their individual probabilities:
\[
P\left( X_{1} \in A_{1}, X_{2} \in A_{2}, \ldots, X_{n} \in A_{n} \right) = P\left( X_{1} \in A_{1} \right) P\left( X_{2} \in A_{2} \right) \ldots P\left( X_{n} \in A_{n} \right).
\]
\end{definition}

The concept of independence for a random vector $\mathbf{X}$ simplifies the computation and understanding of joint probability distributions, particularly in complex problems involving multiple variables. It allows the joint probability distribution of the vector $\mathbf{X}$ to be expressed as the product of the individual marginal distributions of $X_{1}, X_{2}, \ldots, X_{n}$.

\begin{proposition}
Let $\mathbf{X} = (X_{1}, X_{2}, \ldots, X_{n})$ be an $n$-dimensional random vector with joint probability mass function $f_{\mathbf{X}}$ and marginal probability mass functions $f_{X_{1}}, f_{X_{2}}, \ldots, f_{X_{n}}$. The random variables $X_{1}, X_{2}, \ldots, X_{n}$ are independent if and only if for every $(x_{1}, x_{2}, \ldots, x_{n})$ in the support of $\mathbf{X}$, we have:
\[
f_{\mathbf{X}}(x_{1}, x_{2}, \ldots, x_{n}) = f_{X_{1}}(x_{1}) f_{X_{2}}(x_{2}) \ldots f_{X_{n}}(x_{n}).
\]
\end{proposition}
\begin{proof}
Assume that $X_{1}, X_{2}, \ldots, X_{n}$ are independent. Then, for any values $x_{1}, x_{2}, \ldots, x_{n}$, we have:
\[
\begin{aligned}
f_{\mathbf{X}}(x_{1}, x_{2}, \ldots, x_{n}) &= P(X_{1} = x_{1}, X_{2} = x_{2}, \ldots, X_{n} = x_{n}) \\
&= P(X_{1} = x_{1}) P(X_{2} = x_{2}) \ldots P(X_{n} = x_{n}) \\
&= f_{X_{1}}(x_{1}) f_{X_{2}}(x_{2}) \ldots f_{X_{n}}(x_{n}).
\end{aligned}
\]
Conversely, assume that for all $x_{1}, x_{2}, \ldots, x_{n}$,
\[
f_{\mathbf{X}}(x_{1}, x_{2}, \ldots, x_{n}) = f_{X_{1}}(x_{1}) f_{X_{2}}(x_{2}) \ldots f_{X_{n}}(x_{n}).
\]
Then, for any subsets $A_{1}, A_{2}, \ldots, A_{n}$ of $\mathbb{R}$, we have:
\[
\begin{aligned}
P\left( X_{1} \in A_{1}, X_{2} \in A_{2}, \ldots, X_{n} \in A_{n} \right) &= \sum_{(x_{1}, x_{2}, \ldots, x_{n}) \in A_{1} \times A_{2} \times \ldots \times A_{n}} f_{\mathbf{X}}(x_{1}, x_{2}, \ldots, x_{n}) \\
&= \sum_{x_{1} \in A_{1}} \sum_{x_{2} \in A_{2}} \ldots \sum_{x_{n} \in A_{n}} f_{X_{1}}(x_{1}) f_{X_{2}}(x_{2}) \ldots f_{X_{n}}(x_{n}) \\
&= \left( \sum_{x_{1} \in A_{1}} f_{X_{1}}(x_{1}) \right) \left( \sum_{x_{2} \in A_{2}} f_{X_{2}}(x_{2}) \right) \ldots \left( \sum_{x_{n} \in A_{n}} f_{X_{n}}(x_{n}) \right) \\
&= P(X_{1} \in A_{1}) P(X_{2} \in A_{2}) \ldots P(X_{n} \in A_{n}).
\end{aligned}
\]
This equality holds for all subsets $A_{1}, A_{2}, \ldots, A_{n}$, which implies that $X_{1}, X_{2}, \ldots, X_{n}$ are independent.
\end{proof}

% Conditional Distributions

\subsection{Conditional Probability Mass Function}

The concept of conditional probability mass function offers a way to understand the probability of an event given that another event has occurred. In the particular case of discrete random variables, the conditional probability mass function of $Y$ given $X=x$ describes the probability mass function of $Y$ under the condition that $X$ takes a specific value $x$. This concept is pivotal for dissecting the interdependencies between discrete random variables, allowing us to refine our probability assessments based on new information.

\begin{definition}
\label{def:conditional_probability_function}
Let \( \mathbf{X} = (X_{1}, X_{2}, \ldots, X_{n}) \) be an \( n \)-dimensional random vector with joint probability mass function \( f_{\mathbf{X}}(x_{1}, x_{2}, \ldots, x_{n}) \). Partition \( \mathbf{X} \) into two subvectors: \( \mathbf{Y} = (Y_{1}, Y_{2}, \ldots, Y_{k}) \), a \( k \)-dimensional random vector consisting of \( k \) discrete random variables selected from \( \mathbf{X} \), and \( \mathbf{Z} = (Z_{1}, Z_{2}, \ldots, Z_{n-k}) \), the remaining \( n - k \) discrete random variables of \( \mathbf{X} \). Let \( f_{\mathbf{Z}} \) represent the marginal probability mass function of \( \mathbf{Z} \) across its \( n - k \) dimensions. Provided that for any vector \( \mathbf{z} \in \mathbb{R}^{n - k} \) the condition \( f_{\mathbf{Z}}(\mathbf{z}) > 0 \) holds, the \emph{conditional probability mass function}\index{Conditional probability mass function} \( f_{\mathbf{Y}|\mathbf{Z}} \) for \( \mathbf{Y} \) given \( \mathbf{Z} = \mathbf{z} \) is defined as:
\[
f_{\mathbf{Y}|\mathbf{Z}} ( \mathbf{y} \mid \mathbf{z} ) = \frac{f_{\mathbf{X}} ( \mathbf{y}, \mathbf{z} )}{f_{\mathbf{Z}} ( \mathbf{z} )}
\]
\end{definition}

Next example demonstrates how to compute the conditional probability mass function of \( Y \) given \( X \).

\begin{example}
Consider two discrete random variables \( X \) and \( Y \), each taking values in \( \{0, 1\} \), with the following joint probability mass function:
\[
\begin{array}{c|cc}
\hline
X \backslash Y & 0 & 1 \\
\hline
0 & 0.3 & 0.2 \\
1 & 0.1 & 0.4 \\
\hline
\end{array}
\]
We can compute the marginal probability mass function of \( X \):
\[
\begin{aligned}
f_{X}(0) &= f_{X,Y}(0,0) + f_{X,Y}(0,1) = 0.3 + 0.2 = 0.5, \\
f_{X}(1) &= f_{X,Y}(1,0) + f_{X,Y}(1,1) = 0.1 + 0.4 = 0.5.
\end{aligned}
\]
Suppose we want to find the conditional probability mass function of \( Y \) given \( X = 0 \), denoted \( f_{Y|X}(y \mid x) \). For \( x = 0 \) we have:
\[
\begin{aligned}
f_{Y|X}(0 \mid 0) &= \frac{f_{X,Y}(0,0)}{f_{X}(0)} = \frac{0.3}{0.5} = 0.6, \\
f_{Y|X}(1 \mid 0) &= \frac{f_{X,Y}(0,1)}{f_{X}(0)} = \frac{0.2}{0.5} = 0.4.
\end{aligned}
\]
For \( x = 1 \) we have:
\[
\begin{aligned}
f_{Y|X}(0 \mid 1) &= \frac{f_{X,Y}(1,0)}{f_{X}(1)} = \frac{0.1}{0.5} = 0.2, \\
f_{Y|X}(1 \mid 1) &= \frac{f_{X,Y}(1,1)}{f_{X}(1)} = \frac{0.4}{0.5} = 0.8.
\end{aligned}
\]
\end{example}

Next proposition generalizes the multiplication rule (see Equation \ref{eq:multiplication_rule}) by combining marginal and conditional probability mass functions to derive the joint probability mass function for any configuration of discrete random variables within a random vector, accounting for the complex dependencies and interactions among multiple variables.

\begin{proposition}
Let \( \mathbf{X} \), \( \mathbf{Y} \), \( \mathbf{Z} \), \( f_{\mathbf{X}}(\mathbf{x}) \), \( f_{\mathbf{Z}}(\mathbf{z}) \), and \( f_{\mathbf{Y}|\mathbf{Z}}(\mathbf{y} \mid \mathbf{z}) \) be defined as in Definition \ref{def:conditional_probability_function}. Then, for each \( \mathbf{z} \) such that \( f_{\mathbf{Z}}(\mathbf{z}) > 0 \) and each possible value of \( \mathbf{y} \), the joint probability mass function is given by:
\[
f_{\mathbf{X}}(\mathbf{x}) = f_{\mathbf{Y}|\mathbf{Z}}(\mathbf{y} \mid \mathbf{z}) f_{\mathbf{Z}}(\mathbf{z})
\]
where \( \mathbf{x} = (\mathbf{y}, \mathbf{z}) \) represents a specific instantiation of the random vector \( \mathbf{X} \).
\end{proposition}
\begin{proof}
By the definition of the conditional probability mass function:
\[
f_{\mathbf{Y}|\mathbf{Z}}(\mathbf{y} \mid \mathbf{z}) = \frac{f_{\mathbf{X}}(\mathbf{y}, \mathbf{z})}{f_{\mathbf{Z}}(\mathbf{z})}, \quad \text{for } f_{\mathbf{Z}}(\mathbf{z}) > 0.
\]
Rearranging this equation gives:
\[
f_{\mathbf{X}}(\mathbf{y}, \mathbf{z}) = f_{\mathbf{Y}|\mathbf{Z}}(\mathbf{y} \mid \mathbf{z}) f_{\mathbf{Z}}(\mathbf{z}).
\]
Since \( \mathbf{x} = (\mathbf{y}, \mathbf{z}) \), we have:
\[
f_{\mathbf{X}}(\mathbf{x}) = f_{\mathbf{Y}|\mathbf{Z}}(\mathbf{y} \mid \mathbf{z}) f_{\mathbf{Z}}(\mathbf{z}).
\]
\end{proof}

Similarly, the conditional probability mass function of \( \mathbf{Z} \) given \( \mathbf{Y} = \mathbf{y} \), denoted as \( f_{\mathbf{Z}|\mathbf{Y}}(\mathbf{z} \mid \mathbf{y}) \), can be combined with the marginal probability mass function of \( \mathbf{Y} \), \( f_{\mathbf{Y}}(\mathbf{y}) \), to yield the same joint probability mass function of \( \mathbf{X} \):
\[
f_{\mathbf{X}}(\mathbf{x}) = f_{\mathbf{Z}|\mathbf{Y}}(\mathbf{z} \mid \mathbf{y}) f_{\mathbf{Y}}(\mathbf{y}).
\]

Bayes' theorem (see Theorem \ref{th:Bayes_theorem}) provides a way to update our probability estimates for a hypothesis given new evidence. For random vectors, the theorem can be generalized to accommodate the multi-dimensional nature of the variables involved. 

\begin{theorem}[Bayes' Theorem for Random Vectors]\index{Bayes' theorem}
Let \( \mathbf{X} = (X_1, X_2, \ldots, X_n) \) and \( \mathbf{Y} = (Y_1, Y_2, \ldots, Y_m) \) be two random vectors representing different sets of discrete random variables. Suppose we are interested in the conditional probability distribution of \( \mathbf{X} \) given observed values of \( \mathbf{Y} \), denoted as \( \mathbf{y} = (y_1, y_2, \ldots, y_m) \). The generalized Bayes' theorem for random vectors can be stated as:
\[
P(\mathbf{X} = \mathbf{x} \mid \mathbf{Y} = \mathbf{y}) = \frac{P(\mathbf{Y} = \mathbf{y} \mid \mathbf{X} = \mathbf{x}) P(\mathbf{X} = \mathbf{x})}{P(\mathbf{Y} = \mathbf{y})}
\]
where \( P(\mathbf{Y} = \mathbf{y}) \) is the marginal probability of \( \mathbf{Y} \), which can also be expressed using the law of total probability as:
\[
P(\mathbf{Y} = \mathbf{y}) = \sum_{\mathbf{x}} P(\mathbf{Y} = \mathbf{y} \mid \mathbf{X} = \mathbf{x}) P(\mathbf{X} = \mathbf{x})
\]
for discrete random vectors.
\end{theorem}
\begin{proof}
By the definition of conditional probability:
\[
P(\mathbf{X} = \mathbf{x} \mid \mathbf{Y} = \mathbf{y}) = \frac{P(\mathbf{X} = \mathbf{x}, \mathbf{Y} = \mathbf{y})}{P(\mathbf{Y} = \mathbf{y})}.
\]
Also, we can express the joint probability as:
\[
P(\mathbf{X} = \mathbf{x}, \mathbf{Y} = \mathbf{y}) = P(\mathbf{Y} = \mathbf{y} \mid \mathbf{X} = \mathbf{x}) P(\mathbf{X} = \mathbf{x}).
\]
Substituting back into the first equation:
\[
P(\mathbf{X} = \mathbf{x} \mid \mathbf{Y} = \mathbf{y}) = \frac{P(\mathbf{Y} = \mathbf{y} \mid \mathbf{X} = \mathbf{x}) P(\mathbf{X} = \mathbf{x})}{P(\mathbf{Y} = \mathbf{y})}.
\]
\end{proof}

This generalized form of Bayes' theorem allows us to update our belief about the probability distribution of a set of discrete random variables \( \mathbf{X} \) based on new information encapsulated in another set of discrete random variables \( \mathbf{Y} \). It emphasizes the interplay between the prior information we have about \( \mathbf{X} \), the likelihood of observing \( \mathbf{Y} = \mathbf{y} \) given \( \mathbf{X} = \mathbf{x} \), and the evidence provided by the actual observation of \( \mathbf{Y} = \mathbf{y} \).

Building on the familiar concept of independence between discrete random variables (see Defintion \ref{def:independent_random_variables}), an important extension is the idea of conditional independence. This concept comes into play when the independence of a set of discrete random variables is considered in the context of being conditioned on another set of variables.

\begin{definition}
Let \( \mathbf{Z} \) be a random vector with joint probability mass function \( f_{\mathbf{Z}}(\mathbf{z}) \). The variables of the random vector \( \mathbf{X} = (X_{1}, \ldots, X_{n}) \) are \emph{conditionally independent}\index{Conditional independence of random variables} given \( \mathbf{Z} \) if, for all \( \mathbf{z} \) such that \( f_{\mathbf{Z}}(\mathbf{z}) > 0 \), we have:
\[
f_{\mathbf{X} \mid \mathbf{Z}} (\mathbf{x} \mid \mathbf{z}) = \prod_{i=1}^{n} f_{X_i \mid \mathbf{Z}}(x_{i} \mid \mathbf{z}),
\]
where \( f_{\mathbf{X} \mid \mathbf{Z}}(\mathbf{x} \mid \mathbf{z}) \) is the conditional probability mass function of \( \mathbf{X} \) given \( \mathbf{Z} = \mathbf{z} \), and \( f_{X_i \mid \mathbf{Z}}(x_{i} \mid \mathbf{z}) \) is the conditional probability mass function of \( X_{i} \) given \( \mathbf{Z} = \mathbf{z} \).
\end{definition}

%
% Section: Characterizing Distributions
%

\section{Characterizing Distributions}
\label{sec:probability_expectation}

A \emph{measure of central tendency} is a number derived from a probability distribution, intended as a summary of that distribution. The most common measures of central tendency in use are the expected value and the median. Each of these measures provides a different approach to characterize distributions. It is also common to use \emph{metrics of dispersion} to describe the variability of a distribution around the measures of centrality. We will review two metrics of dispersion, the variance and the standard deviation. The metrics of disperson can also be used in case of bivariate distributions, under the names of covariance and correlation, to measure the \emph{statistical relationship} between two discrete random variables. All these measures allow us to summarize and compare distributions.

%
% Subsection: Measures of Central Tendency
%

\subsection{Measures of Central Tendency}

The most common measures of central tendency in use to characterize probability distributions are the expected value and the median.

% Expected Value

\subsubsection*{Expeced Value}

The expected value of a discrete random variable is computed as the weighted average of all possible values of the variable, where weights are given by the probabilities of the outcomes.

\begin{definition}\label{probability:expectation}
Let $X$ be a discrete random variable whose probability mass function is $f$. The \emph{expected value}\index{Expected value} of $X$, denoted by $E\left(X\right)$, is defined as:
\[
E\left(X\right)=\sum_{x}xf\left(x\right)
\]
\end{definition}

The definition of expected value considers only the distribution of the discrete random variable, not the original outcomes. Thus, two different discrete random variables with the same distribution will have equal expected values, even if the underlying discrete probability spaces are different.

The term "expected value" can be somewhat misleading because, for many discrete distributions, the expected value is not necessarily one of the possible values. For instance, when rolling a six-sided die, the expected value is \(3.5\), which is not an actual outcome of the roll. This counterintuitive aspect of the concept of expected value has led to considerable confusion in scientific research.

A drawback of the expected value is that it can be greatly affected by a small change in the probability assigned to a large value of $X$.

\begin{example}\label{ex:expected_salary}
Consider a company with a population of 100 employees, and we define a discrete random variable based on their salaries. Let's $X = \{300, 6000\}$ with probabilities of $99/100$ and $1/100$ respectively. The expected salary is calculated as:
\[ 
E\left(X\right) = (300 \times 99/100) + (6000 \times 1/100) = 357
\]
Now, suppose that one of the base employees is promoted to the executive level with the same salary of \$6000. The recalculated expected salary would be:
\[
E\left(X\right) = (300 \times 98/10) + (6000 \times 2/100) = 414
\]
This example shows how just changing the salary of one single person can increase the expected salary of the company by more than $13\%$.
\end{example}

The expected value of the linear combination of $n$ discrete random variables is the linear combination of their respective expected values.

\begin{proposition}
Let $X_{1}, \ldots, X_{n}$ be $n$ independent discrete random variables with expectations $E\left(X_{i}\right)$, and let $a_1, \ldots, a_n$ and $b$ constants, then
\[
E\left(a_{1}X_{1}+\ldots+a_{n}X_{n}+b\right)=a_{1}E\left(X_{1}\right)+\ldots+a_{n}E\left(X_{n}\right)+b
\]
\end{proposition}
\begin{proof}
We have that
\begin{multline}
E \left(a_1 X_1 + \ldots + a_n X_n +b \right) = 
\sum_{x_1} \ldots \sum_{x_n} \left(a_ 1 x_1 + \ldots + a_n x_n + b  \right) f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} \ldots \sum_{x_n} a_1 x_1 f\left(x_1, \ldots, x_n \right) + \ldots + \sum_{x_1} \ldots \sum_{x_n} a_n x_n f\left(x_1, \ldots, x_n \right) + \sum_{x_1} \ldots \sum_{x_n} b f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} a_1 x_1 f\left(x_1\right) + \ldots + \sum_{x_n} a_n x_n f\left( x_n \right) + b = 
a_1 \sum_{x_1} x_1 f\left(x_1\right) + \ldots + a_n \sum_{x_n} x_n f\left( x_n \right) + b = \\
a_1 E\left(X_1\right) + \ldots + a_n E\left(X_n\right) + b
\end{multline}
\end{proof}

The expected value of the product of $n$ independent discrete random variables is equal to the product of the individual expected values.

\begin{proposition}
Let $X_{1}, \ldots, X_{n}$ be $n$ independent discrete random variables with expectations $E\left(X_{i}\right)$, then:
\[
E\left(\prod_{i=1}^{n}X_{i}\right)=\prod_{i=1}^{n}E\left(X_{i}\right)
\]
\end{proposition}
\begin{proof}
We have that
\begin{multline}
E \left(X_1  \cdot \ldots \cdot X_n  \right) = 
\sum_{x_1} \ldots \sum_{x_n} \left(x_1 \cdot \ldots \cdot x_n  \right) f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} \ldots \sum_{x_n} x_1 f\left(x_1, \ldots, x_n \right) \cdot \ldots \cdot \sum_{x_1} \ldots \sum_{x_n} x_n f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} x_1 f\left(x_1\right) \cdot \ldots \cdot \sum_{x_n} x_n f\left( x_n \right) = 
E \left( X_1 \right) \cdot \ldots \cdot E \left( X_n \right)
\end{multline}
\end{proof}

The expected value of the product of non-independent discrete random variables is not necesarily equal to the product of their individual expected values.

% The Median

\subsubsection*{The Median}

The median of a discrete random variable is a measure of central tendency that represents the point that separates the higher half from the lower half of a probability distribution.

\begin{definition}
Let $X$ be a discrete random variable. The \emph{median}\index{Median} of $X$, denoted by $m$, is the value that satisfies:
\[
Pr\left(X\leq m\right)\geq \frac{1}{2} \quad \text{and} \quad Pr\left(X\geq m\right)\geq \frac{1}{2}
\]
\end{definition}

The definition of the median considers the distribution of the discrete random variable, ensuring that at least half of the probability mass lies on either side of the median.

The "median" is often more intuitive than "expected value" because it is always one of the possible values, although in the case of an even number of outcomes it is customary to use the average of the two middle values.

The median is a robust measure of central tendency, especially useful when the data has outliers or is skewed.

\begin{example}
Consider the company from Example \ref{ex:expected_salary}. The median of the salaries is calculated as the value $m$ for which $Pr\left(X\leq m\right)\geq \frac{1}{2}$. In this case, $m=300$. Now, suppose that one of the base employees is promoted to the executive level with the same salary of \$6000. The recalculated median would still be $m=300$. This example shows that the median is a more robust measure than the expected value in the presence of outliers.
\end{example}

%
% subsection: Measures of Dispersion
%

\subsection{Measures of Dispersion}

The most common measures of dispersion in use to characterize probability distributions are the variance and its squared root, called standard deviation.

% The Variance

\subsubsection*{The Variance}

The variance of a discrete random variable is a measure of the spread or dispersion of the possible values of the variable around the expected value.

\begin{definition}
Let $X$ be a discrete random variable with expected value $E(X)$ and probability mass function $f$. The \emph{variance}\index{Variance} of $X$, denoted by $Var(X)$, is defined as:
\[
Var(X) = E[(X - E(X))^2] = \sum_{x} (x - E(X))^2 f(x)
\]
\end{definition}

Variance considers the distribution of the discrete random variable and provides a measure of how much the values differ from the expected value. For instance, if all possible values of a discrete random variable are the same, the variance is zero.

As it was the case of the expected value, a drawback of the variance is that it can be influenced significantly by outliers because it involves squaring the deviations from the mean.

\begin{example}
Consider the company from Example \ref{ex:expected_salary}, with an expected salary of $E(X) = 357$: We calculate the variance as:
\[
Var(X) = (300 - 357)^2 \times 99/100 + (6000 - 357)^2 \times 1/100 = 3218250
\]
\end{example}

Next proposition states that for a linear combination of independent discrete random variables, the variance of the combination is the weighted sum of the variances of the individual variables, where the weights are the squares of the coefficients in the linear combination.

\begin{proposition}
Let \(X_{1}, \ldots, X_{n}\) be $n$ independent discrete random variables with finite expected values, and \(a_{1},\ldots,a_{n}\) and \(b\) be arbitrary constants, then:
\[
Var\left(a_{1}X_{1}+\ldots+a_{n}X_{n}+b\right)=a_{1}^{2}Var\left(X_{1}\right)+\ldots+a_{n}^{2}Var\left(X_{n}\right)
\]
\end{proposition}
\begin{proof}
Let \(Y = a_{1}X_{1} + \ldots + a_{n}X_{n} + b\). The variance of \(Y\) is given by:
\[
Var(Y) = Var(a_{1}X_{1} + \ldots + a_{n}X_{n} + b)
\]
Since variance is unaffected by the addition of a constant, we can ignore \(b\):
\[
Var(Y) = Var(a_{1}X_{1} + \ldots + a_{n}X_{n})
\]
Now, using the linearity of variance for independent discrete random variables, we have:
\[
Var(a_{1}X_{1} + \ldots + a_{n}X_{n}) = Var(a_{1}X_{1}) + \ldots + Var(a_{n}X_{n})
\]
Next, we use the property that for any discrete random variable \(X_i\) and constant \(a_i\), \(Var(a_i X_i) = a_i^2 Var(X_i)\):
\[
Var(a_{1}X_{1}) + \ldots + Var(a_{n}X_{n}) = a_{1}^2 Var(X_{1}) + \ldots + a_{n}^2 Var(X_{n})
\]
Therefore, we have:
\[
Var(a_{1}X_{1} + \ldots + a_{n}X_{n} + b) = a_{1}^{2}Var(X_{1}) + \ldots + a_{n}^{2}Var(X_{n})
\]
\end{proof}

A key relationship in probability theory is the formula that expresses variance in terms of expected values. This relationship not only simplifies calculations but also provides deeper insight into the nature of variance as a measure of risk and variability.

\begin{proposition}
Let $X$ be a discrete random variable with expected value $E(X)$. Then, the variance of $X$ is given by:
\[
Var(X) = E(X^2) - [E(X)]^2
\]
\end{proposition}
\begin{proof}
To prove this proposition, we start from the definition of variance:
\[
Var(X) = E[(X - E(X))^2]
\]
Expanding the square inside the expectation yields:
\[
Var(X) = E[X^2 - 2X \cdot E(X) + (E(X))^2]
\]
Applying the linearity of expectation, this becomes:
\[
Var(X) = E(X^2) - 2E(X)E(X) + E((E(X))^2)
\]
Since \(E(X)\) is a constant, the expectation of a constant is the constant itself, so:
\[
E((E(X))^2) = (E(X))^2
\]
Thus, the expression simplifies to:
\[
Var(X) = E(X^2) - 2(E(X))^2 + (E(X))^2
\]
Simplifying further, we cancel out terms:
\[
Var(X) = E(X^2) - (E(X))^2
\]
\end{proof}

% Standard Deviation

\subsubsection*{Standard Deviation}

The standard deviation is also a statistical measure that quantifies the dispersion or variability in a set of data values. However, unlike variance, which squares the differences from the mean, resulting in units that are the square of the original data units, the standard deviation is the square root of the variance. This adjustment is crucial because it brings the units back to the original units of the data, making the measure more intuitive and directly interpretable.

\begin{definition}
Let $X$ be a discrete random variable with expected value $E(X)$,  variance $Var(X)$ and probability mass function $f$. The \emph{standard deviation}\index{Standar deviation} of $X$, denoted by $\sigma$, is defined as:
\[
\sigma = \sqrt{Var(X)} = \sqrt{\sum_{x} (x - E(X))^2 f(x)}
\]
\end{definition}

The standard deviation provides a numerical summary of how scattered the values of $X$ are around the mean. A smaller standard deviation indicates that the values tend to be closer to the mean (less spread out), whereas a larger standard deviation indicates that the values are more spread out from the mean.

\begin{proposition}
Let $X$ be a discrete random variable with standard deviation $\sigma_X$, and $a$ and $b$ be arbitrary constants, then the standard deviation $\sigma_Y$ of the discrete random variable $Y = aX + b$ is $|a|$ times the standard deviation of $X$, i.e., $\sigma_Y = |a| \sigma_X$.
\end{proposition}
\begin{proof}
The variance of $Y$ can be calculated as:
\[
\sigma_Y^2 = \sum_{x} (a x + b - (a E(X) + b))^2 f(x) = a^2 \sum_{x} (x - E(X))^2 f(x) = a^2 \sigma_X^2
\]
Taking the square root of both sides, we get:
\[
\sigma_Y = \sqrt{a^2 \sigma_X^2} = |a| \sigma_X.
\]
\end{proof}

%
% Measures of Statistical Relationship
%

\subsection{Measures of Statistical Relationship}

Covariance and correlation are measures of the relationship between two variables. Covariance indicates the direction of the linear relationship, showing whether the variables tend to increase or decrease together, but it doesn't standardize the scale of the data. Correlation, on the other hand, standardizes this relationship, giving a value between -1 and 1, where -1 means a perfect negative linear relationship, 1 means a perfect positive linear relationship, and 0 indicates no linear relationship.

% Covariance

\subsubsection*{Covariance}

Covariance is a measure used to determine the degree to which two discrete random variables \(X\) and \(Y\) vary together. It gives an indication of the direction of the linear relationship between the variables.

\begin{definition}
Let $X$ and $Y$ be two discrete random variables with finite expected values $E(X)$ and $E(Y)$ respectively. The \emph{covariance}\index{Covariance} of $X$ and $Y$, denoted by $Cov\left(X, Y\right)$ is defined as
\[
Cov\left(X, Y\right) = E\left[\left(X- E(X) \right) \left(Y - E(Y) \right) \right]
\]
\end{definition}

The sign of the covariance reveals the direction of the relationship between two variables. A positive covariance means that as \(X\) increases, \(Y\) also tends to increase, while a negative covariance indicates that as \(X\) increases, \(Y\) tends to decrease. A covariance of zero suggests no linear relationship between the variables, but itâ€™s important to note that this does not rule out the possibility of a non-linear relationship. Variables may still be related in complex, non-linear ways that covariance cannot capture.

Next proposition presents a fundamental way to calculate covariance, showing how it measures the degree to which two variables vary together in relation to their individual expected values.

\begin{proposition}
Let $X$ and $Y$ be two discrete random variables with finite expected values $E(X)$ and $E(Y)$ respectively. We have that
\[
Cov\left(X,Y\right)=E\left(XY\right)-E\left(X\right)E\left(Y\right)
\]
\end{proposition}
\begin{proof}
The covariance of \(X\) and \(Y\) is defined as the expected value of the product of their deviations from their respective means. Expanding the product within the expectation gives:
\[
\operatorname{Cov}(X, Y) = \mathrm{E}[(X - \mathrm{E}(X))(Y - \mathrm{E}(Y))] = \mathrm{E}[XY - X\mathrm{E}(Y) - Y\mathrm{E}(X) + \mathrm{E}(X)\mathrm{E}(Y)]
\]
Using the linearity of expectation, this expression can be simplified as:
\[
\mathrm{E}[XY - X\mathrm{E}(Y) - Y\mathrm{E}(X) + \mathrm{E}(X)\mathrm{E}(Y)] = \mathrm{E}(XY) - \mathrm{E}(X\mathrm{E}(Y)) - \mathrm{E}(Y\mathrm{E}(X)) + \mathrm{E}(\mathrm{E}(X)\mathrm{E}(Y))
\]
And since \(\mathrm{E}(Y)\) and \(\mathrm{E}(X)\) are constants:
\[
\mathrm{E}(X\mathrm{E}(Y)) = \mathrm{E}(X)\mathrm{E}(Y) \quad \text{and} \quad \mathrm{E}(Y\mathrm{E}(X)) = \mathrm{E}(Y)\mathrm{E}(X)
\]
Hence, the expression further simplifies to:
\[
\mathrm{E}(XY) - \mathrm{E}(X)\mathrm{E}(Y) - \mathrm{E}(Y)\mathrm{E}(X) + \mathrm{E}(X)\mathrm{E}(Y) = \mathrm{E}(XY) - \mathrm{E}(X)\mathrm{E}(Y)
\]
Thus, the covariance of \(X\) and \(Y\) is proved to be:
\[
\operatorname{Cov}(X, Y) = \mathrm{E}(XY) - \mathrm{E}(X)\mathrm{E}(Y)
\]
\end{proof}

Next proposition states an important result that highlights the relationship between independence and covariance for discrete random variables.

\begin{proposition}
If X and Y are independent discrete random variables with $0<Var(X)<\infty$ and $0<Var(Y)<\infty$ then $Cov\left(X,Y\right)=0$
\end{proposition}
\begin{proof}
Let \( X \) and \( Y \) be independent discrete random variables. By the definition of covariance, we have:
\[
\text{Cov}(X, Y) = E\left[ (X - E[X])(Y - E[Y]) \right].
\]
Expanding this, we get:
\[
\text{Cov}(X, Y) = E[XY] - E[X]E[Y].
\]
Since \( X \) and \( Y \) are independent, the expectation of the product of independent random variables is the product of their expectations:
\[
E[XY] = E[X]E[Y].
\]
Substituting this into the expression for covariance, we get:
\[
\text{Cov}(X, Y) = E[X]E[Y] - E[X]E[Y] = 0.
\]
Thus, \( \text{Cov}(X, Y) = 0 \).
\end{proof}

The converse is not true as a general rule. Two dependent discrete random variables can be uncorrelated.

% Correlation

\subsubsection*{Correlation}

The magnitude of the covariance is not standardized, meaning that it can be difficult to interpret the strength of the relationship without context. This is why correlation, which normalizes covariance, is often preferred to measure the strength of the linear relationship between two variables.

\begin{definition}
Let $X$ and $Y$ be two discrete random variables with finite variances $Var(X)$ and $Var(Y)$ respectively. Then the \emph{correlation}\index{Correlation} of $X$ and $Y$, denoted by $Cor\left(X,Y\right)$, is defined as
\[
Cor\left(X,Y\right)=\frac{Cov\left(X,Y\right)}{Var(X) Var(Y)}
\]
\end{definition}

Correlation is a statistical measure that quantifies the strength and direction of a linear relationship between two discrete random variables. Unlike covariance, correlation is dimensionless and standardized, providing a value between -1 and 1.

\begin{proposition}
The correlation coefficient \(\rho_{X,Y}\) lies in the range \([-1, 1]\).
\end{proposition}
\begin{proof}
Correlation normalizes covariance by the product of the standard deviations of \(X\) and \(Y\), thus it is dimensionless:
\[
\rho_{X,Y} = \frac{\operatorname{Cov}(X, Y)}{\sigma_X \sigma_Y}
\]
where \(\sigma_X = \sqrt{\operatorname{Var}(X)}\) and \(\sigma_Y = \sqrt{\operatorname{Var}(Y)}\). By the Cauchy-Schwarz inequality, we have:
\[
(\mathrm{E}[XY])^2 \leq \mathrm{E}[X^2] \mathrm{E}[Y^2]
\]
Substituting for expectations from the definitions of variance and covariance, and rearranging terms, the absolute value of the covariance does not exceed the product of the standard deviations of \(X\) and \(Y\):
\[
|\operatorname{Cov}(X, Y)| \leq \sigma_X \sigma_Y
\]
This implies:
\[
-1 \leq \frac{\operatorname{Cov}(X, Y)}{\sigma_X \sigma_Y} \leq 1
\]
Thus:
\[
-1 \leq \rho_{X,Y} \leq 1
\]
\end{proof}

Correlation is a vital tool in statistics for assessing how strong a linear relationship exists between two variables. A correlation close to \(-1\) or \(+1\) indicates a strong linear relationship, whereas a correlation close to \(0\) suggests a weak linear relationship. This metric is particularly useful in fields such as finance, economics, and the natural sciences where understanding the relationship between variables is crucial for modeling and prediction.

\begin{example}
Let's take a practical example of two random variables: "hours of exercise per week" (\(X\)) and "weight loss in kilograms over a month" (\(Y\)). Suppose we gather the following data from 5 individuals:
\[
\begin{array}{|c|c|c|}
\hline
\text{Person} & \text{Hours of Exercise (X)} & \text{Weight Loss (Y)} \\
\hline
1 & 5 & 1 \\
2 & 10 & 2 \\
3 & 15 & 3 \\
4 & 20 & 3.5 \\
5 & 25 & 4 \\
\hline
\end{array}
\]
Now, let's calculate the correlation between the hours of exercise and the weight loss to see how strongly these two variables are related. Using the formula for the Pearson correlation coefficient:
\[
r = \frac{\text{cov}(X, Y)}{\sigma_X \sigma_Y}
\]
we calculate \( r \approx 0.98 \), indicating a very strong positive correlation. This means that as the hours of exercise increase, weight loss tends to increase as well.
\end{example}

The next proposition establishes that if two discrete random variables are independent and have finite variances, their correlation must be zero.

\begin{proposition}
Let $X$ and $Y$ be two independent discrete random variables with finite variances $Var(X)$ and $Var(Y)$, then we have that $Cor\left(X,Y\right)=0$
\end{proposition}
\begin{proof}
Let \( X \) and \( Y \) be two independent discrete random variables with finite variances. The correlation between \( X \) and \( Y \) is given by:
\[
\text{Cor}(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X)} \sqrt{\text{Var}(Y)}}.
\]
By the definition of covariance, we have:
\[
\text{Cov}(X, Y) = E[XY] - E[X]E[Y].
\]
Since \( X \) and \( Y \) are independent, we know that:
\[
E[XY] = E[X] E[Y].
\]
Substituting this into the expression for covariance, we get:
\[
\text{Cov}(X, Y) = E[X] E[Y] - E[X] E[Y] = 0.
\]
Thus, the covariance is zero. Therefore, the correlation becomes:
\[
\text{Cor}(X, Y) = \frac{0}{\sqrt{\text{Var}(X)} \sqrt{\text{Var}(Y)}} = 0.
\]
\end{proof}

%
% Section: Distribution
%

\section{Common Distributions}
\label{sec:probability_distributions}

In this section, we will define and discuss several important families of distributions that are widely used in applications of probability theory. Specifically, we will introduce the families of discrete distributions: uniform, Bernoulli, binomial, and discrete normal. We will briefly explain how each of these distribution families arises in practical problems and why they may serve as appropriate probability models for certain experiments. For each family, we will present the form of the probability mass function and discuss some of the fundamental properties of the distributions within the family. 

%
% Uniform Distribution
%

\subsection{Uniform Distribution}

The uniform distribution is the simplest probability distribution in probability theory. It models scenarios where all outcomes are equally likely.

\begin{definition}
A discrete random variable $X$ is said to follow an \emph{uniform distribution}\index{Uniform distribution} if the probability of each value is the same. Specifically, if $X$ takes on values in the set $\{x_1, x_2, \ldots, x_n\}$, the probability mass function is given by:
\[
P(X = x_i) = \frac{1}{n} \quad \text{for } i = 1, 2, \ldots, n.
\]
\end{definition}

An example of the discrete uniform distribution would be throwing a fair die. In contrast to other distributions studied in this section, the uniform distribution is non-parametric, meaning it does not depend on a parameter to be fully specified.

The expected value of a discrete uniform random variable $X$ that takes on values in $\{x_1, x_2, \ldots, x_n\}$ is $E(X) = \frac{x_1 + x_2 + \ldots + x_n}{n}$. The variance of a discrete uniform random variable $X$ that takes on values in $\{x_1, x_2, \ldots, x_n\}$ is $Var(X) = \frac{1}{n} \sum_{i=1}^n (x_i - E(X))^2$.

%
% Bernoulli Distribution
%

\subsection{Bernoulli Distribution}

The Bernoulli distribution is one of the simplest and most fundamental probability distributions in probability theory. It models scenarios where there are only two possible outcomes, often termed "success" and "failure."

\begin{definition}
A discrete random variable $X$ is said to follow a \emph{Bernoulli distribution}\index{Bernoulli distribution} with parameter $p$, with $0 \leq p \leq 1$, if it takes the value 1 (representing "success") with probability $p$ and the value 0 (representing "failure") with probability $1-p$. The probability mass function is given by:
\[
P(X = x) = 
\begin{cases} 
p & \text{if } x = 1, \\
1 - p & \text{if } x = 0.
\end{cases}
\]
\end{definition}

The Bernoulli distribution arises naturally in many applied problems, especially those involving binary outcomes, in which the parameter $p$ represents the probability of success. For example, in medical studies, the outcome of a treatment can be a success (cure) or a failure (no cure), which can be modeled using a Bernoulli distribution.

The expected value of a Bernoulli random variable $X$ with parameter $p$ is $E(X) = p$. This makes intuitive sense, as the expected value represents the average outcome of many trials, which would tend toward the probability of success. The variance of a Bernoulli random variable is given by $\text{Var}(X) = p(1 - p)$. The variance measures the spread or dispersion of the distribution around the expected value. The product $p(1 - p)$ achieves its maximum value when $p = 0.5$, indicating maximum uncertainty when the probabilities of success and failure are equal.

\begin{definition}
Let $X_{1}, X_{2}, \ldots$ be a sequence of discrete random variables that follows a Bernoulli distribution with parameter $p$, then it is said that $X_{1}, X_{2}, \ldots$ are \emph{Bernoulli trials}\index{Bernoulli trial} with parameter $p$. An infinite sequence of independent Bernoulli trials is called a \emph{Bernoulli process}\index{Bernoulli process}.
\end{definition}

The Bernoulli process serves as a foundation for more complex stochastic processes and is used in various applications, such as modeling binary events over time.

%
% Binomial Distribution
%

\subsection{Binomial Distribution}

The binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success. This distribution is widely used in probability theory for various applications involving binary outcomes.

\begin{definition}
\label{def:binomial_distribution}
A discrete random variable $X$ is said to follow a \emph{binomial distribution}\index{Binomial distribution} with parameters $n$ and $p$ if it represents the number of successes in $n$ independent Bernoulli trials, each with success probability $p$. The probability mass function is given by:
\[
P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k} \quad \text{for } k = 0, 1, 2, \ldots, n.
\]
\end{definition}

The parameter $n$ is the number of trials, and $p$ is the probability of success on each trial. The term $\binom{n}{k}$ is the binomial coefficient, representing the number of ways to choose $k$ successes out of $n$ trials.

The binomial distribution arises naturally in many applied problems where the outcomes are binary (success/failure).

\begin{example}
Suppose a company produces light bulbs, and historically, they know that 95\% of the bulbs they produce are functional (success) and 5\% are defective (failure). The company wants to check the quality of a batch of 20 light bulbs by randomly selecting and testing them. This problem can be modeled using the binomial distribution, where the number of trials is $n = 20$ and the probability of success is $p = 0.95$. For example, the probability of exactly 18 out of 20 light bulbs being functional is:
\[
P(X = 18) = \binom{20}{18} (0.95)^{18} (0.05)^2.
\]
\end{example}

The expected value of a binomial random variable $X$ with parameters $n$ and $p$ is $E(X) = np$. This represents the average number of successes in $n$ trials. The variance of a binomial random variable is $Var(X) = np(1 - p)$. The variance measures the spread or dispersion of the distribution around the expected value.

%
% Discrete Normal Distribution
%

\subsection{Discrete Normal Distribution}

The normal distribution is a continuous probability distribution that frequently appears in natural and social sciences, primarily due to the Central Limit Theorem (see Theorem \ref{th:central_limit_theorem_pdf}). However, many real-world situations involve discrete outcomes, such as counts of events or integer-valued random variables. In these cases, a discrete analog to the normal distribution is required. This need does not represent a limitation, as continuous distributions often emerge as limiting abstractions of inherently discrete processes.

In this section, we introduce the discrete normal distribution, which serves as a discrete counterpart to the normal distribution for integer-valued random variables. We derive its key properties through formal definitions and propositions.

To define the discrete normal distribution, we first introduce the concept of a standardized discrete random variable.

\begin{definition}
Let $X$ be a discrete random variable with expected value $E(X)$ and variance $Var(X)$. The \emph{standardized discrete random variable}\index{Standarized discrete random variable} of $X$, denoted by $Z$, is defined as:
\[
Z = \frac{X - E(X)}{\sqrt{Var(X)}}
\]
\end{definition}

This normalization centers the distribution around 0 (the standardized expected value) and scales it by the standard deviation. 

\begin{example}
If the discrete random variable $X$ follows a binomial distribution with parameters $n$ and $p$, its standardized version $Z$ is given by:
\[
Z = \frac{X - np}{\sqrt{np(1-p)}}
\]
The probability mass function of $Z$ can be written as:
\[
P\left(Z = k\right) = P\left( Z = \frac{k - np}{\sqrt{np(1-p)}} \right) = \binom{n}{k} p^k (1-p)^{n-k}, \quad k = 0, 1, \dots, n.
\]
\end{example}

The normal distribution arises as the limit of the standardized binomial distribution as \( n \to \infty \). However, the normal distribution is continuous, while the binomial distribution is discrete. To define a discrete analog to the normal distribution, we consider a discrete distribution whose probabilities are proportional to the normal probability density function evaluated at integer points.

\begin{definition}
A discrete random variable \( X \) is said to follow a \emph{discrete normal distribution}\index{Discrete normal distribution} with parameters \( \mu \) and \( \sigma^2 \) if it has the following probability mass function:
\[
P(X = k) = \frac{1}{Z} \exp\left( -\frac{(k - \mu)^2}{2\sigma^2} \right), \quad k \in \mathbb{Z},
\]
where \( Z \) is the normalization constant given by:
\[
Z = \sum_{k = -\infty}^{\infty} \exp\left( -\frac{(k - \mu)^2}{2\sigma^2} \right).
\]
\end{definition}

This distribution can be regarded as a fundamental model for systems where outcomes are inherently discrete, such as the distribution of counts or events.

In practice, this distribution retains the discrete nature of integer-valued random variables while adopting the bell-shaped curve characteristic of the normal distribution. The discrete normal distribution thus encapsulates the essence of the normal distribution while remaining defined on the set of integers.

\begin{proposition}
The normalization constant \( Z \) in the discrete normal distribution satisfies:
\[
Z \approx \sqrt{2\pi \sigma^2},
\]
for large \( \sigma \), and thus the probability mass function can be approximated by:
\[
P(X = k) \approx \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left( -\frac{(k - \mu)^2}{2\sigma^2} \right).
\]
\end{proposition}
\begin{proof}
For large \( \sigma \), the sum in \( Z \) can be approximated by an integral:
\[
Z = \sum_{k = -\infty}^{\infty} \exp\left( -\frac{(k - \mu)^2}{2\sigma^2} \right) \approx \int_{-\infty}^{\infty} \exp\left( -\frac{(x - \mu)^2}{2\sigma^2} \right) \, dx = \sqrt{2\pi \sigma^2}.
\]
Therefore, for large \( \sigma \), the discrete normal PMF approaches the continuous normal PDF evaluated at integer points.
\end{proof}

\begin{example}
Suppose we want to model the number of times a fair six-sided die shows a "6" in 100 rolls. The number of successes \( X \) follows a binomial distribution with parameters \( n = 100 \) and \( p = \frac{1}{6} \). The expected value and variance are:
\[
E(X) = np = \frac{100}{6} \approx 16.67, \quad \operatorname{Var}(X) = np(1 - p) = 100 \times \frac{1}{6} \times \frac{5}{6} \approx 13.89.
\]
For large \( n \), the binomial distribution can be approximated by the discrete normal distribution with \( \mu = 16.67 \) and \( \sigma^2 = 13.89 \). The PMF is:
\[
P(X = k) \approx \frac{1}{Z} \exp\left( -\frac{(k - 16.67)^2}{2 \times 13.89} \right),
\]
where \( Z \) is the normalization constant.

We can use this approximation to compute probabilities, such as \( P(X = 20) \) or \( P(X = 15) \). For instance:
\[
P(X = 20) \approx \frac{1}{Z} \exp\left( -\frac{(20 - 16.67)^2}{2 \times 13.89} \right) \approx \frac{1}{Z} \exp\left( -\frac{11.11}{27.78} \right).
\]
\end{example}

The following proposition highlights an important property of the discrete normal distribution, specifically its symmetry around the mean.

\begin{proposition}
The discrete normal distribution is symmetric about its mean \( \mu \). Formally, for any integer \( k \):
\[
P(X = \mu + k) = P(X = \mu - k).
\]
\end{proposition}
\begin{proof}
The symmetry follows directly from the functional form of the PMF:
\[
P(X = \mu + k) = \frac{1}{Z} \exp\left( -\frac{(\mu + k - \mu)^2}{2\sigma^2} \right) = \frac{1}{Z} \exp\left( -\frac{k^2}{2\sigma^2} \right).
\]
Similarly,
\[
P(X = \mu - k) = \frac{1}{Z} \exp\left( -\frac{(\mu - k - \mu)^2}{2\sigma^2} \right) = \frac{1}{Z} \exp\left( -\frac{(-k)^2}{2\sigma^2} \right) = \frac{1}{Z} \exp\left( -\frac{k^2}{2\sigma^2} \right).
\]
Since both expressions are equal, the distribution is symmetric about \( \mu \).
\end{proof}

The discrete normal distribution serves as a bridge between the discrete and continuous realms of probability distributions. It retains many key properties of the continuous normal distribution, such as symmetry, centrality, and the bell-shaped curve, while accommodating the discrete nature of certain discrete random variables. By formally defining the distribution and studying its properties, we gain a powerful tool for modeling real-world phenomena where outcomes are discrete but exhibit normal-like behavior.

This distribution is especially useful in areas such as manufacturing, education, and epidemiology, where event counts or integer-valued outcomes naturally arise, and where the underlying variability can be captured using a normal-like distribution. The discrete normal distribution allows us to model these scenarios more precisely, ensuring that our probabilistic models align with the real-world structure of the data.

%
% Section: Large Random Samples
%

\section{Large Random Samples}
\label{sec:probability_random_samples}

A random sample is a collection of independent and identically distributed random variables. Random samples are fundamental in probability theory, providing a basis for making inferences about an unknown probability distribution. Two key results that arise from random samples are the law of large numbers, which ensures that the sample mean converges to the expected value of the distribution as the sample size increases, and the central limit theorem, which states that the distribution of the sample mean approaches a normal distribution as the sample size becomes large, regardless of the original population's distribution.

{\color{red} [...] statistical methods do not directly describe reality but provide a conceptual framework (i.e., a model) for understanding how data is generated and how we can make inferences about the population from which the data comes. [...] Random sampling assumes that every member of the population has an equal probability of being selected, and selections are independent of one another. This means the sample is a simplified, idealized representation of how real-world data might be generated. [...] The model assumes perfect randomness, which rarely occurs in practice. True random sampling is often an unattainable ideal, but it provides a baseline against which deviations can be measured. [...] Random sampling allows for the development of probability distributions that describe the expected behavior of sample statistics (e.g., sample mean, variance). This makes it possible to quantify uncertainty and draw conclusions about the population. [...] The random sampling model is a representation of how data might be collected, but it is not the same as reality [...] models are tools that work well in specific contexts, but their assumptions often do not hold universally. The practical use of random sampling depends on how well the model fits the real-world context. }

% Random Samples

\subsection{Random Samples}

The concept of a random sample is pivotal in the field of statistical learning, as assuming a set of discrete random variables constitutes a random sample greatly simplifies the mathematical underpinnings of inferential methods. Nevertheless, the criteria for a collection of variables to be considered a random sample are not always met in real-world scenarios.

\begin{definition}
Let \( f \) be a probability mass function, and let \( X_1, X_2, \ldots, X_n \) be a collection of discrete random variables. The variables \( X_1, X_2, \ldots, X_n \) form a \emph{random sample}\index{Random sample} from the distribution \( f \) if each discrete random variable \( X_i \) follows the probability mass function \( f \), and the variables \( X_1, X_2, \ldots, X_n \) are independent of each other. These discrete random variables are also referred to as \emph{independent and identically distributed (i.i.d.)}\index{Independent and identically distributed}. The number \( n \) of discrete random variables is referred to as the \emph{sample size}\index{Sample size}.
\end{definition}

The joint probability mass function \( g \) of a random sample \( X_1, X_2, \ldots, X_n \) is given by:
\[
g\left( x_1, x_2, \ldots, x_n \right) = f\left( x_1 \right) f\left( x_2 \right) \cdots f\left( x_n \right)
\]

\begin{example}
A factory produces light bulbs, and the quality control department is interested in determining the proportion of defective bulbs. The number of defective bulbs in a batch of 100 bulbs is modeled using a binomial distribution, where each bulb has a probability \( p \) of being defective. The department randomly selects 5 batches, each containing 100 bulbs. Let the discrete random variables \( X_1, X_2, X_3, X_4, X_5 \) represent the number of defective bulbs in each of these 5 batches. Each \( X_i \) follows the binomial distribution \( \text{Binomial}(100, p) \). The discrete random variables \( X_1, X_2, X_3, X_4, X_5 \) are independent and identically distributed, and thus, they form a random sample.
\end{example}

In the area of statistical inference, it is also highly convenient to compute the sample mean, as the average of \( n \) discrete random variables.

\begin{definition}
Let \( X_1, X_2, \ldots, X_n \) be \( n \) discrete random variables. The \emph{sample mean}\index{Sample mean} of \( X_1, X_2, \ldots, X_n \), denoted by \( \bar{X}_n \), is defined as:
\[
\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i
\]
\end{definition}

Do not confuse \( \frac{1}{n} \left( X_1 + X_2 + \ldots + X_n \right) \), which is a discrete random variable\footnote{Recall that with \( X_1 + \ldots + X_n \) we are defining a new discrete random variable over the Cartesian product of the original sample spaces, not adding \( n \) probability distributions.}, with the expected value \( E\left( X_1 + X_2 + \ldots + X_n \right) \), which is a real number.

As we will show in the next proposition, the concept of sample mean is particularly relevant for the case of \( X_1, X_2, \ldots, X_n \) being a random sample.

\begin{proposition}\label{prop:sample_mean}
Let \( X_1, \ldots, X_n \) be a random sample with finite mean \( E\left( X_i \right) = \mu \) and finite variance \( \operatorname{Var}\left( X_i \right) = \sigma^2 \), and let \( \bar{X}_n = \frac{1}{n} \left( X_1 + \ldots + X_n \right) \) be the sample mean. Then we have that \( E\left( \bar{X}_n \right) = \mu \) and \( \operatorname{Var}\left( \bar{X}_n \right) = \sigma^2 / n \).
\end{proposition}
\begin{proof}
Using the linearity of expectation, we have:
\[
E\left( \bar{X}_n \right) = E\left( \frac{1}{n} \sum_{i=1}^{n} X_i \right) = \frac{1}{n} \sum_{i=1}^{n} E\left( X_i \right) = \frac{1}{n} \cdot n \cdot \mu = \mu
\]
Using the properties of variance and the fact that the \( X_i \) are independent, we have:
\[
\operatorname{Var}\left( \bar{X}_n \right) = \operatorname{Var}\left( \frac{1}{n} \sum_{i=1}^{n} X_i \right)
\]
Since variance is homogeneous of degree 2, we can factor out the constant \( \frac{1}{n} \):
\[
\operatorname{Var}\left( \bar{X}_n \right) = \left( \frac{1}{n} \right)^2 \operatorname{Var}\left( \sum_{i=1}^{n} X_i \right)
\]
For independent discrete random variables, the variance of the sum is the sum of the variances:
\[
\operatorname{Var}\left( \sum_{i=1}^{n} X_i \right) = \sum_{i=1}^{n} \operatorname{Var}\left( X_i \right) = n \cdot \sigma^2
\]
Thus:
\[
\operatorname{Var}\left( \bar{X}_n \right) = \left( \frac{1}{n} \right)^2 \cdot n \cdot \sigma^2 = \frac{\sigma^2}{n}
\]
\end{proof}

The probability distribution of $\bar{X}_n$ will be more concentrated arount the mean value $\mu$ that was the original distribution. 

In the area of statistical inference, it is also highly convenient to compute the sample variance as a measure of the dispersion of \( n \) discrete random variables. In particular, we will compute the sample variance of random samples.

\begin{definition}
Let \( X_1, X_2, \ldots, X_n \) be a set of discrete random variables with sample mean \( \bar{X}_n \). The \emph{sample variance}\index{Sample variance} of \( X_1, X_2, \ldots, X_n \), denoted by \( S^2 \), is defined as:
\[
S^2 = \frac{1}{n - 1} \sum_{i=1}^n \left( X_i - \bar{X}_n \right)^2
\]
\end{definition}

Do not confuse the sample variance, which is a statistic based on observed values, with the theoretical variance of the distribution. We will study some important properties of the variance of large random samples in Section \ref{sec:probability_random_samples}.

\begin{example}
Continuing from the previous example, suppose we have the observed values of \( X_1, X_2, \ldots, X_5 \) as follows:
\[
X_1 = 6, \quad X_2 = 4, \quad X_3 = 5, \quad X_4 = 7, \quad X_5 = 3
\]
The sample mean is:
\[
\bar{X}_5 = \frac{1}{5} \left( 6 + 4 + 5 + 7 + 3 \right) = \frac{25}{5} = 5
\]
The sample variance is:
\[
S^2 = \frac{1}{5 - 1} \sum_{i=1}^{5} \left( X_i - 5 \right)^2 = \frac{1}{4} \left[ (6 - 5)^2 + (4 - 5)^2 + (5 - 5)^2 + (7 - 5)^2 + (3 - 5)^2 \right]
\]
\[
S^2 = \frac{1}{4} \left( 1 + 1 + 0 + 4 + 4 \right) = \frac{10}{4} = 2.5
\]
Thus, the sample variance is 2.5.
\end{example}

The sample standard deviation is used to estimate the standard deviation of a population based on a sample taken from it. Unlike the population standard deviation, which uses the true mean of the entire population, the sample standard deviation uses the mean of the sample as an estimate of the true mean.

\begin{definition}
Let \( X_1, X_2, \ldots, X_n \) be \( n \) discrete random variables with sample mean \( \bar{X}_n \) and sample variance \( S^2 \). The \emph{sample standard deviation}\index{Sample standard deviation} of \( X_1, X_2, \ldots, X_n \), denoted by \( S \), is defined as:
\[
S = \sqrt{S^2} = \sqrt{ \frac{1}{n - 1} \sum_{i=1}^{n} \left( X_i - \bar{X}_n \right)^2 }
\]
\end{definition}

\begin{example}
Using the sample variance calculated in the previous example, the sample standard deviation is:
\[
S = \sqrt{S^2} = \sqrt{2.5} \approx 1.5811
\]
\end{example}

The sample covariance is used to estimate the covariance between two populations based on a sample taken from them.

\begin{definition}
Let \( X_1, X_2, \ldots, X_n \) and \( Y_1, Y_2, \ldots, Y_n \) be two sets of discrete random variables with sample means \( \bar{X} \) and \( \bar{Y} \) respectively. The \emph{sample covariance}\index{Sample covariance} between \( X \) and \( Y \) is defined as:
\[
s_{XY} = \frac{1}{n - 1} \sum_{i=1}^n \left( X_i - \bar{X} \right) \left( Y_i - \bar{Y} \right)
\]
\end{definition}

\begin{example}
Suppose we have paired observations of two discrete random variables \( X \) and \( Y \):
\[
\begin{array}{c|ccccc}
\hline
i & 1 & 2 & 3 & 4 & 5 \\
\hline
X_i & 2 & 4 & 6 & 8 & 10 \\
Y_i & 1 & 3 & 5 & 7 & 9 \\
\hline
\end{array}
\]
Compute the sample means:
\[
\bar{X} = \frac{1}{5} \sum_{i=1}^{5} X_i = \frac{30}{5} = 6, \quad \bar{Y} = \frac{1}{5} \sum_{i=1}^{5} Y_i = \frac{25}{5} = 5
\]
Compute the sample covariance:
\[
s_{XY} = \frac{1}{5 - 1} \sum_{i=1}^{5} \left( X_i - 6 \right) \left( Y_i - 5 \right)
\]
\[
s_{XY} = \frac{1}{4} \left[ (2 - 6)(1 - 5) + (4 - 6)(3 - 5) + (6 - 6)(5 - 5) + (8 - 6)(7 - 5) + (10 - 6)(9 - 5) \right]
\]
\[
s_{XY} = \frac{1}{4} \left[ ( -4 )( -4 ) + ( -2 )( -2 ) + ( 0 )( 0 ) + ( 2 )( 2 ) + ( 4 )( 4 ) \right] = \frac{1}{4} \left[ 16 + 4 + 0 + 4 + 16 \right] = \frac{40}{4} = 10
\]
Thus, the sample covariance between \( X \) and \( Y \) is 10.
\end{example}

% Law of Large Numbers

\subsection{Law of Large Numbers}

The law of large numbers is a theorem that states that the sample mean of a large random sample, i.e. a large number of independent and identically distributed discrete random variables, should be close to the expected value of the discrete random variables, and that the more variables we add, the closer will be to that value. Before to prove this important theorem we have to prove two other related propostions: Markov's inequality and Chebyshev's inequality.

Markov's inequality provides a simple way to measure how unlikely it is for a discrete random variable to take on large values, given its expected value. It leverages the fact that if the average value of discrete random variable $X$ is small, then it is improbable for $X$ to take on a large value (recall Example \ref{ex:expected_salary}). Markov's inequality is particularly useful because it makes no assumptions about the distribution of the discrete random variable, apart from it being non-negative and having a finite expected value.

\begin{proposition}[Markov's Inequality]\index{Markov's Inequality}
Let $X$ be a nonnegative discrete random variable, i.e. $P\left( X \geq 0 \right) = 1$, with expected value $E(X)$. Then for every real number $t>0$ we have that 
\[
P \left( X \geq t \right) \leq \frac{E(X)}{t}
\]
\end{proposition}
\begin{proof}
Let $f$ be the probability mass function of $X$. Since $X$ is non-negative we have that $E(X) = \sum_{x>0} x f \left( x \right)$. Then
\[
E(X) = \sum_{x>0} x f \left( x \right) = \sum_{x=0}^{t} x f \left( x \right) + \sum_{x=t}^{\infty} x f \left( x \right) \geq
\sum_{x=t}^{\infty} x f \left( x \right) \geq \sum_{x=t}^{\infty} t f \left( x \right) = t \sum_{x=t}^{\infty} f \left( x \right) =
t P \left( X \geq t \right)
\]
that is, $E(X) \geq t P \left( X \geq t \right)$.
\end{proof}

For example, if the expected value of a discrete random variable $X$ is $10$, and we are interested in the probability that $X$ is at least $50$, Markov's inequality tells us that $P(X \geq 50) \leq 10/50 = 0.2$. While Markov's inequality is very general and easy to use, it is often not tight (i.e., the bound is not very close to the true probability). This is because it does not use any other information about the distribution of $X$ other than its expectation.

Chebyshev's inequality provides a bound on the probability that a discrete random variable deviates from its expected value by more than a specified number of standard deviations. Unlike Markov's inequality, which only requires non-negativity, Chebyshev's inequality does not require the discrete random variable to be nonnegative, but it requires knowledge of both the mean and variance of the discrete random variable. The inequality is applicable to any discrete random variable with a finite mean and variance, regardless of the specific distribution.

\begin{corollary}[Chebyshev's inequality]\index{Chebyshev's inequality}
 Let $X$ be a discrete random variable with expected value $E(X)$ and variance $Var(X)$. Then for every real number $t > 0$ we have that
\[
P \left( \left| X - E(X) \right| \geq t \right) \leq \frac{var(X)}{t^2}
\]
\end{corollary}
\begin{proof}
Applying Markov's inequality we have that
\[
P \left( \left| X - E(X) \right| \geq t \right) = P \left( \left( X - E(X) \right)^2 \geq t^2 \right) \leq \frac{E \left( \left( X - E(X) \right)^2 \right)}{t^2} = \frac{var(X)}{t^2}
\]
\end{proof}

Chebyshev's inequality provides insight into the concentration of a discrete random variable around its mean. It quantifies the idea that most of the probability mass of a discrete random variable lies within a certain range of its mean, with fewer values appearing further away. This concept is particularly useful in understanding the spread of a distribution and the likelihood of large deviations.

The last element we need to formally prove the law of large numbers is to introduce the concept of convergence in probability for discrete random variables. Intuitively, a sequence of discrete random variables converges in probability to a value $b$ if $X_n$ lies around $b$.

\begin{definition}
Let $X_{1}, X_{2}, \ldots$ be a sequence of discrete random variables. It is said that the sequence $X_{1}, X_{2}, \ldots$ \emph{converges in probability}\index{Convergence in probability} to $b$ , denoted by $X_{n} \overset{p}{\rightarrow}b$, if for every positive real number $\varepsilon>0$ we have that
\[
\lim_{n \rightarrow \infty} P \left( \left| X_{n} - b \right| < \varepsilon \right) = 1
\]
\end{definition}

Convergence in probability essentially means that as the sequence progresses (i.e., as $n$ increases), the discrete random variables $X_n$ become arbitrarily close to the number $b$ with high probability. The distance between $X_n$ and $b$ can be made as small as desired, and the likelihood of $X_n$ being far from $b$ diminishes as $n$ increases.

Given the above definitions and partial results, we can now formally introduce and prove the law of large numbers.

\begin{theorem}[Law of Large Numbers]\index{Law of Large Numbers}
\label{th:law_large_numbers}
Let $X_1, \ldots, X_n$ be a random sample with finite expected value and finite variance, and let $\overline {X}_n = \frac {1}{n} \left( X_1 + \ldots + X_n \right)$ be the sample mean. Then we have that
\[
\overline {X}_n \overset{p}{\rightarrow} \mu
\]
\end{theorem}
\begin{proof}
Since the variables $X_1, \ldots, X_n$ are independent and identically distributed, we have that the variance of the sample mean is $Var \left( \overline {X}_n \right) = \frac{\sigma^2}{n}$ and that the mean is $E \left( \overline {X}_n \right) = \mu$ (see Proposition \ref{prop:sample_mean}). Applying the Chebyshev's inequality to the discrete random variable $\overline {X}_n$ we have that
\[
P \left( \left| \overline {X}_n- \mu \right| \geq \varepsilon \right) \leq \frac{\sigma ^2}{n \varepsilon^2}.
\]
From there we can obtain
\[
P \left( \left| \overline {X}_n - \mu \right| < \varepsilon \right) = 1 - P \left( \left| \overline {X}_n - \mu \right| \geq \varepsilon \right) \geq 1 - \frac{\sigma ^2}{n \varepsilon^2}.
\]
As n approaches infinity, the above expression approaches 1. Aplying the definition of convergence in probability, we have that
\[
\overline{X}_n \overset{p}{\rightarrow} \mu
\]
\end{proof}

It is very important to note that the law of large numbers only works in case that the discrete random variables are independent and identicaly distributed. Also, that the law is true only in the limit. If we have a finite number of discrete random variables, the sample mean will be close to the distribution mean, but not necesarily equal.

Let's see an example of the use of the law of large numbers in practice.

\begin{example}
Consider the experiment of rolling a fair six-sided die $\Omega = \{1, 2, 3, 4, 5, 6\}$, being each side equally probable with $P(\omega)=1/6$ for $\omega \in \Omega$. Let $X:\Omega \rightarrow \mathbb{R}$ be a discrete random variable that maps each side of the die with the number depicted on it. The probability mass function $f: range(X) \rightarrow [0, 1]$ of $X$ assigns $1/6$ to each value of $range(X)$. The expected value of $X$ is $E(X) = 3.5$.

Let's consider a random sample of size two, that is, trowing two dice (one after the other, so they are distinguishible), and call the corresponding discrete random variables $X_1$ and $X_2$. The sample mean would be the discrete random variable $\frac{X_1 + X_2}{2}: \Omega \times \Omega \rightarrow \mathbb{R}$ that maps each pair $(\omega_1, \omega_2) \in \Omega \times \Omega$ to the real value $X_1(\omega_1) + X_2(\omega_2) / 2$. 

The law of large numbers proposes to study the discrete random variable $\frac{X_1 + X_2}{2} - E(X): \Omega \times \Omega \rightarrow \mathbb{R}$, whose probability mass function is depicted in Figure \ref{fig:law_large_numbers_1}. And lets, for example, compute the probability $P(  |\frac{X_1 + X_2}{2} - E(X)| < 1 )$ which is $0.444$.

\begin{figure}[t]
\centering
\begin{tikzpicture}
    \begin{axis}[
        ybar,
        bar width=12pt,
        ymin=0,
        xlabel={Discrete Random Variable},
        ylabel={Probability},
        xtick={-2.5,-2,...,2.5},
        xticklabel style={font=\footnotesize},
        yticklabel style={font=\footnotesize, /pgf/number format/fixed},
        xlabel style={font=\footnotesize},
        ylabel style={font=\footnotesize},
        xticklabel style={/pgf/number format/fixed},
        enlargelimits=0.15,
        ymajorgrids=true,
        grid style=dashed
    ]
    \addplot[fill={rgb,255:red,243;green,102;blue,25}] coordinates {
        (-2.5, 0.0278)
        (-2.0, 0.0556)
        (-1.5, 0.0833)
        (-1.0, 0.1111)
        (-0.5, 0.1389)
        (0.0, 0.1667)
        (0.5, 0.1389)
        (1.0, 0.1111)
        (1.5, 0.0833)
        (2.0, 0.0556)
        (2.5, 0.0278)
    };
    \end{axis}
\end{tikzpicture}
\caption{\label{fig:law_large_numbers_1}Probability mass function of the discrete random variable $\frac{X_1 + X_2 }{2} - \mathbb{E}(X)$.}
\end{figure}

Now let's consider a random sample of size ten. In Figure \ref{fig:law_large_numbers_2} is depicted the probability mass function of the discrete random variable $\frac{X_1 + X_2 + \ldots +  X_{10}}{10} - E(X): \Omega \times \ldots \times \Omega \rightarrow \mathbb{R}$. The probability $P(  |\frac{X_1 + X_2 + \ldots +  X_{10}}{10} - E(X)| < 1 )$ is $0.973$.

\begin{figure}[t]
\centering
\begin{tikzpicture}
    \begin{axis}[
        ybar,
        bar width=10pt,
        ymin=0,
        xlabel={Discrete Random Variable},
        ylabel={Probability},
        xtick={-2.5,-2,-1.5,-1,-0.5,0,0.5,1,1.5,2,2.5},
        xticklabel style={font=\footnotesize},
        yticklabel style={font=\footnotesize},
        xlabel style={font=\footnotesize},
        ylabel style={font=\footnotesize},
        xticklabel style={/pgf/number format/fixed},
        enlargelimits=0.15,
        ymajorgrids=true,
        grid style=dashed
    ]
    \addplot[fill={rgb,255:red,243;green,102;blue,25}] coordinates {
        (-2.5, 0.0010)
        (-2.0, 0.0100)
        (-1.5, 0.0400)
        (-1.0, 0.1200)
        (-0.5, 0.2100)
        (0.0, 0.2380)
        (0.5, 0.2100)
        (1.0, 0.1200)
        (1.5, 0.0400)
        (2.0, 0.0100)
        (2.5, 0.0010)
    };
    \end{axis}
\end{tikzpicture}
\caption{\label{fig:law_large_numbers_2}Probability mass function of the discrete random variable $\frac{X_1 + X_2 + \ldots +  X_{10}}{10} - E(X)$.}
\end{figure}

\end{example}

It is important to clarify that the law of large numbers refers to the sample mean, that is $\frac {1}{n} \sum_{i=1}^{n} X_{i}$, and it is not necesarily true for other formulas, like for example, the deviation from the theoretical expected value $\sum_{i=1}^{n} X_{i} - n \times E(X)$ which not only it does not converge, but inscreases in absolute value as $n$ increases (see Example \ref{ex:gambler's_fallacy}).

\begin{example}
\label{ex:gambler's_fallacy}
If we toss a fair coin, the probability that the outcome will be head is equal to $1/2$. According to the law of large numbers, the proportion of heads in a large number of coin tosses will be close to $1/2$. However, the difference between the number of heads and tails will not be close to zero. In fact, the larger the number of coin tosses, the larger will be this difference. This is a highly conterintuitive fact, since most of the people think that the more we toss the coin, the closer will be the number of heads to the number of tails, which is not true.
\end{example}

% Central Limit Theorem

\subsection{Central Limit Theorem}

Let $X_1, \ldots, X_n$ be a sample of $n$ independent and identically distributed discrete random variables with mean $\mu$ and variance $\sigma^2$. As we saw in the previous section, the law of large numbers states that the sample average $\overline {X}_n$ converges in probability to $\mu$ as $n$ increases. The central limit theorem states that the distribution of the difference between the sample average $\overline {X}_n$ and the population mean $\mu$, when multiplied by the factor $\sqrt {n}$ approximates to the normal distribution with mean $0$ and variance $\sigma^2 / n$. The theorem is true regardless of the shape of the original random vairables.

\begin{theorem}[Central Limit Theorem]
\label{th:central_limit_theorem_pdf}\index{Central limit theorem}
Let $X_{1}, \ldots, X_{n}$ be a random sample of size $n$ from a distribution with mean $\mu$ and finite variance $\sigma^{2}$. Define the standardized sum as
\[
Z_n = \frac{\overline{X}_{n}-\mu}{\sigma/\sqrt{n}},
\]
where $\overline{X}_{n} = \frac{1}{n} \sum_{i=1}^{n} X_i$ is the sample mean. Then, as $n \rightarrow \infty$, the probability density function of $Z_n$ converges to the probability density function of the standard normal distribution $\phi(x)$, that is,
\[
\lim_{n \rightarrow \infty} f_{Z_n}(x) = \phi(x),
\]
where $f_{Z_n}(x)$ is the probability density function of $Z_n$, and $\phi(x)$ is the probability density function of the standard normal distribution, given by
\[
\phi(x) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x^2}{2}\right).
\]
\end{theorem}
\begin{proof}
Let $X_{1}, \ldots, X_{n}$ be independent and identically distributed (i.i.d.) discrete random variables with mean $\mu$ and finite variance $\sigma^2$. Define the sample mean as
\[
\overline{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i.
\]
We are interested in the distribution of the standardized variable
\[
Z_n = \frac{\overline{X}_n - \mu}{\sigma / \sqrt{n}}.
\]

First, consider the sum of the $X_i$'s, which we write as
\[
S_n = \sum_{i=1}^{n} X_i.
\]
The sample mean can then be written as
\[
\overline{X}_n = \frac{S_n}{n}.
\]
Thus, the standardized variable $Z_n$ becomes
\[
Z_n = \frac{S_n - n\mu}{\sigma\sqrt{n}}.
\]

To analyze the behavior of $Z_n$ as $n \to \infty$, we consider the sum $S_n = \sum_{i=1}^{n} X_i$. According to the Law of Large Numbers, $S_n/n$ converges to $\mu$, and hence $Z_n$ should converge to a normal distribution due to the nature of sums of independent discrete random variables.

For large $n$, $Z_n$ can be approximated by considering the Taylor expansion of the exponential function. Since the $X_i$'s are i.i.d., the distribution of $S_n$ can be approximated by a normal distribution with mean $n\mu$ and variance $n\sigma^2$. The key idea is that as $n$ increases, the distribution of $Z_n$ approaches a normal distribution because the sum of i.i.d. discrete random variables tends to be normally distributed by the Central Limit Theorem.

Let us consider the moment generating function (MGF) of $Z_n$. The MGF of $Z_n$ is given by:
\[
M_{Z_n}(t) = \mathbb{E}\left[\exp\left(t Z_n\right)\right] = \mathbb{E}\left[\exp\left(t \frac{S_n - n\mu}{\sigma\sqrt{n}}\right)\right].
\]
For large $n$, by the Central Limit Theorem, the MGF of $Z_n$ approaches that of a standard normal variable $N(0,1)$:
\[
M_{Z_n}(t) \approx \exp\left(\frac{t^2}{2}\right),
\]
which implies that $Z_n$ converges in distribution to $N(0,1)$ as $n \to \infty$.

Since $Z_n$ converges in distribution to $N(0,1)$, the probability density function of $Z_n$, denoted by $f_{Z_n}(x)$, must converge to the probability density function of a standard normal distribution $\phi(x)$, given by:
\[
\phi(x) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x^2}{2}\right).
\]
Thus, we have
\[
\lim_{n \rightarrow \infty} f_{Z_n}(x) = \phi(x).
\]

Therefore, as $n \to \infty$, the distribution of the standardized sum $Z_n$ approaches the standard normal distribution, completing the proof.
\end{proof}

The Central Limit Theorem is a fundamental concept in probability theory used in statistical analysis and inference. It allows us to compute the probability that the sample average is close to the distribution mean. It is important to recall the conditons for the central limit theorem to be true: the samples must be independent and identically distributed, the original distribution has to have a finite variance, and the sample size must be sufficiently large.

\begin{example}
Suppose a factory produces light bulbs, and the lifespan of each light bulb is a discrete random variable with a mean of $\mu = 1000$ hours and a standard deviation of $\sigma = 50$ hours. The factory tests a random sample of $n = 36$ light bulbs to estimate the average lifespan of the bulbs produced in a particular batch. What is the probability that the sample mean lifespan of these 36 bulbs is between 990 and 1010 hours?

We can apply the Central Limit Theorem to solve this problem because we are dealing with the sample mean of a large number of independent, identically distributed discrete random variables (lifespans of light bulbs). First, define the sample mean $\overline{X}_n$ as:
\[
\overline{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i,
\]
where $X_i$ represents the lifespan of the $i$-th light bulb in the sample, and $n = 36$ is the sample size.

By the Central Limit Theorem, for sufficiently large $n$, the distribution of the sample mean $\overline{X}_n$ approaches a normal distribution with mean $\mu$ and standard deviation $\sigma/\sqrt{n}$:
\[
\overline{X}_n \sim N\left(\mu, \frac{\sigma}{\sqrt{n}}\right) = N\left(1000, \frac{50}{\sqrt{36}}\right) = N\left(1000, \frac{50}{6}\right) = N\left(1000, 8.33\right).
\]

Next, we calculate the z-scores corresponding to 990 hours and 1010 hours using the formula:
\[
z = \frac{X - \mu}{\sigma/\sqrt{n}},
\]
where $X$ is the value for which we want to find the z-score.

For $X = 990$:
\[
z_{990} = \frac{990 - 1000}{8.33} \approx \frac{-10}{8.33} \approx -1.20.
\]

For $X = 1010$:
\[
z_{1010} = \frac{1010 - 1000}{8.33} \approx \frac{10}{8.33} \approx 1.20.
\]

Now, we use the standard normal distribution table (or a calculator) to find the probabilities corresponding to these z-scores:
\[
P(z_{990} \leq Z \leq z_{1010}) = P(-1.20 \leq Z \leq 1.20).
\]

From the standard normal distribution table:
\[
P(Z \leq 1.20) \approx 0.8849,
\]
\[
P(Z \leq -1.20) \approx 0.1151.
\]

Thus, the probability that the sample mean lifespan of the 36 light bulbs is between 990 and 1010 hours is:
\[
P(990 \leq \overline{X}_n \leq 1010) = P(-1.20 \leq Z \leq 1.20) = 0.8849 - 0.1151 = 0.7698.
\]

The probability that the sample mean lifespan of the 36 light bulbs falls between 990 and 1010 hours is approximately 76.98\%. This result demonstrates how the Central Limit Theorem allows us to use the normal distribution to approximate the sampling distribution of the sample mean, even when the original data is not normally distributed, as long as the sample size is sufficiently large.
\end{example}

%
% Section: References
%
\section*{References}

\cite{degroot1986probability} is a widely respected textbook in the fields of statistics and probability theory. First published in 1975, this book is known for its clear exposition of the fundamental concepts of probability and statistics, making it suitable for both beginners and those with some background in the subject. The book's approach balances theory and application, making it useful both for learning theoretical underpinnings and for applying probability and statistics to real-world problems.

\cite{childers2013philosophy} offers a comprehensive introduction to the foundational aspects of probability, with a focus on the philosophical questions it raises. Childers explores various interpretations of probability, including frequentist, propensity, classical, Bayesian, and objective Bayesian, and presents these complex ideas in a way that is accessible even to those without a strong background in probability or mathematics.

An example of the problems associated with the misinterpretation of expected value is the St. Petersburg Paradox\index{St. Petersburg Paradox}. Introduced by Nicholas Bernoulli in 1713, this paradox involves a gambling game with an infinite expected payoff, yet no reasonable person would pay more than \$25 to play it. Despite being three centuries old, the paradox continues to inspire new arguments and solutions in recent years (see \cite{huang2013three} for a historical review of the main proposed solutions).
