%
% CHAPTER: Surfeit
%

\chapterimage{Alexander_cuts_the_Gordian_Knot.pdf} % Chapter heading image

\chapter{Surfeit}
\label{chap:Redundancy}

\begin{quote}
\begin{flushright}
\emph{Everything should be made as simple as possible,\\
but not simpler.}\\
Albert Einstein 
\end{flushright}
\end{quote}
\bigskip

Surfeit is the final metric we introduce to quantitatively assess our understanding of a research entity. It measures the presence of superfluous symbols in the description used to model that entity. Intuitively, our lack of knowledge is often reflected in the length (i.e., number of symbols) of our current description. Lengthy descriptions tend to include erroneous or redundant elements. As our understanding of the subject improves, we should be able to identify and remove these unnecessary symbols, resulting in a more concise and accurate description.

We define the surfeit of a description for an entity as the difference in length between the given description and the optimal (i.e., shortest) one for that entity. Within the framework of the theory of nescience, we assume that the pinnacle of knowledge about an entity, its perfect description, is represented by the shortest description capable of fully reconstructing the entity's representation. This notion of perfection relies on both the validity of the representation and the accuracy of the description.

The length of the most concise description of an entity is determined by the Kolmogorov complexity of a representation of that entity. In practical scenarios, given that our knowledge of entities is typically incomplete, the most concise possible description remains unknown. Moreover, as previously discussed, Kolmogorov complexity is not computable in general. Consequently, surfeit is a metric that must be approximated in practice.

If we could construct a perfect description of an entity, it would necessarily be a random string; otherwise, it would contain redundant elements that could be eliminated. Within the framework of the theory of nescience, attaining perfect knowledge corresponds to reaching a state of randomness. This inherent randomness defines a boundary on the depth of understanding achievable for a given research topic. However, rather than constituting a limitation, recognizing and understanding this boundary opens new opportunities in both science and technology. For example, by assessing how far our current description deviates from a random string, we can estimate our proximity to realizing a perfect description.

In this chapter, we formally introduce the concept of surfeit and examine its properties, including conditional surfeit. We will also present the notion of redundancy as a practical approximation of surfeit. Strategies for reducing both surfeit and redundancy will be discussed, as well as the relationship between reductions in surfeit and changes in inaccuracy or miscoding. Finally, we extend the concept of surfeit to support the analysis of entire research areas.

%
% Section: Surfeit
%

\section{Surfeit}
\label{sec:Definition_surfeit}

Given the length of a description of a representation for an entity and the length of its shortest possible description, we can introduce a relative measure to quantify the unnecessary effort involved in explaining the entity using that particular description. We call this quantity \emph{surfeit}. Surfeit is a key component of our definition of nescience, as it reflects the degree to which our current understanding of the research entity includes superfluous or redundant information.

\begin{definition}[Surfeit]
Given a representation $r \in \mathcal{B}^\ast$, and a description $d \in \mathcal{D}$ for $r$, we define the \emph{surfeit of the description $d$ for the representation $r$}\index{Surfeit}, denoted by $\sigma(d, r)$, as
\[
\sigma (d, r) = \frac{ | l(d) - K(r) |}{l(d)}
\]
\end{definition}

For most descriptions, the length of the description $l(d)$ for $r$ will exceed the length of its shortest possible description $K(r)$. Intuitively, the less we know about an entity, the longer our description tends to be. As our understanding of the entity improves, we should be able to remove all redundant elements from the description. There may also be cases where the description is shorter than the optimal one. In such instances, the description oversimplifies the problem, which can be equally problematic. This justifies the use of the absolute value $| l(d) - K(r) |$ rather than simply $l(d) - K(r)$. Naturally, the current description might also be inaccurate, or the representation may be invalid. These concerns are addressed separately by the metrics of inaccuracy and miscoding.

In our definition of surfeit, we chose a relative measure rather than an absolute one (i.e., $| l(d) - K(r) |$) because we aim to compare the surfeit not only among different models of the same entity but also across models of different entities. We prefer to use $K(r)$ instead of the equivalent $l \left( r^\star \right)$ to maintain consistency with the definition of inaccuracy provided in Section \ref{sec:inaccuracy:inaccuracy}.

\begin{example}
In a practical machine learning scenario, consider the task of classifying images of cats and dogs, where the representation consists of a set of training images. An initial complex model, burdened with excessive parameters and irrelevant features, can be seen as a lengthy "description" of the problem. The optimal model, by contrast, achieves a balance between accuracy and simplicity. Surfeit quantifies the excess complexity of the initial model relative to this optimal one. It measures the "extra" components that are not essential for accurate classification. A high surfeit indicates that the model is overly complicated, highlighting the need for simplification to improve both efficiency and generalization, a fundamental principle in machine learning for building models that perform well on unseen data.
\end{example}

The surfeit of a description is a number between $0$ and $1$.

\begin{proposition}
\label{prop:range_surfeit}
Let $r \in \mathcal{B}^\ast$ be a representation, and $d \in \mathcal{D}^\star_r$ one of its valid descriptions, then we have that $0 \leq \sigma(d, r) \leq 1$.
\end{proposition}
\begin{proof}
The numerator $|l(d) - K(r)|$ is non-negative for all $d$ and $r$, and the denominator $l(d) > 0$ (as descriptions must be non-empty). Hence, the entire expression is non-negative $\sigma(d, r) \geq 0$.

To show that $\sigma(d, r) \leq 1$, observe that:
\[
|l(d) - K(r)| = l(d) - K(r) \leq l(d).
\]
since $d$ is a valid description of $r$, and so $l(d) \geq K(r)$. Thus:
\[
\sigma(d, r) = \frac{|l(d) - K(r)|}{l(d)} \leq \frac{l(d)}{l(d)} = 1.
\]
\end{proof}

The surfeit is zero when the length of the description $l(d)$ equals the Kolmogorov complexity $K(r)$ of the representation $r$ of the entity, indicating that the description has achieved theoretical conciseness.

\begin{proposition}
\label{prop:zero_surfeit}
Let $r \in \mathcal{B}^\ast$ be a representation, and $d \in \mathcal{D}^\star_r$ a valid description for $r$, then we have that  $\sigma(d, r) = 0$ if and only if $l(d) = l(d^\star)$.
\end{proposition}
\begin{proof}
Assume $l(d) = l(d^\star) = K(r)$. Then:
\[
\sigma(d, r) = \frac{ |l(d) - K(r)| }{ l(d) } = \frac{ |K(r) - K(r)| }{ K(r) } = \frac{0}{K(r)} = 0.
\]
Assume $\sigma(d, r) = 0$. Then:
\[
\frac{ |l(d) - K(r)| }{ l(d) } = 0.
\]
This implies that the numerator must be zero, i.e.:
\[
|l(d) - K(r)| = 0 \Rightarrow l(d) = K(r) = l(d^\star).
\]
\end{proof}

It is important to distinguish between conciseness and correctness. A surfeit of zero indicates that the description is as brief as theoretically possible, but not necessarily accurate. The accuracy of a description is evaluated using the inaccuracy metric. Thus, while a zero surfeit reflects the optimal compactness of a description, its correctness and reliability must be assessed separately through inaccuracy. Together, surfeit and inaccuracy provide a more complete assessment of the description's efficiency and validity.

The following example underscores the need to balance minimal surfeit with low inaccuracy in order to develop models that are both efficient and reliable.

\begin{example}
In a machine learning scenario, consider a model designed to classify emails as spam or not. Suppose this model achieves a surfeit of zero, indicating optimal conciseness with no superfluous elements in its description. However, despite this streamlined complexity, the model exhibits high inaccuracy, frequently misclassifying emails. This illustrates the distinction between surfeit and inaccuracy: while the model is theoretically as concise as possible, reflected in its zero surfeit, its practical utility is compromised by incorrect classifications, as captured by the inaccuracy metric. 
\end{example}

It is important to note that, in theory, more than one description may yield a surfeit of zero for the same representation. This occurs when two distinct, yet incompressible, descriptions have exactly the same length. In such cases, both descriptions are equally concise and minimal, meaning that neither contains redundant information that could be further compressed. Since surfeit measures the relative difference between the actual length of a description and the shortest possible one, any description whose length equals the Kolmogorov complexity $K(r)$ of the representation will result in $\sigma(d, r) = 0$. Therefore, while the perfect description is often referred to as unique, from a theoretical standpoint, multiple descriptions of equal minimal length and incompressibility may coexist.

%
% Section: Redundancy
%

\section{Redundancy}
\label{sec:Definition_redundancy}

Our definition of surfeit compares the length of a description with the Kolmogorov complexity of the representation, not with the Kolmogorov complexity of the description itself (i.e., $K(d)$). In other words, surfeit is not a measure of the redundancy within a description. It is possible to construct an incompressible description (i.e., one without any internal redundancy) that is not the shortest possible description of the representation it refers to (see Example \ref{ex:description_neural}). Such a description would not be redundant in the traditional sense, yet it would still exhibit surfeit according to the theory of nescience.

Moreover, it might happen that the description $d$ under consideration does not describe the representation $r$; in other words, $d \notin \mathcal{D}_r$. For practical applications, it is useful to introduce an alternative, and arguably weaker, notion of redundancy that applies solely to the description itself, independently of any particular representation.

\begin{definition}[Redundancy]
Given a description $d \in \mathcal{D}$, we define the \emph{redundancy}\index{Redundacy} of the description $d$, denoted by $\rho(d)$, as
\[
\rho(d) = 1 - \frac{K(d)}{l(d)}
\]
\end{definition}

The redundancy of a description $d$ is a quantity related to the description itself, and it does not depend on the representation $r$ being described. The following example shows that a description can have low redundancy (being incompressible) and still have high surfeit if it is longer than necessary to describe the underlying task.

\begin{example}
Consider the task of computing the first $n$ digits of $\pi$. Description $d_1$ is a program that implements an algorithm to generate and print these $n$ digits. Description $d_2$ is a self-extracting compressed program that simply stores the first $n$ digits of $\pi$ as a hard-coded string, followed by a basic routine to print them. 

Description $d_2$ does not exploit the mathematical structure of $\pi$, it merely reproduces the result. Therefore, relative to the representation of the task (computing $\pi$), the surfeit \( \sigma(d_2, r) \) is high because \( l(d_2) \gg K(r) \). 

In contrast, description $d_1$ is significantly shorter since it captures the generative process behind $\pi$. While $d_1$ may still contain some redundancy \( \rho(d_1) > 0 \), for instance due to suboptimal coding practices, its surfeit remains low since it is near the minimal length required to describe the task effectively.
\end{example}

The redundancy of a description always falls within the range $[0, 1]$.

\begin{proposition}
We have that $0 \leq \rho(d) \leq 1$ for all $d \in \mathcal{D}$.
\end{proposition}
\begin{proof}
Since Kolmogorov complexity is always less than or equal to the length of the string, we have $0 \leq K(d) \leq l(d)$, 
which implies that $0 \leq \frac{K(d)}{l(d)} \leq 1$. Subtracting from 1 reverses the inequalities.
\end{proof}

Finally, next proposition formalizes our intuition that the surfeit of a description is greater or equal than its redundancy.

\begin{proposition}
\label{prop:surfeit_comparison}
Let $r \in \mathcal{B}^\ast$ be a representation , and $d \in \mathcal{D}^\star_r$ one of its valid descriptions, then we have that $\rho(d) \leq \sigma(d, r)$.
\end{proposition}
\begin{proof}
Proving that $\rho(d) \leq \sigma(d, r)$ is equivalent to prove that $K(d) \geq K(r)$ for all $d$. Lets assume that there exist a $d$ such that $K(d) < K(r)$, that would mean there exists a Turing machine $\langle TM, a \rangle$ such that $TM(a)=r$ but $l(<TM, a>) < K(r)$. That is a contradiction with the fact that $K(r)$ is the length of the shortest possible Turing machine that prints $r$.
\end{proof}

It would be very nice if Proposition \ref{prop:surfeit_comparison} applies to all possible description. Unfortunately, the proposition is true only when we deal with valid descriptions (from $\mathcal{D}^\star_r$).

%
% Section: Conditional Surfeit
%

\section{Conditional Surfeit}

We are interested in studying how the surfeit of a description for a representation is affected when some background knowledge is assumed. In particular, we examine the case where a description is constructed under the assumption that some prior information is already known. The surfeit of such a description is referred to as the \emph{conditional surfeit}.

\begin{definition}
Let $r \in \mathcal{B}^\ast$ be a representation, $s \in \mathcal{B}^\ast$ a string, and $d \in \mathcal{D}$ a description of $r$ given $s$. We define the \emph{conditional surfeit}\index{Conditional surfeit} of the conditional description $d_{r \mid s}$, denoted by $\sigma(d_{r \mid s})$, as: 
\[
\sigma(d_{r \mid s}) = 1 - \frac{K\left( r \mid s \right)}{l \left( d_{r \mid s} \right)}
\]
\end{definition}

This definition is primarily motivated by practical considerations. When we assume perfect knowledge of $s$, we can isolate and study the informational content of $r$ that is not already covered by $s$, that is, the new knowledge introduced by $r$ relative to $s$. Conditional surfeit thus allows us to quantify the conciseness of a description in light of what is already known.

Conditional surfeit, being a relative measure, is a number between $0$ and $1$.

\begin{proposition}
Let $r \in \mathcal{B}^\ast$ be a representation, $s \in \mathcal{B}^\ast$ be a string, and $d \in \mathcal{D}$ be a description of $r$ given $s$. We have that $0 \leq \sigma(d_{r \mid s}) \leq 1$.
\end{proposition}
\begin{proof}
Given that $l \left( d_{r \mid s} \right) > 0$ and that $K\left( r \mid s \right) > 0$, since they are the lengths of non-empty strings, and that $ll \left( d_{r \mid s} \right) \geq K\left( r \mid s \right)$.
\end{proof}

Intuition tell us that the surfeit of a description could only decrease if we assume the background knowledge given by the description of another topic. This is because we require that this background knowledge must be a perfect description (it presents no surfeit). However, as it was the case of joint surfeit, we have to wait until Chapter \ref{chap:Nescience} to formalize this intuition.

In the same way we introduced the concept of redundancy of a description as a weaker version of the concept of surfeit, we can also introduce the concept of conditional redundancy as a weaker version of the concept of conditional surfeit.

\begin{definition}
Let $s \in \mathcal{B}^\ast$ be a string, and $d \in \mathcal{D}$ be a conditional description given $s$. We define the \emph{conditional surfeit} of the conditional description $d_{t \mid s^\star}$, denoted by $\sigma(d_{t \mid s^\star})$, as: 
\[
\rho(d_{r \mid s}) = 1 - \frac{K \left( d_{t \mid s^\star} \right)}{l \left( d_{t \mid s^\star} \right)}
\]
\end{definition}

Conditional surfeit is a relative measure, and so, a number between $0$ and $1$.

\begin{proposition}
We have that $0 \leq \rho(d_{t \mid s^\star}) \leq 1$ for all $t,s$ and all $d_{t,s}$.
\end{proposition}
\begin{proof}
Given that $K(d_{t \mid s^\star}) \leq l(d_{t \mid s^\star})$ we have that $\frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} \leq 1$ and so, $1 - \frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} \geq 0$. Also, since $\frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} > 0$ (both quantities are positive integers), we have that $1 - \frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} \leq 1$.
\end{proof}

Finally, we can extend our concepts of conditional surfeit and conditional redundancy to multiple, but fine, number of topics.

\begin{definition}
Let $t, s_1, s_2, \ldots, s_n \in \mathcal{T}$ be a finite collection of topics, and let $d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star}$ any conditional description of $t$ given $s_1, s_2, \ldots, s_n$. We define the \emph{conditional surfeit} of the description $d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star}$, denoted by $\sigma(d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star})$, as: 
\[
\sigma(d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star}) = 1 - \frac{K\left( t \mid s_1^\star, s_2^\star, \ldots,s_n^\star \right)}{l \left( d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star} \right)}
\]
And the \emph{conditional redundancy} of the description $d_{t_1, t_2, \ldots, t_n}$, denoted by $\rho(d_{t_1, t_2, \ldots, t_n})$, as:
\[
\rho(d_{t_1, t_2, \ldots, t_n}) = 1 - \frac{K(d_{t_1, t_2, \ldots, t_n})}{l \left( d_{t_1, t_2, \ldots, t_n} \right)}
\]
\end{definition}

It is easy to show that the properties of conditional surfeit and conditional redundancy apply to the case of multiple topics as well.

%
% Section: Decreasing Surfeit
%

\section{Decreasing Surfeit}

Our objective is to reduce the surfeit of our current best description $d_1$, thereby enhancing our understanding of the original entity. This improvement may involve either refining $d_1$ to eliminate redundant symbols or developing an entirely new description based on a different modeling approach. In either case, the result is a new description $d_2$. In this section, we analyze how the introduction of a new description $d_2$ affects the surfeit relative to that of the original description $d_1$.

\begin{definition}
Let $r \in \mathcal{B}^\ast$ be a representation, and let $d_1, d_2 \in \mathcal{D}$ be two descriptions. We define the \emph{variation of surfeit}\index{Variation of surfeit} between the descriptions $d_1$ and $d_2$, with respect to $r$, denoted by $\Delta^{a}_{\sigma} ( d_1, d_2, r )$, as:
\[
\Delta^{a}_{\sigma} ( d_1, d_2, r ) = \sigma(d_1, r) - \sigma(d_2, r)
\]
\end{definition}

Since surfeit is bounded between 0 and 1, the maximum possible variation is $\pm 1$. A positive value of $\Delta_{\sigma}$ indicates that $d_2$ is preferable to $d_1$ in terms of surfeit. Conversely, a negative value suggests that $d_1$ is more concise than $d_2$.

It is important to emphasize that a new description may also introduce a substantial increase in inaccuracy, potentially offsetting the improvement in surfeit. For a detailed discussion of inaccuracy, refer to Chapter \ref{chap:Error}, and for an explanation of how inaccuracy and surfeit combine into the unified metric of nescience, see Chapter \ref{chap:Nescience}.

We can also introduce a relative measure of the variation in surfeit when transitioning from description $d_1$ to $d_2$.

\begin{definition}
Let $r \in \mathcal{B}^\ast$ be a representation, and $d_1, d_2 \in \mathcal{D}$ be two descriptions. We define the \emph{relative variation of surfeit}\index{Relative variation of surfeit} between descriptions $d_1$ and $d_2$, denoted by $\Delta^{r}_{\sigma}(d_1, d_2, r)$, as:
\[
\Delta^{r}_{\sigma}(d_1, d_2, r) = \frac{\sigma(d_1, r) - \sigma(d_2, r)}{\sigma(d_1, r)}
\]
provided that $\sigma(d_1, r) \neq 0$.
\end{definition}

A value of $0$ indicates that there is no change in surfeit between the two descriptions. A value of $1$ corresponds to a complete elimination of surfeit, meaning that the new description $d_2$ has zero surfeit, while the original description $d_1$ had a nonzero surfeit. Negative values indicate that surfeit has increased, suggesting that the new description $d_2$ either includes more irrelevant or redundant symbols than $d_1$, or is too short to adequately represent the entity, thus omitting important information. It is important to note that the relative variation can become arbitrarily negative. This occurs when the surfeit of the original description $\sigma(d_1, r)$ is close to zero, while the surfeit of the new description $\sigma(d_2, r)$ is significantly larger. In such cases, the denominator becomes very small, causing the relative variation to diverge toward $-\infty$.

As surfeit approaches zero, relative variations become increasingly unstable. A small absolute change can result in a large relative variation when the initial surfeit is very low. For example, if $\sigma(d_1, r) = 0.1$ and surfeit decreases by 0.05, the relative improvement is 50\%. However, if $\sigma(d_1, r) = 0.9$ and the same absolute reduction occurs, the relative improvement is only about 5.6\%. Therefore, both absolute and relative variations are important for assessing the significance and scale of changes in surfeit.

%
% Section: Surfeit-inaccuracy rate of Change
%

\section{Surfeit-inaccuracy rate of Change}

In the preceding section, we examined how the surfeit of a description can be reduced by modifying that description, either by adding missing symbols, or by removing redundant, or irrelevant symbols. In this section, we turn our attention to a more general approach for reducing the surfeit associated with an entity, by allowing a increase in the inaccuracy of the description. Rather than reducing the surfeit in isolation, it may be more effective to modify both, surfeit and inaccuracy, simultaneously.

The balance between the amount of inaccuracy we are willing to accept in order to achieve a reduction in surfeit is referred to as the \emph{surfeit-inaccuracy trade-off}. For a broader discussion of trade-offs in multi-objective optimization, refer to Section \ref{sec:trade_offs}.

\begin{definition}
Let $d_1, d_2 \in \mathcal{D}$ be two descriptions, and $r \in \mathcal{B}^\star$ a representation. We define the \emph{rate of change between the surfeit and inaccuracy} of the descriptions $d_1, d_2$ given the representation $r$, denoted by $\Delta_{\sigma \iota} (d_1, d_2, r)$ as:
\[
\Delta_{\sigma \iota} ( \mathbf{x}_1, \mathbf{x}_2 ) = \frac{\sigma(d_2, r) - \iota(d_1, r)}{\iota(d_2, r) - \iota(d_1, r)}
\] 
provided that $\iota(d_2, r) - \iota(d_1, r) \neq 0$.
\end{definition}

The ratio $\Delta_{\sigma \iota}$ represents the rate of change between surfeit and inaccuracy when transitioning from the first description to the second. A positive value of $\Delta_{\sigma \iota}$ implies that both quantities, surfeit and inaccuracy, either decrease (which is desirable) or increase (which is undesirable). The interpretation becomes more nuanced when $\Delta_{\sigma \iota}$ is negative, indicating that one of the quantities decreases while the other increases. In such cases, two scenarios must be considered:

\begin{enumerate}[label=(\roman*)]
\item Surfeit decreases and inaccuracy increases, we aim for $\Delta_{\sigma \iota}(d_1, d_2, r) < M$, where $M < -1$, thereby ensuring that the reduction in surfeit compensates for the increase in inaccuracy.
\item Surfeit increases and inaccuracy decreases, we aim for $\Delta_{\sigma \iota}(d_1, d_2, r) > M$, where $-1 < M < 0$, thereby ensuring that the reduction in inaccuracy justifies the increase in surfeit.
\end{enumerate}

In both cases, caution is warranted when the change in inaccuracy is small, as it can disproportionately affect the ratio and potentially lead to misleading conclusions.

\begin{example}
Let $d_1, d_2 \in \mathcal{D}$ be two descriptions, and $r \in \mathcal{B}^\star$ a representation. For $d_1$, the surfeit $\sigma(d_1, r_1)$ is 0.40, and the inaccuracy $\iota(d_1, r)$ is 0.15. For $d_2$, the surfeit $\sigma(d_2, r)$ is 0.20 (a decrease from 0.40), and the inaccuracy $\iota(d_2, r)$ is 0.25 (an increase from 0.15). Using the definition of the rate of change, we compute:
\[
\Delta_{\sigma \iota} (d_1, d_2, r) = \frac{0.40 - 0.20}{0.15 - 0.25} = \frac{0.20}{-0.10} = -2
\]
In this case, transitioning from description $d_1$ to $d_2$ results in a decrease of surfeit by 0.20 units and an increase in inaccuracy by 0.10 units. The rate of change is $-2$.
\end{example}

Having a very small change in inaccuracy can significantly amplify the value of the rate of change, potentially giving the misleading impression that surfeit and inaccuracy are varying at an extreme rate, even when the actual changes are minor. This phenomenon is illustrated in the following example.

\begin{example}
Let $d_1 \in \mathcal{D}$ be a description and $r \in \mathcal{B}^\star$ a representation with an surfeit $\sigma(d_1, r)$ of 0.35 and a inaccuracy $\iota(d_1, r)$ of 0.20. Let $d_1 \in \mathcal{D}$ be a second description with an surfeit $\sigma(d_2, r)$ of 0.30 (a slight decrease from 0.35) and a inaccuracy $\iota(d_2, r)$ of 0.2001 (a very small increase from 0.20). Applying the definition of the rate of change:
\[
\Delta_{\sigma \iota} (d_1, d_2, r) = \frac{0.35 - 0.30}{0.20 - 0.2001} = \frac{0.05}{-0.0001} = -500
\]
The rate of change is $-500$, which may misleadingly suggest a dramatic shift. In reality, the surfeit only decreased by 0.05 units, and the inaccuracy increased by a negligible 0.0001 units. The extremely small denominator inflates the result, making the change appear far more significant than it actually is.
\end{example}

As we attempt to optimize both surfeit and inaccuracy, we inevitably encounter descriptions where improving one objective necessitates compromising the other. The set of such "best trade-off" configurations constitutes the Pareto frontier (see Section \ref{sec:multiobjective_optimization}). Points on the Pareto frontier are said to be Pareto optimal because any attempt to improve one objective leads to a deterioration in the other.

\begin{definition}
Let $d_1 \in \mathcal{D}$ be a description and $r \in \mathcal{B}^\star$ a representation. The description $d_1$ is said to be a Pareto point\index{Pareto point} with respect to surfeit $\sigma$ and inaccuracy $\iota$ if there does not exist another description $d_2$ such that:
\begin{enumerate}
    \item $\sigma(d_2, r) \leq \sigma(d_1, r)$ and $\iota(d_2, r) \leq \iota(d_1, r)$, and
    \item $\sigma(d_2, r) < \sigma(d_1, r)$ or $\iota(d_2, r) < \iota(d_2, r)$.
\end{enumerate}
\end{definition}

In simpler terms, a description is Pareto optimal if: (i) no other description is better in both surfeit and inaccuracy, and (ii) any improvement in one metric necessarily results in a worsening of the other.

The rate of change $\Delta_{\sigma \iota}$ between any two Pareto optimal points provides insight into how the trade-off between surfeit and inaccurac evolves along the Pareto frontier. If decision-makers are more sensitive to changes in surfeit than to inaccuracy, they may prefer configurations with a less negative $\Delta_{\sigma \iota}$. Conversely, if inaccuracy is of greater concern, they may accept solutions where $\Delta_{\sigma \iota}$ indicates a larger increase in surfeit in exchange for a smaller gain in inaccuracy.

%
% Section: Surfeit of Areas
%

\section{Surfeit of Areas}

The concept of surfeit can be extended to research areas, to quantitative measure the amount of extra effort we are using to describe the topics of the area.

\begin{definition}
Let $A \subset \mathcal{T}$ be an area with known subset $\hat{A} = \{t_1, t_2, \ldots, t_n\}$, and let $d_{\hat{A}}$ be a description. We define the \emph{surfeit of the description} $d_{\hat{A}}$ as:
\[
\sigma \left( d_{\hat{A}} \right) = 1  - \frac{K( \langle t_1, t_2, \ldots, t_n \rangle )}{l \left( d_{\hat{A}} \right)}
\]
\end{definition}

As it was the case of the concept of redundancy, in general we do not know the complexity of the area $K(\hat{A})$, and so, in practice, it must be approximated by the complexity of the descriptions themselves $K(\hat{d}_{\hat{A}})$. However, in the particular case of areas, we could have also problems with the quantity $\hat{d}_{\hat{A}}$, since it requires to study the conditional descriptions of the topics included in the area.

\begin{definition}
Let $A \subset \mathcal{T}$ be an area with known subset $\hat{A} = \{t_1, t_2, \ldots, t_n\}$, and let $d_{\hat{A}}$ be a description. We define the \emph{weak redundancy of the description} $d_{\hat{A}}$ as:
\[
\rho(d_{\hat{A}}) =  1  - \frac{K \left( d_{\hat{A}} \right)}{l \left( d_{\hat{A}} \right)}
\]
\end{definition}

%
% Section: References
%

\section*{References}

The concept of redundancy has been also investigated in the context of information theory, since we are interested on using codes with low redundancy (see for example \cite{abramson1963information}).

