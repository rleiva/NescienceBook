%
% CHAPTER: Probability Theory
%

\chapterimage{Koenigsberg_Map_by_Bering_1613.pdf} % Chapter heading image

\chapter{Discrete Probability}
\label{chap:Probability Theory}

\begin{quote}
\begin{flushright}
\emph{Mathematics may be defined as the subject in which\\
we never know what we are talking about,\\
nor whether what we are saying is true.}\\
Bertrand Russell
\end{flushright}
\end{quote}
\bigskip

%
% Section: Foundations
%

\section{Foundations}
\label{sec:probability_foundations}

\emph{Probability} is a very difficult concept to grasp. Imagine we throw a dice and we want to calculate the probability that the number that appears is even. The dice has six possible outcomes, and since there are three even numbers, we say that the probability of having an even number is $3/6$ or $1/2$. This what the \emph{classical interpretation}\index{Classical interpretation of probability} of probability proposes: if we have an experiment in which all the possible outcomes are equally likely to happen, the probability of an event is the number of favourables cases divided by the total number of cases. The problem with this interpretations is that "equally likely" is essentially the same thing than "having the same probability", and so, it is a circular definition. An alternative approach to assign probabilities would be to apply the \emph{principle of indiference}\index{Principle of indiference}: in absense of any relevant evidence, all possible outcomes should have the same probability. The problem with this principle araises when there is evidence that not all cases are equal. For example, what happens if we know that the dice is loaded?, how do we assign probabilities when not all the sides are equally likely?

The \emph{frequentist interpretation}\index{Frequentist interpretation of probability} of probability proposes throwing the dice multiple times and calculating the releative frequency of even numbers with respect to the total number of throws. The general idea is to repeat the experiment a sufficiently large number of times under similar conditions and assing the releative frequency of each outcome as its probability. This interpretation has two main limitations. First of all, it is not clear how we should repeat an experiment under "similar conditions", since if we use exactly the same conditions the results of all the trials would be the same. The second is that it is not defined what a "large number of times" means (technically speaking, we should repeat the experiment an infinite number of times). From a practical point of view it is very difficult to apply the frequentist interpretation: some experiments cannot be repeated a large number of times, for example, what it is the probability that a candidate wins an election?; probability is defined in terms of a succession of experiments, so we cannot compute the probability of an individual outcome; and we require that the relative frequency limit exists, that is not always the case, for example, in finacial time series.

A third intepreation of the concept of probability, called \emph{subjective interpretation}\index{Subjective interpretation of probability}, proposes to assign to each event a probability based on our degree of belief: the more we belive that an event is true, the higher its probability. Of course, not all possible combinations of probabilities are valid, it is required that some rules of coherence must be satisfied. For example, if we are betting on the result of the dice thrown, an assigment of probabilities that guarantees that we will loose all of our money (what it is called a \emph{dutch book}\index{Dutch book}) do not satify the conditions of the subjective interpretation. It turns out that the conditions neccesary and sufficient to guarantee a fair bet are the same conditions required by the axioms of probability introduced below. In this sense, we can assign to events whatever probabilities we want, as long as they are consistant with the axioms of probability. The problem with the subjective interpretation is that people have different degrees of belief. A solution to this problem is proposed by the \emph{Bayesian interpretation}\index{Bayesian interpretation of probability} of probability: we start with a tentative assigment of probabilities, and as we get futher evidence, we modify our degree of belief, or probability, accordingly; as we get more evidence, estimated probabilities will converge to the true probabilities (see {\color{red} XXX} for more information about Bayes theorem). In any case, assigning probabilities to an infinite number of events is not, in general, human attainable. 

Currently, the concept of probabilty is defined axiomatically (\emph{axiomatic interpretation}\index{Axiomatic interpretation of probability}), which means we give up in trying to define what a probability is, and instead we assume as true some of its properties. Intuitively, a probability should be a number between $0$ and $1$, where an event that cannot happen has a probability of zero, and an event that for sure will happen has a probability of one. That fact that probability must be a number between $0$ and $1$ is more a social convention that a real mathematical requirement, since other ranges of numbers are equally good. We require some additional properties of probabilities. For example, if two events $A$ and $B$, with probabilities $P \left( A \right)$ and $P \left( B \right)$ respectively, cannot happen at the same time, the probabilty that either $A$ or $B$ occurs should be $P \left( A \right) + P \left( B \right)$. If $A$ and $B$ can happen at the same time, but they are not related in any way, that is, they are independent events (whatever that means), we expect that the probability of the two events happening at the same time shold be $P \left( A \right) \dot P \left( B \right)$. Finally, we are also interested in the pobabily of $A$ happeing assuming that $B$ has already happend, that sould be the the fraction of probability $A$ that intersects with $B$. Probability, then, would be anything that satisfies these properties. The problem with the axiomatic interpretation is that too many things satisfy those properties, for example, phisical quantities like normalized mass or normalized volume.

Probability theory is about assigning a number to some events from a sample space. The word "event" is a little bit misleading in this context because, as we will see in Section {\color{red} XX}, it suggest that something happened, which is not always the case. However, to avoid confusion, we will keep calling events to what are essentially subsets.

\begin{definition}
Let $\left( \Omega, \mathcal{A} \right)$ be a field over a non-empty discrete set. We call $\Omega$ the \emph{sample space}\index{Sample space}, its elements \emph{outcomes}\index{Outcome}, and the elements of $\mathcal{A}$ events. In particular, we call $\Omega$ the \emph{certain event}\index{Certain event} and $\varnothing$ the \emph{impossible event}\index{Impossible event}.
\end{definition}

As we saw in Section \ref{sec:sets}, given that $\left( \Omega, \mathcal{A} \right)$ is a field, we have that $\Omega \in \mathcal{A}$ and that $\varnothing \in \mathcal{A}$. Moreover, the union of a finite collection of events is an event $A_1 \cup A_2 \cup \ldots \cup A_n \in \mathcal{A}$, and that the intersection of a fine collection of events is also an event $A_1 \cap A_2 \cap \ldots \cap A_n \in \mathcal{A}$.

As we have said in the previous chapter, our main interest is in discrete mathematics, and so, we will mostly working with probabilities over discrete (finite or countably infinite) sets. An extension of the concept of probability to continuous sets requires the use of $\sigma$-algebras\index{$\sigma$-algebra} of sets instead of fields, and the use of some advanced concepts of measure theory, and so, it is beyond the scope of this book.

The standard axiomaitization used in probability theory are the \emph{Kolmogorov axioms}\index{Kolmogorov axioms}.

\begin{definition}
A \emph{probability}\index{Probability} is a number $P(A) \in \mathbb{R}$ assigned to each event $A \in \mathcal{A}$ of the field $\left( \Omega, \mathcal{A} \right)$, that satisfy the following axioms:

\medskip

\begin{description}
\item [Axiom 1] $P(A) \geq 0$.
\item [Axiom 2] $P(\Omega) = 1$.
\item [Axiom 3] For every finite sequence of disjoint events $A_1, A_2, \ldots, A_n$ we have that $P(\cup_{i=1}^n) = \sum_{i=1}^n P(A_i)$.
\end{description}
\end{definition}

No information is contained in the axioms about how probabilites can be assigned to the events.

\begin{example}
Let $\Omega$ a sample space containing $n$ elements equally probable. If $A \subset \Omega$ is an event with $d(A) = m$, the we have that $P(A) = m/n$.
\end{example}

Let's prove some basic results of probabilities, starting by calculating the probability of the complement of an event, that is, the probability that this event does not happen.

\begin{proposition}
For every event $A$, $P \left( A^{c} \right) = 1 - P \left( A \right)$
\end{proposition}
\begin{proof}
The sets $A$ and $A^c$ are disjoint and $A \cup A^c = \Omega$. Given Axiom 3 we have that $P \left( A \cup A^c \right) = P \left( A \right) + P \left( A^c \right)$, and given Axiom 2 we have that $P \left( A \cup A^c \right) = P(\Omega) = 1$, and so, $P \left( A \right) + P \left( A^c \right) = 1$.
\end{proof}

As a direct consequence of previous proposition, we can derive the probability of the impossible event.

\begin{proposition}
The probability of the impossible event is zero, that is, $P \left( \varnothing \right) = 0$
\end{proposition}
\begin{proof}
Given that $P \left( \varnothing \right) = 1 - P \left( \Omega \right) = 0$
\end{proof}

As expected, sub-events have smaller probabilities than events.

\begin{proposition}
If $A\subset B$ then $P \left( A \right) \leq P \left( B \right)$
\end{proposition}
\begin{proof}
The event $B$ can be decomposed as the union of two disjoint events $A$ and $A^c \cap B$, so we have that $P \left( B \right) = P \left( A \right) + P \left( A^c \cap B \right)$, that combined with the fact that  $P \left( A^c \cap B \right) \geq 0$ proves the proposition.
\end{proof}

Now we have all the elements we need to prove that probabilities are numbers between zero and one.

\begin{proposition}
For every event $A$ we have that $0 \leq P \left( A \right) \leq 1$.
\end{proposition}
\begin{proof}
Based on Axiom 1 and given that $A \subset \Omega$ and so $P \left( A \right) \leq P \left( \Omega \right) = 1$
\end{proof}

Axiom 3 allows us to compute the probability of the union of disjoint events. Next proposition allows us to compute the probability of the union of events that are not disjoint.

\begin{proposition}
For every two events $A$ and $B$, $Pr\left(A\cup B\right)=Pr\left(A\right)+Pr\left(B\right)-Pr\left(A\cap B\right)$.
\end{proposition}
\begin{proof}
The union of sets $A$ and $B$ can be decomposed as the union of the two disjoint sets $A \cup B = B \cup \left( A \cap B^c \right)$, so given Axiom 3 we have that
\[
P \left( A \cup B \right) = P \left( B \right) + P \left( A \cap B^c \right)
\]
In the same way, the set $A$ can be decomposed as the unition of the disjoint sets $A = \left( A \cap B \right) \cup \left( A \cap B^c \right)$, so that
\[
P \left( A \cup B^c \right) = P \left( A \right) - P \left( A \cap B \right)
\]
Combining boths expression we get the desired result.
\end{proof}

A similar formula can be derived for the case of $n$ events.

%
% Section: Conditional Probability
%

\section{Conditional Probability}
\label{sec:probability_conditional}

The concept of conditional probability plays a fundamental role in the area of statistical learning. Traditionally, the conditional probability of event $A$ given event $B$ has been seen as the updated probability of $A$ after we have learnt that $B$ has occurred. However, this interpretation suggests that there is a causal relationshipt between events $B$ and $A$, which is not necessarly true.

Based on Kolmogorov's axioms, conditional probability is defined as a quotient.

\begin{definition}
Let $A$ and $B$ two events such that $P \left( B \right) \neq 0$. The \emph{conditional probability} of $A$ given $B$, denoted by $P \left( A \mid B \right)$, is defined as
\[
P\left(A\mid B\right) = \frac{P\left(A\cap B\right)}{P\left(B\right)}
\]
\end{definition}

Conditional probability is itself a probability, since it satisfy the axioms. Conditional probability $P\left(A\mid B\right)$ is not defined if $P\left(B\right)=0$.

The probability that two events will happen toghether (although not necessarily at the same time), given their conditional probabilities, is $P \left( A \cap B \right) = P \left( A \mid B \right) P \left( B \right)$ or $P \left( A \cap B \right) = P \left( A \mid B \right) P \left( B \right)$. The formula $P \left( A \cap B \right) = P \left( A \mid B \right) P \left( B \right)$ provides a more intuitive interpretation of the concept of conditional probability. There are even some authors that have created alternative axiomatizations of probability in which this property is one of the assumed axioms.

The generalation of this formula for the case of $n$ events, called \emph{multiplication rule}, is $P \left( A_{1} \cap A_{2} \cap \ldots \cap A_{n} \right) = P \left( A_{1} \right) P \left( A_{2} \mid A_{1}\right) \ldots  P \left( A_{n} \mid A_{1}\cap A_{2} \cap \ldots \cap A_{n-1} \right)$.

Independence plays a very important role in probability theory and statistical learning. 

\begin{definition}
Two events $A$ and $B$ are said to be \emph{independent} if $P \left( A \cap B \right) = P \left( A \right) P \left(B \right)$
\end{definition}

From an intuitive point of view, the events $A$ and $B$ are independent if observing that B has occurred does not alter the probability of A. This property can be derived from the definition of independence.

\begin{proposition}
Let $A$ and $B$ two events shuch that $P \left( A \right) > 0$ and $P \left( B \right)>0$, then $A$ and $B$ are independent if and only if $P \left( A \mid B\right) = P \left( A \right)$ and $P \left( B \mid  A \right) = P \left( B \right)$.
\end{proposition}
\begin{proof}
Assume that $A$ and $B$ are independent, that is $P \left( A \cap B \right) = P \left( A \right) P \left(B \right)$, then
\[
P \left( A \mid B \right) = \frac{P\left(A\cap B\right)}{P\left(B\right)} = \frac{P \left( A \right) P \left(B \right)}{P\left(B\right)} = P \left( A \right)
\]
Now let's assume that $P \left( A \mid B \right) = P \left( A \right)$. Given the multiplication rule we have that,
\[
P \left( A \cap B \right) =  P \left( A \mid B \right) P \left( B \right) = P \left( A \right) P \left( B \right)
\] 
The same applies if we interchange $A$ and $B$. 
\end{proof}

As it was the case of conditional probability, some authors claim that independence, being a fundamental concept in probability theory, should be assumed as an axiom, not as a definition.

The concept of independence can be generalized to $n$ events as ... The $k$ events $A_{1},\ldots,A_{k}$ are independents (or mutually independent) if for every subset $A_{i_{1}},\ldots,A_{i_{j}}$ of $j$ of these events $\left(j=2,3,\ldots,k\right)P\left(A_{i_{1}}\cap\ldots\cap A_{i_{j}}\right)=P\left(A_{i_{1}}\right)\ldots P\left(A_{i_{j}}\right)$

\begin{example}
There is some confussion about the difference between mutually exclusive, or disjoint, events and independent events. If $A$ and $B$ are two mutually exclusive events, it does not make too much sense to compute the probability that $A$ will happen given $B$, since if $B$ happens, $A$ cannot happen; in the same way that it does not make too much sense to talk about the conditional probability that $A$ will happen given $B$ if the probability of $B$ is zero.
\end{example}

A particular interesting case is when events $A$ and $B$ are not independent but they become independent if we know that some other even $C$ has happened. 

\begin{definition}
Let's $A$, $B$ and $C$ events such that $P\left( B \cap C \right)>0$. $A$ and $B$ are \emph{conditionally independent} given $C$ if $P\left(A \mid B \cap C \right) = P\left( A \mid C \right)$.
\end{definition}

Baye's theorem provides the foundations of Bayesian inference (see Section XX), a very important technique in the area of statistical learning.

\begin{theorem} (Bayes' Theorem) Let's $A$ and $B$ two events such that $P\left( B \right) \neq 0$. Then we have that
\[
P \left( A \mid B \right) = \frac{P \left( B \mid A \right) P \left( A \right)}{P \left( B \right)}
\]
\end{theorem}
\begin{proof}
\end{proof}

\begin{example}
Let $E$ a disease that affects to one of every one million persons, $P(E) = 1 \times 10^{-6}$, and let $+$ a test designed to detect the disease that fails once every one thousand applications, $P(+ \mid E) = 999/1000$. We are interested in knowing the probability of having the disease if the test is positive $P(E \mid +)$. Applying Bayes' theorem we have that:
\[
P(E \mid +) = \frac{P(+ \mid E) P(E)}{P(+)} = \frac{P(+ \mid E) P(E)}{P(+ \mid E) P(E) + P(+ \mid E^c) P(E^c)} = 0.001
\]
That is, although the test only fails once per thousand applications, it is still very unlikely we have the disease in the case of a positive result. This counterintuitive result is explained because the probability of failure of the test $10^{-3}$ is much higher than the probability of having the disease $10^{-6}$. In practice we solve this problem by applying a second test to those who got a positive result, since the probability of having the disease after two positives is $0.5$ (assuming that the successive repetitions of the test are independent).
\end{example}

The probability $P\left( B \right)$ is called \emph{prior probability}, and $P\left( B\mid A \right)$ is called \emph{posterior probability}.

%
% Section: Random Variables
%

\section{Random Variables}
\label{sec:probability_random_variables}

{\color{red} In particular, if $B$ is a continuous event, the conditional probability is not defined by any individual point. Borel-Kolmogorovo paradox}


A \emph{random variable}\index{Random variable} is a mapping between outcomes and real numbers. Formally, a random variable is a function $X : \Omega \rightarrow \mathbb{R}$, where the probability of having the value $x \in \mathbb{R}$, denoted by $P(X=x)$, is given by $P(\{ \omega \in \Omega : X(\omega) = x\})$. A \emph{parametric random variable} $X$, denoted by $f\left(X \mid \theta \right)$, is a distribution that belongs to a family of functions parameterized by $\theta$, where $\theta$ can be a single parameter, or a vector of several parameters. The \emph{probability mass function}\index{Probability mass function} of a discrete random variable $X$ with range $\{ x_1, x_2, \ldots, x_i, \ldots \}$ is defined as the function $f$ such that $f(x_i) = P(X=x_i)$.

\begin{example}
Suppose we perform $N$ independent trials where each trial either succeeds or fails with probability of success $p$, and let $X$ the random variable defined by the number of successes. The probability of having exactly $n$ successes $P(X=n)$ follows a \emph{binomial distribution} with parameters $N, p$, defined by:
\[
f(n\mid N, p) = \binom{N}{n} p^n (1-p)^{(N-n)}
\]
\end{example}

The \emph{expectation}\index{Expectation} of $X$, denoted by $E(X)$ or $\mu$, is defined as $E(X) = \sum_{x \in \Omega} x f(x)$, and the \emph{standard deviation} as $\sigma = \sqrt{E \left( (X - \mu)^2 \right)}$.

\begin{example}
The expectation is $Np$, and the standard deviation $\sqrt{Np(1-p)}$.
\end{example}

%
% Section: Expectation
%

\section{Characterizing Distributions}
\label{sec:probability_expectation}

A \emph{measure of central tendency} is a number derived from a probability distribution, intended as a summary of that distribution. The most common measures of central tendency in use are the mean and the median. Each of these measures provides a different approach to characterize distributions. It is also common to use \emph{metrics of dispersion} to describe the variability of a distribution around the measures of centrality. We will review two metrics of dispersion, the variance and the standard deviation. The metrics of disperson can also be used in case of bivariate distributions, under the names of covariance and correlation, to measure the \emph{statistical relationship} between two random variables. All these measures allow us to summarize and compare distributions.

% Subsection: Measures of Central Tendency

\subsection{Measures of Central Tendency}

The most comonly used measure of central tendency is the \emph{mean}. The mean of a collection of outcomes is the weigthed average of these outcomes, where the weights are equal to the probabilities. The mean is also know as expected value or expectation.

\begin{definition}\label{probability:expectation}
Let $X$ be a discrete random variable whose probability function is $f$. The \emph{mean} of $X$, denoted by $E\left(X\right)$, is defined as:
\[
E\left(X\right)=\sum_{x}xf\left(x\right)
\]
\end{definition}

Of course, Definition \ref{probability:expectation} only makes sense if the summation converges. It is also possible that the mean is infinite, but in this book we are only interested in finite means. We have defined the concept of mean based on random variables. In this sense, the definition of mean only takes into account the distribution of the random variables, not the original outcomes. That is, two different random variables with the same distribution will have the same mean. When working with random variables it is common to use the name expeced value instead of mean. However, this name is a little bit misleading, since for the majority of the discrete distributions, the expected value is not one of the possible values of the distribution, i.e., the expected value is not expected at all. For example, if we throw a dice, the expected value would be 3.5. This undesired property of something known as expected value has generated a lot of confusion in scientific research. The expectation of a random variable has a physhical interpretation as the center of gravity of the distribution. Expectation, as the center of gravity, can be greatly affected by a small change in the probability assigned to a large value of $X$.

The expectation of the linear combination of $n$ random variables is the linear combination of their expectations.

\begin{proposition}
Let $X_{1}, \ldots, X_{n}$ be $n$ independent discrete random variables with expectations $E\left(X_{i}\right)$, and let $a_1, \ldots, a_n$ and $b$ constants, then
\[
E\left(a_{1}X_{1}+\ldots+a_{n}X_{n}+b\right)=a_{1}E\left(X_{1}\right)+\ldots+a_{n}E\left(X_{n}\right)+b
\]
\end{proposition}
\begin{proof}
We have that
\begin{multline}
E \left(a_1 X_1 + \ldots + a_n X_n +b \right) = 
\sum_{x_1} \ldots \sum_{x_n} \left(a_ 1 x_1 + \ldots + a_n x_n + b  \right) f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} \ldots \sum_{x_n} a_1 x_1 f\left(x_1, \ldots, x_n \right) + \ldots + \sum_{x_1} \ldots \sum_{x_n} a_n x_n f\left(x_1, \ldots, x_n \right) + \sum_{x_1} \ldots \sum_{x_n} b f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} a_1 x_1 f\left(x_1\right) + \ldots + \sum_{x_n} a_n x_n f\left( x_n \right) + b = 
a_1 \sum_{x_1} x_1 f\left(x_1\right) + \ldots + a_n \sum_{x_n} x_n f\left( x_n \right) + b = \\
a_1 E\left(X_1\right) + \ldots + a_n E\left(X_n\right) + b
\end{multline}
\end{proof}

The expectation of the product of $n$ independent random variables is the product of the individual expectations.

\begin{proposition}
Let $X_{1}, \ldots, X_{n}$ be $n$ independent discrete random variables with expectations $E\left(X_{i}\right)$, then:
\[
E\left(\prod_{i=1}^{n}X_{i}\right)=\prod_{i=1}^{n}E\left(X_{i}\right)
\]
\end{proposition}
\begin{proof}
We have that
\begin{multline}
E \left(X_1  \cdot \ldots \cdot X_n  \right) = 
\sum_{x_1} \ldots \sum_{x_n} \left(x_1 \cdot \ldots \cdot x_n  \right) f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} \ldots \sum_{x_n} x_1 f\left(x_1, \ldots, x_n \right) \cdot \ldots \cdot \sum_{x_1} \ldots \sum_{x_n} x_n f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} x_1 f\left(x_1\right) \cdot \ldots \cdot \sum_{x_n} x_n f\left( x_n \right) = 
E \left( X_1 \right) \cdot \ldots \cdot E \left( X_n \right)
\end{multline}
\end{proof}

The expectation of the product of non-independent random variables is not necesarily equal to the product of their individual expectations.

% The Median

We have seen that the mean of a probability distribution is the center of gravity of that distribution. The actual center of the distribution is called  the \emph{median}.

\begin{definition}
Let $X$ be a discrete random variable. Every number $m$ that satisfy the following properties is called a median of the distribution of $X$:
\[
Pr\left(X\leq m\right)\geq1/2 \quad and \quad Pr\left(X\geq m\right) \geq 1/2
\]
\end{definition}

The median divides a probability distribution in two equal parts. A distribution could have more than one median. And, on the contrary of what happens in case of the expectation, every distribution must have at least one median. An advantage of the median over the mean is that we can move a value $x$ larger to the median to any arbitrary larger value, and the median will be remain the same. 

\subsection{Measures of Dispersion}

Definitions of the Variance and the Standard Deviation

\begin{definition}
Let X be a random variable with finite mean and $\mu=E\left(X\right)$. The variance of X, denoted by $Var\left(X\right)$, is defined as follows: $Var\left(X\right)=E\left[\left(X-\mu^{2}\right)\right]$
\end{definition}

{\color{red} If X has infinite mean or if the mean of X does not exist, we say that $Var\left(X\right)$ does not exist. The standard deviation of X is the nonnegative square root of $Var\left(X\right)$ if the variance exists. It is common to denote the standard deviation by the symbol $\sigma$, and the variance by $\sigma^{2}$. Variance depeds only on the distribution.}

\begin{proposition}
For every random variable X, $Var\left(X\right)=E\left(X^{2}\right)-\left[E\left(X\right)\right]^{2}$.
\end{proposition}
\begin{proof}
\end{proof}

{\color{red} The variance (as well as the standard deviation) of a distribution provides a measure of the spread or dispersion of the distribution around its mean $\mu$. The variance of a distribution, as well as its mean, can be made arbitrarily large by placing even a very small but positive amount of probability far enough from the origin on the real line.}

Properties of the Variance

\begin{proposition}
For constants a and b, let $Y=aX+b$, then $Var\left(Y\right)=a^{2}Var\left(X\right)$
and $\sigma_{Y}=\left|a\right|\sigma_{X}$.
\end{proposition}
\begin{proof}
\end{proof}

{\color{red} If $X_{\text{1}},\ldots,X_{n}$ are independent random variables with finite means, and if $a_{1},\ldots,a_{n}$ and b are arbitrary constants, then $Var\left(a_{1}X_{1}+\ldots+a_{n}X_{n}+b\right)=a_{1}^{2}Var\left(X_{1}\right)+\ldots+a_{n}^{2}Var\left(X_{n}\right)$}

\begin{example}
The Variance of a Binomial Distribution

The variance of a random variable X with a binomial distribution of n samples with probability p is $Var\left(X\right)=np\left(1-p\right)$
\end{example}


\subsection{Measures of Statistical Relationship}



Covariance and correlation are attempst to measure the linear dependence between to random variables.

Covarance

\begin{definition}
Definition 183. Let X and Y be random variables having finite means. Let $E\left(X\right)=\mu_{X}$ and $E\left(Y\right)=\mu_{Y}$. The covariance of X and Y, which is denoted by $Cov\left(X,Y\right)$ is defined as $Cov\left(X,Y\right)=E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right]$
\end{definition}

if the expectation exists.

The covariance between X and Y is intended to measre the degree to which X and Y tend to be large at the same time or the degree to which one tends to be large while the other is small.

\begin{proposition}
For all random variables X and Y such that $\sigma_{X}^{2}<\infty$ and $\sigma_{Y}^{2}<\infty Cov\left(X,Y\right)=E\left(XY\right)-E\left(X\right)E\left(Y\right)$
\end{proposition}
\begin{proof}
\end{proof}

Correlation

Correlation is a measure of association between two random variables that is not driven by arbitrary changes in the scales.

\begin{definition}
Let X and Y be random variables with finite variances $\sigma_{X}^{2}$ and $\sigma_{Y}^{2}$ respectively. Then the correlation of X and Y, which is denoted by $\rho\left(X,Y\right)$, is defined as follows $\rho\left(X,Y\right)=\frac{Cov\left(X,Y\right)}{\sigma_{X}\sigma_{Y}}$
\end{definition}

XX

\begin{definition}
It is said that X and Y are positively correlated if $\rho\left(X,Y\right)>0$, that X and Y are negatively correlated if $\rho\left(X,Y\right)<0$ and that X and Yare uncorrelated if $\rho\left(X,Y\right)=0$.
\end{definition}

Properties of Covariance and Correlation

\begin{proposition}
Moreover $-1 \leq \rho\left(X,Y\right) \leq 1$
\end{proposition}
\begin{proof}
\end{proof}

\begin{proposition}
If X and Y are independent random variables with $0<\text{\ensuremath{\sigma_{X}^{2}}}<\infty$ and $0<\text{\ensuremath{\sigma_{Y}^{2}}}<\infty$ then $Cov\left(X,Y\right)=\rho\left(X,Y\right)=0$
\end{proposition}
\begin{proof}
\end{proof}

The converse is not true as a general rule. Two dependent random variables can be uncorrelated.

\begin{proposition}
Suppose that X is a random variable such that $0<\sigma_{X}^{2}\infty$ and $Y=aX+b$ for some constants a and b, where $a\neq0$. If $a>0$ then $\rho\left(X,Y\right)=1$. If $a<0$, then $\rho\left(X,Y\right)=-1$.
\end{proposition}
\begin{proof}
\end{proof}

The converse is also true, that is, if $\left|\rho\left(X,Y\right)\right|=1$ implies that X and Y are linearly related.

\begin{proposition}
If X and Y are random variables such that $Var\left(X\right)<\infty$ and $Var\left(Y\right)<\infty$, then $Var\left(X+Y\right)=Var\left(X\right)+Var\left(Y\right)+2Cov\left(X,Y\right)$
\end{proposition}
\begin{proof}
\end{proof}

For all constants a and b, it can be shown that $Cov\left(aX,bY\right)=abCov\left(X,Y\right)$.

\begin{proposition}
Let X and Y are random variables such that $Var\left(X\right)<\infty$ and $Var\left(Y\right)<\infty$, and let a, b and c be constants, then $Var\left(aX+bY+c\right)=a^{2}Var\left(X\right)+b^{2}Var\left(Y\right)+2abCov\left(X,Y\right)$
\end{proposition}
\begin{proof}
\end{proof}

A special case is $Var\left(X-Y\right)=Var\left(X\right)+Var\left(Y\right)-2Cov\left(X,Y\right)$

\begin{proposition}
If $X_{1},\ldots,X_{n}$ are random variables such that $Var\left(X_{i}\right)<\infty for i=1,\ldots,n$, then $Var\left(\sum_{i=1}^{n}X_{i}\right)=\sum_{i=1}^{n}Var\left(X_{i}\right)+2\sum\sum Cov\left(X_{i},X_{j}\right)$
\end{proposition}
\begin{proof}
\end{proof}

\begin{proposition}
If $X_{1},\ldots,X_{n}$ are uncorrelated random variables, then $Var\left(\sum_{i=1}^{n}X_{i}\right)=\sum_{i=1}^{n}Var\left(X_{i}\right)$
\end{proposition}
\begin{proof}
\end{proof}


%
% Section: Distribution
%

\section{Distributions}
\label{sec:probability_distributions}


%
% Section: Large Random Samples
%

\section{Large Random Samples}
\label{sec:probability_random_samples}

