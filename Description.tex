%
% CHAPTER: Entities, Represenations and Descriptions
%

\chapterimage{owl.pdf} % Chapter heading image

\chapter{Entities, Representations and Descriptions}
\label{cha:Topics-and-Descriptions}

\begin{quote}
\begin{flushright}
\emph{We are all agreed that your theory is crazy. \\
The question which divides us is whether it is crazy enough.} \\
Niels Bohr
\end{flushright}
\end{quote}
\bigskip

The first step to quantitatively measure how much we do not know is to identify clearly the collection of research entities under study. The exact elements of this collection is something that depends on the particular application in which the theory of nescience is being used. Different applications require different collections of entities (mathematical objects, living things, human needs, etc.). Fortunately, the procedure to compute how much we do not know is the same in all cases.

The second step is to provide a method to encode the set of identified entities as strings of symbols, what we call representations. How to properly encode a research entity with symbols is a difficult, still unsolved, epistemological problem. The solution we propose in the theory of nescience is based in the concept of oracle Turing machine. How easy is to implement this solution in practice is something that depends on how abstract are the entities under study. For example, the collection of all abstract mathematical objects is a very difficult set to encode, and so, an approximation has to be found; the collection of all possible computer programs (given that our area of interest is software quality, see Chapter \ref{chap:Software-Engineering}) is far easier to encode, since computer programs are strings themselves.

The final step, once we have found a way to properly encode the original set of entities as string-based representations, is to provide a description, as accurate and succinct as possible, of what we already know about those representations. In the theory of nescience we require that descriptions must be computable, that is, given a description, a computer should be able to fully reconstruct the original representation. A difficult problem that arise with descriptions is that they characterize representations, that is, the encoding of the entities, not the entities themselves, and so, the quality of a description for an entity is conditional to the quality of the representation used.

In this chapter we will formalize all these concepts: entities, representations, descriptions, and many others. We will also see what we mean by perfect knowledge, how to compute the combined representation of multiple entities, and the description of a representation assuming a perfect knowledge of another one.

%
% Section: Entities
%

\section{Entities}
\label{sec:descriptions_entities}

What exactly is a research entity is a difficult, still unsolved, philosophical problem. Our approach to address this complex issue is eminently practical. Our theory starts by assuming there exists a non-empty collection of \emph{entities}\index{Entities} we would like to understand.

\begin{notation}
Denote by $\mathcal{E}$ the set of research entities.
\end{notation}

The exact elements that compose $\mathcal{E}$ is something that depends on the particular domain in which the theory of nescience is being applied, but they usually corresponds to an area or subarea of knowledge. Examples of sets of entities could be: research elements in mathematics (abstract); the kingdom of animalia (living things); known and unknown human needs (abstract); all possible computer programs (strings), etc. Technically speaking $\mathcal{E}$ is not a well defined set, since in general we cannot tell what it is and what is not a member of this set.

The abstract nature of $\mathcal{E}$ has some advantages, but also introduces important limitations. The main limitation is that our definition of nescience is, in general, a non-computable quantity, and so, it must be approximated in practice. The main advantage is that we can apply the new concepts and methods introduced in this book to other problems, not only to the discovery of new scientific knowledge.

In the theory of nescience we do not allow universal sets, that is, we cannot assume the existence of a set $\xi$ that contains everything. The problem of universal sets is that they violate Cantor's theorem (see Example \ref{cantor_theorem}). Cantor's theorem states that the power set $\mathcal{P}(\xi)$ composed by all the possible subsets of $\xi$ has more elements than the original set $\xi$, and this is a contradiction with the fact that $\xi$ contains everything. In the theory of nescience the set $\mathcal{E}$ must be the set of "something".

\begin{example}[Cantor's theorem]
\label{cantor_theorem}
The Cantor's theorem states that for every set $A$ we have that $d(A) < d\left(\mathcal{P}(A)\right)$. Let $f: A \rightarrow \mathcal{P}(A)$ a function that maps every element of $x \in A$ to the set $\{x\} \in \mathcal{P}(A)$; clearly $f$ is injective, and so, $d(A) \leq d\left(\mathcal{P}(A)\right)$. In order to prove that the inequality is strict assume there exist a surjective function $g: A \rightarrow \mathcal{P}(A)$, and consider the set $B = \{ x \in A : x \notin g(x) \}$. Since $g$ is surjective there must exists an $y \in A$ such that $g(y) = B$. However this rises the contraction $y \in B \Leftrightarrow y \notin g(y) = B$. Consequently, the function $g$ does not exists, and so $d(A) < d\left(\mathcal{P}(A)\right)$.
\end{example}

Not all possible sets are valid sets in the theory of nescience, because some sets lead us to paradoxes. For example, the Russel paradox propose to consider the set $R$ composed by all those sets that are not members of themselves; the paradox arises when we try to answer the question of whether $R$ is a member of itself (see Example \ref{ex:russell_paradox}). In order to avoid these kind of problems, the theory of nescience is based on the Zermelo-Fraenkel set of axioms together with the axiom of choice (see Appendix \ref{apx:math}). Russell's paradox is due to a misuse of the set builder notation $\{ : \}$. The \emph{axiom of separation} (if $P$ is a property with parameter $p$, then for any set $x$ and parameter $p$ there exists a set $y=\{u \in x : P(u) \}$ that contains all those sets $u \in x$ that have property $P$) only allows the use of this notation to construct sets that are subsets of already existing sets. A more general \emph{axiom of comprehension} (if $P$ is a property, the there exist a set $y=\{u : P(u) \}$) would be required to allow sets like the one proposed by Russell's paradox. In the ZFC axioms, and in the theory of nescience, the axiom of comprehension is considered to be false.

\begin{example}[Russell's Paradox]
\label{ex:russell_paradox}
Let $R$ be the set composed by all sets that are not members of themselves, that is, $R = \{ x : x \notin x \}$. The contradiction arises when we ask if $R$ is member of itself. If $R$ is not a member of itself, according to its own definition it should be; however if we say that $R$ is a member of itself, the definition tell us that it should not be. Symbolically $R \in R \Leftrightarrow R \notin R$.
\end{example}

In the theory of nescience we do not deal with classical ontological questions, that is, about the classes of things that there exists in the world and that can be known. Also, we do not try to answer any kind of epistemological issues, like for example, how scientific knowledge is validated by appeal to evidence, or what it is the nature of that evidence.

Once we have selected a set $\mathcal{E}$ of entities, the next step is to uniquely encode their elements as strings of symbols, otherwise it would not be possible to properly describe them (unless entities are strings of symbols themselves). How to properly perform this encoding is described in the next section.

%
% Section: Representations
%

\section{Representations}
\label{sec:representations}

How to represent the, possibly abstract, entities of a set $\mathcal{E}$ is a difficult epistemological problem that has been the subject of research for more than two thousand years (see Section \ref{sec:scientific_representation}). In this section we describe a novel solution to this problem, and we will study its advantages and drawbacks. We propose to split the scientific representation problem it two complementary subproblems: how descriptions characterize representations, and how representations encode entities. In this sense, scientific descriptions are models of entities by virtue of an indirect relation through representations. In this section we will focus on the encoding part, and Section \ref{sec:descriptions_models} will be about descriptions.

In the discipline of Kolmogorov complexity, researchers solve the problem of representation by assuming that the set $\mathcal{E}$ is well defined, countable, and that there exists a total encoding function $f:\mathcal{E} \rightarrow \mathbb{N}$ from the set of entities to the set of natural numbers (a kind of GÃ¶del numbering). In the theory of nescience we borrow this idea of encoding entities with numbers, or equivalently, binary strings. The main difference is that we address the problem of encoding any arbitrary set $\mathcal{E}$ by turning it around, that is, by defining a partial function $f_\mathcal{E}:\mathcal{S}^\ast \rightarrow \mathcal{E}$ from the well defined set $\mathcal{S}^\ast$ of all possible finite strings from an alphabet $\mathcal{S}$ to the, perhaps not countable and even not well defined, set $\mathcal{E}$ of entities under study.

Our function $f_\mathcal{E}$, that depends on the set $\mathcal{E}$, is a kind of oracle (inspired by the concept of oracle Turing machine from Definition \ref{def:Oracle-Turing-Machine}) that is able to fully reconstruct the entity $f_\mathcal{E} (x) \in \mathcal{E}$ given its encoding $x \in \mathcal{S}^\ast$, without requiring any external information. Of course, this oracle is a conceptual idea that has no equivalence in the real world for the majority of the sets $\mathcal{E}$, but assuming its existence will allow us to explain and prove important properties of how the process of scientific discovery works. A set of entities $\mathcal{E}$ might have more than one valid oracle $f_\mathcal{E}$. The seleced oracle depends on the practical application in which the theory of nescience is being used. The only restriction is that everything depends on the selected oracle. Change the oracle means we have to change our representations and adapt our descriptions.

Without any loss of generality, for the rest of this book we will only consider binary strings as encoding of entities.

\begin{definition}
\label{def:descriptions_topic}
Let $\mathcal{E}$ be a set of entities, and let $f_\mathcal{E}:\mathcal{B}^\ast \rightarrow \mathcal{E}$ be a partial function, called \emph{representation function}, from the set $\mathcal{B}^\ast$ of all possible finite binary strings to the set $\mathcal{E}$ of entities under study.
\end{definition}

Only the elements of the set $\mathcal{B}^\ast$, that is, binary strings, can serve as representations (problem of ontology). We do not allow drawings, or any other form of physical models, unless they are converted into binary strings. We do not make any distinction between a scientific representations and any other kind of representations (demarcation problem). It is up to the oracle to decide if a string is a valid representation or not.

\begin{figure}[h]
\centering\includegraphics[scale=0.5]{entities_topics_1}
\caption{\label{fig:entities_topics_1}Encodings and Entities.}
\end{figure}

The elements of $\mathcal{E}$ might be encoded in multiple ways (problem of style). Our oracle $f_\mathcal{E}$ admits all the possible encodings as valid representations for the same entity.

\begin{example}
When the topics under study are animals we could use as encodings a detailed description of the body of those animals. In this case, the oracle would be an hypothetical machine that given its description is able to reproduce the original animal. An alternative encoding we could consider is to use the DNAs of the animals.
\end{example}

\begin{notation}
Let $\mathcal{E}$ be a set of entities, and let $f_\mathcal{E}:\mathcal{B}^\ast \rightarrow \mathcal{E}$ a \emph{representation function}. We denote $\mathcal{R}_\mathcal{E} = dom(f_\mathcal{E}$) the set of valid representations for $\mathcal{E}$, and $\mathcal{R}_e = f_\mathcal{E}^{-1}(e)$ the set of valid representations for $e \in \mathcal{E}$.
\end{notation}

According to the theory of nescience, scientific research would be not only about figuring out how to properly encode entities, but also about discovering the inner workings of decoding oracles. The capacity of oracles of reconstruct the original entities is what justifies the fact that we can formulate hypotheses about entities given their representations and the oracle (surrogative reasoning). The more information about the entities is hard coded into the oracle, the less useful are the representations for research. We are interested in that encoding of entities that makes the size of the oracle minimal.

A consequence of working with finite strings as representations (the set $\mathcal{R}_\mathcal{E}$) is that it might happen that there exist entities that are not encoded by any representation (see the gray areas in Figure \ref{fig:entities_topics_1}, in particular, the entity $e_2$ is not encoded by any representation). Intuitively, we could say that for some domains of knowledge the number of problems is much higher than the number of solutions.

\begin{example}
If the collection of entities under study are real numbers, it turns out that there exist numbers that can not be encoded using finite binary strings, since the set $\mathbb{R}$ has the cardinality of the continuum, and the set $\mathcal{B}^\ast$ is numerable.
\end{example}

We distinguish between \emph{knowable} and \emph{unknowable} entities.

\begin{definition}
Let $\mathcal{E}$ be a set of entities, and let $f_\mathcal{E}:\mathcal{B}^\ast \rightarrow \mathcal{E}$ a \emph{representation function}. We say that an entity $e \in \mathcal{E}$ is \emph{knowable} if there exists an $r \in \mathcal{R}_\mathcal{E}$ such that $f_\mathcal{E}(r) = e$. An entity $e \in \mathcal{E}$ is \emph{unknowable} if it is not knowable.
\end{definition}

We assume that, a priori, it is not possible to determine if a set of entities $\mathcal{E}$ is knowable, unknowable, or partially unknowable, and that selecting the right knowable entities to study is a matter of trial and error. In this book  we have nothing to say about unknowable entities, beyond that the unknowability of an entity cannot be proved nor discovered.

Since, in general, our knowledge about the entities and the inner workings of the oracle are incomplete, in practice we will working with another set $\mathcal{R} \subseteq \mathcal{B}^\ast$ of strings that we believe are close to the valid topics that encode the entities of $\mathcal{E}$.

\begin{definition}[Topic]
Let $\mathcal{T} \subseteq \mathcal{B}^\ast$ be a set of finite binary strings. We call $\mathcal{R}$ a \emph{set of represenations} and \emph{representations} to its elements.
\end{definition}

The elements that belong to $\mathcal{R}$ usually change over time, as we better understand the entities of $\mathcal{E}$, and how the oracle encode these entities as strings. The more abstract is our set of entities $\mathcal{E}$, the more difficult will be to approximate them as strings in $\mathcal{T}$. Sometimes we might even incorrectly think that an element of $\mathcal{R}$ is a valid representation (possibility a wrong one).

\begin{example}
The data used in time of Ptolomeo about the position of celestial bodies along the year was a misencoding of the entity "position of celestial bodies". A better encoding was the data provided by Tycho Brahe. Today, we have an even better encoding.
\end{example}

We must be careful when the approximated encodings are not able to fully capture all the details of the original entities, since the results of our analyses could present some bias. In Chapter \ref{chap:Miscoding} we will study in detail the problem of errors due to the miscoding of abstract entities (standard of accuracy), and we will see that not only topics are about entities, but also we require that entities should be about topics (requirement of directionality).

\begin{figure}[h]
\centering\includegraphics[scale=0.5]{entities_topics_2}
\caption{\label{fig:new_topics}Topics and Entities.}
\end{figure}

A consequence of working with approximations (the set $\mathcal{R}$) instead of valid representations (the set $\mathcal{R}_\mathcal{E}$) is that some of the candidate strings currently in use might not encode any entity at all (targetless models). See representation $r_2$ in Figure \ref{fig:entities_topics} and Example \ref{ex:luminiferous_ether}.

\begin{example}
\label{ex:luminiferous_ether}
The entity "luminiferous ether" was a theoretical postulate about a hypothetical medium in which the light would propagate. The ether was used as an explanation of how a wave-based light could propagate through the empty space. In 1887, the results of the Michelson-Morley experiment suggested that the ether did not exist, and after Einstein formulated his special theory of relativity, that successfully explained how light propagates through empty space, the idea of ether was completely dropped.
\end{example}

Finally, we would like to clarify that our aim encoding entities is different from that of Shannon's information theory, as Example \ref{ex:shannon_encoding} shows.

\begin{example}
\label{ex:shannon_encoding}
Consider a set composed by two books, "The Ingenious Nobleman Sir Quixote of La Mancha" and "The Tragedy of Romeo and Juliet". We could encode the fist book with the string "0" and the second one with the string "1". Although those strings allow us to uniquely identify each book in the set, they are not proper encodings in the sense of the theory of nescience. Information theory is about how to uniquely identify an object given a reference and it requires that both, sender and received agree about a mapping between references and objects. Meanwhile, in the theory of nescience we are interested in how to provide a representation that captures all the details and nuances of the original objects. For example, given the strings "0" and "1" it is not possible to make any hypothesis about the influence of Cervantes in the work of Shakespeare. Technically speaking, the size of the oracle, the mappings between references and objects, required by Shannon information theory is not minimal.
\end{example}

An alternative way to deal with the problem described in Example \ref{ex:shannon_encoding}, that is, making some descriptions artificially short, would be to require the set $\mathcal{E}$ to be infinite, as Kolmogorov complexity does (we can not cheat an infinite number of times). However, even requiring the set of entities to be infinite, we can not guarantee that the highly desirable property of surrogative reasoning is satisfied.

\begin{remark}
One of the problems of science, and in general of any human intellectual activity, is that people tend to confuse symbols with what they represent. The theory of nescience has been carefully designed to avoid this problem, by means of clearly stating the difference between research entities and the representation of entities, called topics. However, keeping this distinction always explicit in the explanations would make the book very difficult to read. We have tried to find a compromise between clarity in the exposition and rigor in the definitions. Sometimes, during the introduction of new ideas we talk about topics when we really mean entities. But, in the mathematical definitions and propositions we always make this difference unequivocal. In case of doubt about what we mean, please take the mathematical definitions as the authoritative reference. 
\end{remark}

%
% Section: Joint Representations
%

\section{Joint Representatinos}
\label{sec:descriptions_joint_topic}

It might happen that there are some entities whose existence we do not know yet. That is, there exists entities in $\mathcal{E}$, encoded as strings in $\mathcal{T}_\mathcal{E}$, that are not approximated by any of the topics of $\mathcal{T}$. The collection of those entities is what we call the \emph{unknown unknown} (see Section \ref{sec:New_Research_Topics}). We are interested in to investigate if there exists a procedure to discover those unknown research entities. A possible approach could be by means of combining the already known topics. In this section we provide the tool required by such procedure.

\begin{definition}
Let $t, s \in \mathcal{T}$ be two different topics. We call the \emph{joint topic} of $t$ and $s$ to the concatenation string $ts$.
\end{definition}

It is highly convenient that $ts \in \mathcal{T}$ for any two topics $t$ and $s$ of $\mathcal{T}$. In this sense, we provide the following extension of the concept of set of topics.

\begin{definition}
\label{def:descriptions_extended_topics}
Let $\mathcal{T}$ be a set of topics. We call the \emph{extended set of topics} of $\mathcal{T}$ to the smallest subset of $\mathcal{B}^\ast$ containing $\mathcal{T}$ that it is closed under the operation of concatenation of topics.
\end{definition}

Given that the set $\mathcal{B}^\ast$ is closed under the operation of concatenation, the extended set of topics of any set $\mathcal{T}$ always exists.

\begin{notation}
Since this point whenever say that $\mathcal{T}$ is a set of topics, we refer to the extended version of that set of topics.
\end{notation}

Note that we do not require the set of valid topics $\mathcal{T}_\mathcal{E}$ to be closed under the operation of concatenation. That is, it might happen that $ts \notin \mathcal{T}_\mathcal{E}$, even if it is the case that $t \in \mathcal{T}_\mathcal{E}$ and $s \in \mathcal{T}_\mathcal{E}$.

\begin{proposition}
Let $\mathcal{T}$ be a set of topics. The set $\mathcal{T}$ together with the operation of concatenation has the structure of free monoid.
\end{proposition}
\begin{proof}
As we have seen in Section \ref{sec:strings} the operation of concatenation is associative, and the empty string $\lambda$ plays the role of neutral element.
\end{proof}

The concept of joint topic can be extended to any arbitrary, but finite, collection of topics. In this way, we could add multiple entities to our research, or to the process of discovering new entities.

\begin{definition}
Let $t_1, t_2, \ldots, t_n \in \mathcal{T}$ be a finite collection of topics. We call the \emph{joint topic} of $t_1, t_2, \ldots, t_n$ to the string $t_1 t_2 \ldots t_n$.
\end{definition}

It is easy to show that $t_1 t_2 \ldots t_n \in \mathcal{T}$ for all $t_1, t_2, \ldots, t_n \in \mathcal{T}$, that is, $\mathcal{T}$ is closed under the operation of concatenation of multiple, finite, topics.

%
% Section: Descriptions
%

\section{Descriptions}
\label{sec:descriptions_models}

Our aim with the set of topics $\mathcal{T}$ is not to find the shortest possible representation of the entities of $\mathcal{E}$, but to provide a complete and detailed representation of each entity, no matter its length. However, as we have said in the preface of this book, human understanding and computer forecasting requires the derivation of concise models of the original entities.

{\color{red} From an otological point of view, models are just strings of symbols that satisfy some requirements, like for example, being computable. In this sense, in the theory of nescience, models are a subset of the mathematical models.}

{\color{red} Models in the theory of nescince does not represent entities (target systems), or they do thrugh topics [...] problem of style [...] }

A model\footnote{Although we are very aware they are not exactly the same thing, in this book we will use the words "model", "description" and "theory" interchangeably.} is a finite binary string mapped to a topic, that is, to the representation of an entity. In this case we require that models must be computable, so we can fully and effectively reconstruct the original topic given its description. The requirement of computability of models allows us to clearly state the limits of the concept of nescience. For example, the problem of self-referential descriptions, like the Berry paradox\index{Berry paradox}\footnote{The Berry paradox proposes to consider "the smallest positive integer not describable in fewer than twelve words". The paradox arises because we have just described this number with only eleven words.}, can be addressed in the scope of the limits of computation.

\begin{definition} [Model]
\label{def:descriptions_model}
Let $m \in \mathcal{B}^\ast$ be a binary string in the form $m = \langle TM,a \rangle$, where $TM$ is the encoding of a prefix free Turing machine and $a$ is the input string to that machine. If $TM(a)$ is defined, we call $m$ a \emph{model}\index{Model}. Let $t \in \mathcal{T}$ be a topic, if $TM(a) = t$ we say that $m$ is a \emph{valid model of the topic}\index{Model of a topic} $t$.
\end{definition}

\begin{notation}
In case of dealing with multiple topics and models, and in order to avoid ambiguity, we could denote by $m_t$ a model for the topic $t$.
\end{notation}

Intuitively, a model is composed by two parts, a Turing machine that should compress all the regularities found in the topic, and a string containing a literal description of what is left, that is, the non-compressible part.

Note that not all possible strings are models, and that not all possible models describe topics.

\begin{definition}
\label{def:descriptions_model}
We define the \emph{set of models}\index{Set of valid models}\index{Valid models}, denoted by $\mathcal{M}$, as:
\[
\mathcal{M} = \{ m \in \mathcal{B}^\ast : m = \langle TM,a \rangle \wedge TM(a) \downarrow \}.
\]
Let $\mathcal{T}$ be a set of topics. We define the set of all \emph{valid models of $\mathcal{T}$}, denoted by $\mathcal{M}_{\mathcal{T}}$, as:
\[
\mathcal{M}_{\mathcal{T}} = \{ m \in \mathcal{M} : \exists t \in \mathcal{T},\, TM(a) = t \}.
\]
Finally, given a topic $t \in \mathcal{T}$, we define the set of \emph{valid models} of $t$, denoted by $\mathcal{M}_t$, as:
\[
\mathcal{M}_t = \{ m \in \mathcal{M}_{\mathcal{T}} : TM(a) = t \}.
\]
\end{definition}

\begin{example}
\label{ex:topics_models_graph}
Consider the set of all possible finite graphs\index{Graph}. Since graphs are abstract mathematical objects, we need a way to represent them as strings, for example, by using a binary encoding of their adjacency matrices (see Section \ref{sec:Graphs} for an introduction to graphs). The model $m = \langle TM, t \rangle$, where $t$ is the representation of a graph and $TM$ is a Turing machine that just halts, will be part of $\mathcal{M}_t$ since $TM(t) = t$. On the contrary of what happened in Section \ref{sec:descriptions_topics}, in this case we are concerned about the fact that this model may not be shortest possible description of $t$.
\end{example}

As Example \ref{ex:topics_models_graph} pointed it out, in general there is little value on models that are longer than the topics they describe.

\begin{definition}
\label{def:trivial_model}
Let $t \in \mathcal{T}$ be a topic, and $m \in \mathcal{M}_t$ one of its models. If $l(m) \geq l(t)$, we say that $m$ is a \emph{trivial model}\index{Trivial model} of the topic $t$.
\end{definition}

We could have considered that a model $m$ for a topic $t$ such that $l(m) = l(t)$ is a non-trivial model, since it might happen that there is no shortest possible description of that topic than the topic itself, for example when the topic is a random string. However, it does not make any sense to do research about random topics, since it is not possible to find short models, and so, we can not apply the results of our theory to them. This is why those models are considered trivial. In the same line, we could ask what happens if we replace each element of $\mathcal{T}$ by its shortest possible model (or the shortest known model) since they describe the same entity. The answer is that we cannot do that, since in general, this is no the expected behavior of the oracle $f_\mathcal{E}$.

Since each valid model describes one, and only one, topic, we can define a function that maps models into topics. Given that models are Turing machines, it is natural to use as description function a universal Turing machine. Consequently, not only the individual models of topics are computable, but also it is computable the function that maps models into topics.

\begin{definition}
We call \emph{description function}\index{Description function}, denoted by $\delta$, to any universal Turing machine $\delta : \mathcal{M} \rightarrow \mathcal{T}$ that maps models to their corresponding topics.
\end{definition}

If $m$ is a model of topic $t$, then we have that $\delta \left( m \right) = \delta \left( \langle TM,a\rangle \right) = TM(a) = t$.

Inspired by the Occam's razor principle\index{Occam's razor principle}\footnote{The Occam's razor principle refers to the number of assumptions of an explanation, not to the length of the explanation itself.}, if two explanations are indifferent, we should prefer the shortest one. Therefore, the limit of what can be known (understand) about a topic, that is, its perfect model, is given by the shortest model that allows us to reconstruct the topic.

\begin{definition}
\label{def:descriptions_perfect_model}
Given the set of models $\mathcal{M}_t$ of a topic $t \in \mathcal{T}$, let $m^{\star} \in \mathcal{M}_t$ be the shortest possible model of $t$ using the standard shortlex ordering. We call $m^{\star}$ the \emph{perfect model}\index{Perfect model} of topic $t$.
\end{definition}

Unfortunately, the perfect model of a topic is in general not known and, as Proposition \ref{prop:nescience-kolmogorov} shows, there exist no algorithm to compute it. In practice what we have to do is to use an approximation to estimate how far our current best model is from the perfect model, that is, how much we do not know about a particular topic.

\begin{proposition}
\label{prop:nescience-kolmogorov}
Given a topic $t \in \mathcal{T}$, we have that $l \left(m^{\star} \right) = K\left( t \right)$.
\end{proposition}
\begin{proof}
Apply Definition \ref{def:Kolmogorov-Complexity} and the fact that we require that the Turing machines $TM$ used in definitions $\langle TM,a\rangle$ must be prefix-free.
\end{proof}

The length of a model $l \left( m \right)$ for a topic $t$ is something that depends on how we encode Turing machines. The encoding method is given by the description function $\delta$ used. Fortunately, if we replace our description function by another one, the length of perfect models do not change (up to a constant that does not depend on the topics themselves).

\begin{corollary}
Let $t \in \mathcal{T}$ be a model, $\delta$ and $\dot{\delta}$ two different description functions, and $m^{\star}$ the perfect model of $t$ using $\delta$ and $\dot{m}^{\star}$ the perfect model using $\dot{\delta}$, then we have that $l \left( m^{\star} \right) \leq l \left( \dot{m}^{\star} \right) + c$ where $c$ is a constant that does not depend on $t$.
\end{corollary}
\begin{proof}
Apply Theorem \ref{prop:nescience-kolmogorov} and Theorem \ref{def:Invariance-theorem}.
\end{proof}

For the rest of this book we will assume that $\delta$ is fixed to a reference universal Turing machine. Alternatively, the reader could consider that all the theorems provided in this book that deal with the length of shortest models are valid up to a constant that does not depend on the topics themselves.

A remarkable consequence of Proposition \ref{prop:nescience-kolmogorov} is that perfect models must be incompressible, that is, \emph{perfect knowledge implies randomness} (see Section \ref{sec:incompressibility_randomness}). The converse, in general, does not hold, since we could have a random model that it is not the shortest possible one, that is, a model $m$ such that $l(m) = K(m)$ but $l(m^{\star}) < l(m)$.

\begin{example}
\label{ex:description_neural}
We could define a deep neural network\index{Neural network} with an input layer of one thousands nodes, ten hidden layers of fifty thousands nodes each, and an output layer of one thousand nodes, and then train the network to output a fixed string of one thousand 1's for any given input string. The Kolmogorov complexity of this neural network is much higher that the complexity of a string of one thousand 1's.
\end{example}

%
% Section: Models for Joint Topics
%

\section{Models for Joint Topics}
\label{sec:descriptions_joint_model}

In Section \ref{sec:descriptions_joint_topic} we introduced the concept of joint topic $ts$ of two topics $t$ and $s$. In this section we are interested in to study how the length of the optimal model of a joint topic relates to the length of the optimal models of the individual topics.

The length of the perfect model of a joint topic is greater or equal than the length of the perfect model of any of the individual topics. Intuitively, the more topics we add to our research, the longer will take to describe them.

\begin{proposition}
\label{prop:joint_length}
Given any two topics $t,s \in \mathcal{T}$, we have that $l \left( m_{ts}^{\star} \right) \geq l \left( m_{t}^{\star} \right)$ and $l \left( m_{ts}^{\star} \right) \geq l \left( m_{s}^{\star} \right)$.
\end{proposition}
\begin{proof}
The statement $l \left( m_{ts}^{\star} \right) \geq l \left( m_{t}^{\star} \right)$ is equivalent to $K(t,s) \geq K(t)$. Then apply Proposition \ref{prop:excess_kolmogorov}.
\end{proof}

If the selected two models partially overlap, we could take advantage of this fact to come up with a shorter description. In the worst case, the perfect model of a joint topic would be just the concatenation of the perfect models of the involved topics.

\begin{proposition}
\label{prop:joint_sum}
Given any two topics $t,s \in \mathcal{T}$, we have that $l \left( m_{ts}^{\star} \right) \leq l \left( m_{t}^{\star} \right) + l \left( m_{s}^{\star} \right)$.
\end{proposition}
\begin{proof}
The statement $l \left( m_{ts}^{\star} \right) \leq l \left( m_{t}^{\star} \right) + l \left( m_{s}^{\star} \right)$ is equivalent to $K(t,s) \leq K(t) + K(s)$, then apply Proposition \ref{prop:additive_kolmogorov}.
\end{proof}

Finally, next proposition proves that the order of the topics in the perfect model of a joint topic does not change its length.

\begin{proposition}
\label{prop:joint_order}
Given any two topics $t,s \in \mathcal{T}$, we have that $l \left( m_{ts}^{\star} \right) = l \left( m_{st}^{\star} \right) + c$.
\end{proposition}
\begin{proof}
The statement $l \left( m_{ts}^{\star} \right) = l \left( m_{st}^{\star} \right) + c$ is equivalent to $K(t,s) = K(s,t) + c$, then apply Proposition \ref{prop:kolmogorov_order}.
\end{proof}

Propositions \ref{prop:joint_length}, \ref{prop:joint_sum} and \ref{prop:joint_order} can be generalized to any arbitrary, but finite, collection of topics $t_1, t_2, \ldots, t_n$.

\begin{proposition}
\label{prop:joint_multiple_topics}
Let $t_1, t_2, \ldots, t_n \in \mathcal{T}$ be a finite collection of topics. Then, we have that:

\renewcommand{\theenumi}{\roman{enumi}}
\begin{enumerate}
\item $l(m_{t_1 t_2 \ldots t_n}^\star) \geq l(m_ {t_i}^\star) \; \forall \, 0 \leq i \leq n$,
\item $l(m_{t_1 t_2 \ldots t_n}^\star) \leq l(m_ {t_1}^\star) + l(m_ {t_2}^\star) + \ldots + l(m_ {t_n}^\star)$,
\item $l(m_{t_1 \ldots t_i \ldots t_j \ldots t_n}^\star) = l(m_{t_1 \ldots t_j \ldots t_i \ldots t_n}^\star) + c \; \forall \, 0 \leq i \leq j \leq n$,
\item $l(m_{t_1 \ldots t_{n-1}}^\star) \leq l(m_{t_1 \ldots t_{n-1} t_n}^\star)$.
\end{enumerate}
\end{proposition}
\begin{proof}
Apply Propositions \ref{prop:joint_length}, \ref{prop:joint_sum} and \ref{prop:joint_order} to individual pairs of topics $i$ and $j$.
\end{proof}

%
% Section: Conditional Models
%

\section{Conditional Models}

Sometimes it is cumbersome to include all the information needed to reconstruct a topic in its model, since that would require very large strings. In those cases, it is usually more convenient to assume some already existing background knowledge, and compute how much we do not know about a topic given that background, what we call conditional models.

\begin{definition}
Let $t,s \in \mathcal{T}$ be two different topics. We say that the string $\langle TM,a \rangle$ is a \emph{valid conditional model}\index{Conditional model} of the topic $t$ given a perfect model of $s$, or a conditional model of $t$ given $s$ for short, denoted by $m_{t \mid s^\star}$, if $TM \left(\langle m_s^\star, a \rangle \right) = t$.
\end{definition}

From a practical point of view, perhaps it would have been a better idea to define the conditional model of a topic $t$ given any model of topic $s$ instead of its perfect model, that is, by using $TM \left( \langle m_s, a \rangle \right)$ instead of $TM \left( \langle m_s^\star, a \rangle \right)$. However, it is very difficult to say anything useful about how much we do not know about topic $t$ assuming an incomplete background knowledge of $s$.

\begin{example}
For each topic $t \in \mathcal{T}$ there always exists a conditional model $m_{t \mid s^\star}$ that describes $t$ independently of the topic $s \in \mathcal{T}$. For example, we can use a Turing machine that given the input $\langle m_s^\star, a \rangle$ safely ignores the $m_s^\star$ substring.
\end{example}

Note that $m_{t \mid s^\star}$ does not belong to $\mathcal{M}_t$, since $m_s^\star$ is required to compute the topic $t$, but it is not part of the conditional model itself. A new definition is required to capture this concept.

\begin{definition}
Given topics $t,s \in \mathcal{T}$, we define the \emph{set of conditional models}\index{Set of conditional models} of $t$ given the perfect model of $s$, denoted by $\mathcal{M}_{t \mid s^\star}$, as:
\[
\mathcal{M}_{t \mid s^\star} = \{ m \in \mathcal{M}_{\mathcal{T}} : TM \left(\langle m_s^\star, a \rangle \right) = t \}.
\]
\end{definition}

Again, we are interested in the concept of perfect conditional model. The perfect conditional model of a topic given the perfect model of a second topic is the shortest possible string that allow us to fully reconstruct the topic given the perfect model of the second one.

\begin{definition}
Let $t,s \in \mathcal{T}$ be two topics, and let $m_{t \mid s^\star}^{\star} \in \mathcal{M}_{t \mid s^\star}$ be the shortest possible conditional model of $t$ given a perfect model of $s$ using the standard shortlex ordering. We call $m_{t \mid s^\star}^{\star}$ the \emph{perfect conditional model}\index{Perfect conditional model} of topic $t$ given a perfect model of topic $s$, or perfect conditional model of $t$ given $s$ for short.
\end{definition}

The length of perfect conditional model is equal or shorter that their unconditional counterparts. That is, assuming some already existing background knowledge could reduce the effort required to describe a topic.

\begin{proposition}
\label{prop:description_conditional_inequality}
For any topic $t \in \mathcal{T}$ we have that $l \left( m_{t \mid s^\star}^{\star} \right) \leq l \left(m^\star_t\right)$ for all $s \in \mathcal{T}$.
\end{proposition}
\begin{proof}
The statement $l \left( m_{t \mid s^\star}^{\star} \right) \leq l \left(m^\star_t\right)$ is equivalent to $K(t \mid s^\star) \leq K(t)$, then apply Proposition \ref{prop:kolmogorov_conditional}.
\end{proof}

Next proposition shows the relation between the lengths of models, join models and conditional models.

\begin{proposition}
\label{prop:description_conditional_joint}
Let $t, s \in \mathcal{T}$ two different topics. We have that:
\[
l \left( m_{t \mid s^\star}^{\star} \right) \leq l \left( m_t^\star \right) \leq l \left( m_{ts}^\star \right)
\]
\end{proposition}
\begin{proof}
The statement $l \left( m_{t \mid s^\star}^{\star} \right) \leq l \left( m_t^\star \right) \leq l \left( m_{ts}^\star \right)$ is equivalent to $K(t | s^\star ) \leq K(t)$ and $K(t) \leq K(t, s)$, then apply Proposition \ref{prop:kolmogorov_relations}.
\end{proof}

As it was the case of joint models, the concept of conditional model can be extended to finite collections of topics.

\begin{definition}
Let $t, s_1, s_2, \ldots, s_n \in T$ be a finite collection of topics. We say that the string $\langle TM,a \rangle$ is a \emph{valid conditional model} of topic $t$ given the perfect models of topics $s_1, s_2, \ldots, s_n$, or conditional model of $t$ given $s_1, s_2, \ldots, s_n$ for short, and denoted by $m_{t \mid s_1^\star, s_2^\star, \ldots, s_n^\star}$, if
\[
TM \left(\langle m_{s_1}^\star, m_{s_2}^\star, \ldots, m_{s_n}^\star, a \rangle \right) = t
\]
\end{definition}

In the next definition we provide the generalization of the concept of perfect conditional models.

\begin{definition}
Let $\mathcal{M}_{t \mid s_1^\star, s_2^\star, \ldots, s_n^\star}$ be the set of conditional models of topic $t$ given the perfect models of topics $s_1, s_2, \ldots, s_n$, and let $m_{t \mid s_1^\star, s_2^\star, \ldots, s_n^\star}^\star \in \mathcal{M}_{t \mid s_1^\star, s_2^\star, \ldots, s_n^\star}$ be its shortest possible conditional model using the standard shortlex ordering. We call $m_{t \mid s_1^\star, s_2^\star, \ldots, s_n^\star}^\star$ the \emph{perfect conditional model} of topic $t$ given the perfect models of topics $s_1, s_2, \ldots, s_n$, or the perfect model of $t$ given $s_1, s_2, \ldots, s_n$ for short.
\end{definition}

Next proposition generalizes Propositions \ref{prop:description_conditional_inequality} and \ref{prop:description_conditional_joint} to any arbitrary, but finite, collection of topics $s_1, s_2, \ldots, s_n$. Moreover, the proposition shows that the more background knowledge we assume for a research topic, the shorter is the perfect model for that topic.

\begin{proposition}
\label{prop:joint_multiple_topics}
Let $t, s_1, s_2, \ldots, s_n, s_{n+1} \in T$ be a finite collection of topics. Then, we have that:

\renewcommand{\theenumi}{\roman{enumi}}
\begin{enumerate}
\item $l \left( m_{t \mid s_1^\star, s_2^\star, \ldots, s_n^\star}^\star \right) \leq l \left( m^\star_t \right)$,
\item $l \left( m_{t \mid s_1^\star, s_2^\star, \ldots, s_n^\star}^\star \right) \leq m_t^\star \leq l(m_{t_1 s_1 \ldots s_n}^\star)$,
\item $l \left( m_{t \mid s_1^\star, s_2^\star, \ldots, s_n^\star}^\star \right) \geq l \left( m_{t \mid s_1^\star, s_2^\star, \ldots, s_n^\star, s_{n+1}^\star}^\star \right)$.
\end{enumerate}
\end{proposition}
\begin{proof}
Apply Propositions \ref{prop:description_conditional_inequality} and \ref{prop:description_conditional_joint} to individual pairs of topics $i$ and $j$.
\end{proof}


%
% Section: Areas
%

\section{Research Areas}
\label{sec:areas}

Topics can be grouped into research areas. The concept of area is useful as long as all the topics included in the area share a common property. The particular details of grouping properties depend on the practical applications in which the theory of nescience is being used.

\begin{definition}
Given a set of topics $\mathcal{T}$, we define a \emph{research area}\index{Research area} $A$ as a subset of topics $A \subset \mathcal{T}$.
\end{definition}

If we want to know how much we do not know about a research area, first we have to provide a model for that area. Unfortunately, in general, areas are infinite, but our knowledge is finite, and so, we can only partially describe them.

\begin{definition}
Let $A \subset \mathcal{T}$ be an area. We define the \emph{known subset of the area}\index{Known subset of an area} $A$, denoted by $\hat{A}$, as the set composed by those topics $t_1, t_2, \ldots, t_n \in A$ for which at least one non-trivial model is known.
\end{definition}

As our understanding of a research area changes, the number of topics included in its known subset changes as well. The properties of areas studied in this book will be always relative to our current knowledge.

\begin{definition}
Let $A \subset \mathcal{T}$ be an area with known subset $\hat{A}$. We call a \emph{model of the area $A$} given the known subset $\hat{A}$, abbreviated as \emph{model of $A$}, and denoted by $m_{\hat{A}}$, to any string in the form $\langle TM, a\rangle$ such that $TM(a) = \langle t_1, t_2, \ldots, t_n\rangle$, where $\{t_1, t_2, \ldots, t_n\} = \hat{A}$.
\end{definition}

Since our definition of model requires that the Turing machine halts at some point, we cannot provide models for infinite areas. In case of an infinite area we will always refer to its finite known subset.

\begin{definition}
Given the set of topics $\mathcal{T}$ and an area $A \subset \mathcal{T}$, we define the set of \emph{valid models of the area $A$} given the known subset $\hat{A}$\index{Set of models of an area}, denoted by $\mathcal{M}_{\hat{A}}$, as:
\[
\mathcal{M}_{\hat{A}} = \{ m \in \mathcal{M} : TM \left(\langle m, a \rangle \right) = \hat{A} \}.
\]
\end{definition}

Finally, we are interested in the perfect model for a research area, that is, the shortest possible model that fully describes its known subset. According to Definition \ref{def:trivial_model}, if we are aware of the existence of a topic $t \in A$, that topic should be part of $\hat{A}$, even in the case we have not started yet to do research about that particular topic.

\begin{definition}
Let $A \subset \mathcal{T}$ be an area with known subset $\hat{A}$, and let $m_{\hat{A}}^{\star} \in \mathcal{M}_{\hat{A}}$ be the shortest possible model of $A$ using the standard shortlex ordering. We call  $m_{\hat{A}}^{\star}$ the \emph{perfect model of the area $A$} given the known subset $\hat{A}$\index{Perfect model of an area}, abbreviated as \emph{perfect model of $A$}.
\end{definition}

Next proposition shows the relation between the model of an area, and the models of the topics that compose the known subset of that area. In general, the models for an area are different from the collection of models of the individual topics.

\begin{proposition}
Let $A \subset \mathcal{T}$ be an area with known subset $\hat{A} = \{t_1, t_2, \ldots, t_n\}$, then we have that $l \left( m_{\hat{A}}^{\star} \right) \leq l(m_ {t_1}^\star) + l(m_ {t_2}^\star) + \ldots + l(m_ {t_n}^\star)$.
\end{proposition}
\begin{proof}
Apply Proposition \ref{prop:joint_multiple_topics}-ii. 
\end{proof}

Also, as it was proved in Proposition \ref{prop:joint_multiple_topics}, the order in which the topics are listed in the description of an area is not relevant when dealing with the perfect model for that area.

Areas can overlap, that is, given two areas $A$ and $B$ it might happen that $A \cap B \neq \varnothing$. Moreover, areas can be subsets of other areas, creating an hierarchy of areas. We are interested in the length of perfect models of areas in relation to the length of perfect models of other areas.

\begin{proposition}
Let $A, B \subset \mathcal{T}$ be two areas such that $A \subset B$, and let $\hat{A}$ and $\hat{B}$ be their know subsets respectively, then we have that $l \left( m_{\hat{A}}^{\star} \right) \leq l \left( m_{\hat{B}}^{\star} \right)$.
\end{proposition}
\begin{proof}
If $t \in \hat{A}$ there exists a non-trivial model $m_t$ for $t$, and given that $A \subset B$, $m_t$ is also a non-trival model for $B$, and so $t \in \hat{B}$. Finally, we apply \ref{prop:joint_multiple_topics}-iv.
\end{proof}

Next proposition \label{prop:areas_union} shows how the length of the shortest possible model of areas relate to the union and intersection of such areas.

\begin{proposition}
\label{prop:areas_union}
Let $A, B \subset \mathcal{T}$ be two areas with know subsets $\hat{A}$ and $\hat{B}$ respectively, then we have that $l \left( m_{\hat{A} \cup \hat{B}}^{\star} \right) = l \left( m_{\hat{A}}^{\star} \right) + l \left( m_{\hat{B}}^{\star} \right) - l \left( m_{\hat{A} \cap \hat{B}}^{\star} \right)$.
\end{proposition}
\begin{proof}
{\color{red} TODO}
\end{proof}

A consequence of Proposition \label{prop:areas_union} is that $l \left( m_{\hat{A} \cup \hat{B}}^{\star} \right) \leq l \left( m_{\hat{A}}^{\star} \right) + l \left( m_{\hat{B}}^{\star} \right)$, that is, when we combines two different research areas, how much we do not know about these areas decreases.

In the same way we introduced a chain rule for entropy in Proposition \ref{prop:chain_rule_entropy}, we can provide a chain rule for the shortest length for a model of a research area.

\begin{proposition}
Let $A, B \subset \mathcal{T}$ be two areas with know subsets $\hat{A}$ and $\hat{B}$, then we have that $l \left( m_{\hat{A} \cup \hat{B}}^{\star} \right) = l \left( m_{\hat{A}}^{\star} \right) + l \left( m_{\hat{B} \backslash \hat{A}}^{\star} \right)$.
\end{proposition}
\begin{proof}
{\color{red} TODO}
\end{proof}

%
% Section: References
%

\section*{References}

For more information about Russell's paradox, Cantor theorem and universal sets refer, for example, to \cite{jech2013set}. The idea of using a function to assigns to each symbol and well-formed formula of some formal language a unique natural number (GÃ¶del number) was introduced by Kurt GÃ¶del for the proof of his incompleteness theorems \cite{godel1931formal}. A detailed description of the Berry paradox from the point of view of computability can be found at \cite{chaitin1995berry}. For a detailed account of the implications of Kolmogorov complexity being true up to a constant, please refer to \cite{li2013introduction}. That oracle machines are not mechanical was stated by Turing when he introduced the concept of oracle machine in \cite{turing1939systems}.

%
% Section: TO DO List
%

\section*{TO DO List}

\begin{itemize}

\item The concept of research entity should have a clear interpretation in the context of Proof Theory, Category Theory and Type Theory.

\end{itemize}


