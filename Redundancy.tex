%
% CHAPTER: Surfeit
%

\chapterimage{Alexander_cuts_the_Gordian_Knot.pdf} % Chapter heading image

\chapter{Surfeit}
\label{chap:Redundancy}

\begin{quote}
\begin{flushright}
\emph{If you can't explain it simply,\\
you don't understand it well enough.}\\
Albert Einstein 
\end{flushright}
\end{quote}
\bigskip

Surfeit is a quantity that measures how redundant is the model we are using to understand a research topic. Intuitively, the more ignorant we are about the topic, the longer will be our current best model. Long models usually contain elements that are wrong or that are not needed, and a better understanding of the topic should allow us to remove all those unnecessary symbols. 

We define the surfeit of a topic's model as the difference between the length of this model and the length of the best possible model for that topic. In the theory of nescience we assume that the theoretical limit of what can be known about a topic, that is, its perfect model, is the shortest possible description that allows us to fully reconstruct the topic (a representation of the entity). Of course, perfection is conditional on the representation being valid and the model accurate.

The length of the shortest possible model of a topic is given by the Kolmogorov complexity of that topic. As we have seen, this quantity is not computable for the general case. Moreover, in practice, and given that our knowledge about entities is in general incomplete, we do not know the shortest possible model either. Surfeit is a quantity that has to be approximated in practice.

If we were able to come up with a perfect description of an entity, that description must be a random string, otherwise it would contain redundant elements that can be removed. According to the theory of nescience, perfect knowledge implies randomness. Randomness imposes a limit on how much we can know about a particular research topic. Far from being a handicap, the proper understanding of this limitation opens new opportunities in science and technology. For example, by means of computing how far our current model is from being a random string we can estimate how far we are from having a perfect model.

In this chapter we will introduce formally the concept of surfeit, and study its properties. We will see ...

%
% Section: Surfeit
%

\section{Surfeit}
\label{sec:Definition_redundancy}

Given the length of a description of a representation for an entity, and the length of its shortest possible description, we can introduce a relative measure of how much unneeded effort we are using to explain the entity when using that description. We call this quantity \emph{surfeit}, and it will be part of our definition of nescience, that is, how much we do not know about that research entity.

\begin{definition}[Surfeit]
Given a representation $r \in \mathcal{B}^\ast$, and $d \in \mathcal{D}$ a non-pleonastic description for $r$, we define the \emph{surfeit of the description $d$ for the representation $r$}, denoted by $\sigma(d, r)$, as
\[
\sigma (d, r) = 1 - \frac{K(r)}{l(d)}
\]
\end{definition}

In our definition of surfeit we have used a relative measure instead of an absolute one (i.e., $l(d) - K(r)$), because beside to compare the surfeit of different models for the same entity, we are also interested in to compare the surfeit of different entities. We prefer to use $K(r)$ instead of the equivalent $l \left( r^\star \right)$ in order to be consistent with the definition of inaccuracy provided in Section \ref{sec:inaccuracy:inaccuracy}.

Intuitively, the more ignorant we are about an entity, the longer will be our current best known description\footnote{The concept of \emph{current best description} will be introduced formally at Chapter \ref{chap:Nescience}.}, since a better understanding of that entity means that we should be able to remove all the redundant elements from that description. Of course, the quality of a description is conditional to the quality of the representation in use.

\begin{example}
{\color{red} TODO: Provide an example to clarify the concept.}
\end{example}

The surfeit of a description is a number between $0$ and $1$.

\begin{proposition}
\label{prop:range_redundancy}
Let $r \in \mathcal{B}^\ast$ be a representation , and $d \in \mathcal{D}^\star_r$ one of its valid descriptions, then we have that $0 \leq \sigma(d, r) \leq 1$.
\end{proposition}
\begin{proof}
Given that $l\left( d \right)>0$ and that $K\left( r \right)>0$, since they are the lengths of non-empty strings, and that $l\left( d \right) \geq K\left( r \right)$ since we do not consider pleonastic descriptions.
\end{proof}

{\color{red} Explain when it is the case that surfait is zero, and that there could be more than one description that makes surfeit zero for the same representation.}

Our definition of surfeit compares the length of a description with the Kolmogorov complexity of the representation, not with the Kolmogorov complexity of the description itself (i.e., $K\left( d \right)$). That is, surfeit is not a measure of the redundancy of a description. It might happen that we come up with an incompressible description (no redundant elements to remove), that it is not the shortest possible one that describes the representation (see Example \ref{ex:description_neural}). Such a description would not be redundant in the traditional sense, but it still will present some surfeit in the sense of the theory of nescience. Moreover,it migth happen that the description $d$ we are considering does not describe the representation $r$, that is, $d \in \mathcal{D}_r$. In practice it is highly convenient to introduce the following alternative (we could say weaker) characterization of the concept of redundant model:

\begin{definition}[Redundancy]
Given a description $d \in \mathcal{D}$, we define the \emph{redundancy} of the description $d$, denoted by $\rho(d)$, as
\[
\rho(d) = 1 - \frac{K(d)}{l(d)}
\]
\end{definition}

The redundancy of a description $d$ is a quantity related to the description itself, and it does not depend on the representation $r$ being described.

\begin{example}
{\color{red} TODO: Provide an example to clarify the concept.}
\end{example}

We have that the redundancy of a description is  a number between $0$ and $1$.

\begin{proposition}
We have that $0 \leq \rho(d) \leq 1$ for all $d \in \mathcal{D}$.
\end{proposition}
\begin{proof}
Apply Proposition \ref{prop:kolmogorov_length}.
\end{proof}

{\color{red} Clarify when the redundancy is zero}

Finally, next proposition formalizes our intuition that the surfeit of a description is greater or equal than its redundancy.

\begin{proposition}
\label{prop:surfeit_comparison}
Let $r \in \mathcal{B}^\ast$ be a representation , and $d \in \mathcal{D}^\star_r$ one of its valid descriptions, then we have that $\rho(d) \leq \sigma(d, r)$.
\end{proposition}
\begin{proof}
Proving that $\rho(d) \leq \sigma(d, r)$ is equivalent to prove that $K(d) \geq K(r)$ for all $d$. Lets assume that there exist a $d$ such that $K(d) < K(r)$, that would mean there exists a Turing machine $\langle TM, a \rangle$ such that $TM(a)=r$ but $l(<TM, a>) < K(r)$. That is a contradiction with the fact that $K(r)$ is the length of the shortest possible Turing machine that prints $r$.
\end{proof}

It would be very nice if Proposition \ref{prop:surfeit_comparison} applies to all possible description. Unfortunately, the proposition is true only when we deal with valid descriptions (from $\mathcal{D}^\star_r$), as Example \ref{ex:surfeit_non_comparison} shows.

\begin{example}
\label{ex:surfeit_non_comparison}
{\color{red} TODO: Provide an example.}
\end{example}


%
% Section: Conditional Surfeit
%

\section{Conditional Surfeit}

We are interested into study how the surfeit of a description for a representation is affected when some background knowledge is assumed. That is, we want to know the surfeit of a conditional description for a representation, what we call \emph{conditional surfeit}.

\begin{definition}
Let $r \in \mathcal{B}^\ast$ be a representation, $s \in \mathcal{B}^\ast$ be a string, and $d \in \mathcal{D}$ be a description of $r$ given $s$. We define the \emph{conditional surfeit} of the conditional description $d_{r \mid s}$, denoted by $\sigma(d_{r \mid s})$, as: 
\[
\sigma(d_{r \mid s}) = 1 - \frac{K\left( r \mid s \right)}{l \left( d_{r \mid s} \right)}
\]
\end{definition}

This definition is required mostly for practical purposes, since given that our knowledge of $s$ is perfect, we can focus on studying what it is new in topic $t$, that is, in that part not covered by the assumed (perfect) background knowledge.

Conditional surfeit, being a relative measure, is a number between $0$ and $1$.

\begin{proposition}
Let $r \in \mathcal{B}^\ast$ be a representation, $s \in \mathcal{B}^\ast$ be a string, and $d \in \mathcal{D}$ be a description of $r$ given $s$. We have that $0 \leq \sigma(d_{r \mid s}) \leq 1$.
\end{proposition}
\begin{proof}
Given that $l \left( d_{r \mid s} \right) > 0$ and that $K\left( r \mid s \right) > 0$, since they are the lengths of non-empty strings, {\color{red} and that $l\left( d \right) \geq K\left( r \right)$ since we do not consider pleonastic descriptions}.

\end{proof}

Intuition tell us that the surfeit of a description could only decrease if we assume the background knowledge given by the description of another topic. This is because we require that this background knowledge must be a perfect description (it presents no surfeit). However, as it was the case of joint surfeit, we have to wait until Chapter \ref{chap:Nescience} to formalize this intuition.

In the same way we introduced the concept of redundancy of a description as a weaker version of the concept of surfeit, we can also introduce the concept of conditional redundancy as a weaker version of the concept of conditional surfeit.

\begin{definition}
Let $s \in \mathcal{B}^\ast$ be a string, and $d \in \mathcal{D}$ be a conditional description given $s$. We define the \emph{conditional surfeit} of the conditional description $d_{t \mid s^\star}$, denoted by $\sigma(d_{t \mid s^\star})$, as: 
\[
\rho(d_{r \mid s}) = 1 - \frac{K \left( d_{t \mid s^\star} \right)}{l \left( d_{t \mid s^\star} \right)}
\]
\end{definition}

Conditional surfeit is a relative measure, and so, a number between $0$ and $1$.

\begin{proposition}
We have that $0 \leq \rho(d_{t \mid s^\star}) \leq 1$ for all $t,s$ and all $d_{t,s}$.
\end{proposition}
\begin{proof}
Given that $K(d_{t \mid s^\star}) \leq l(d_{t \mid s^\star})$ we have that $\frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} \leq 1$ and so, $1 - \frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} \geq 0$. Also, since $\frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} > 0$ (both quantities are positive integers), we have that $1 - \frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} \leq 1$.
\end{proof}

Finally, we can extend our concepts of conditional surfeit and conditional redundancy to multiple, but fine, number of topics.

\begin{definition}
Let $t, s_1, s_2, \ldots, s_n \in \mathcal{T}$ be a finite collection of topics, and let $d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star}$ any conditional description of $t$ given $s_1, s_2, \ldots, s_n$. We define the \emph{conditional surfeit} of the description $d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star}$, denoted by $\sigma(d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star})$, as: 
\[
\sigma(d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star}) = 1 - \frac{K\left( t \mid s_1^\star, s_2^\star, \ldots,s_n^\star \right)}{l \left( d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star} \right)}
\]
And the \emph{conditional redundancy} of the description $d_{t_1, t_2, \ldots, t_n}$, denoted by $\rho(d_{t_1, t_2, \ldots, t_n})$, as:
\[
\rho(d_{t_1, t_2, \ldots, t_n}) = 1 - \frac{K(d_{t_1, t_2, \ldots, t_n})}{l \left( d_{t_1, t_2, \ldots, t_n} \right)}
\]
\end{definition}

It is easy to show that the properties of conditional surfeit and conditional redundancy apply to the case of multiple topics as well.

%
% Section: Surfeit of Areas
%

\section{Surfeit of Areas}

{\color{red} Review this section}

The concept of surfeit can be extended to research areas, to quantitative measure the amount of extra effort we are using to describe the topics of the area.

\begin{definition}
Let $A \subset \mathcal{T}$ be an area with known subset $\hat{A} = \{t_1, t_2, \ldots, t_n\}$, and let $d_{\hat{A}}$ be a description. We define the \emph{surfeit of the description} $d_{\hat{A}}$ as:
\[
\sigma \left( d_{\hat{A}} \right) = 1  - \frac{K( \langle t_1, t_2, \ldots, t_n \rangle )}{l \left( d_{\hat{A}} \right)}
\]
\end{definition}

As it was the case of the concept of redundancy, in general we do not know the complexity of the area $K(\hat{A})$, and so, in practice, it must be approximated by the complexity of the descriptions themselves $K(\hat{d}_{\hat{A}})$. However, in the particular case of areas, we could have also problems with the quantity $\hat{d}_{\hat{A}}$, since it requires to study the conditional descriptions of the topics included in the area.

\begin{definition}
Let $A \subset \mathcal{T}$ be an area with known subset $\hat{A} = \{t_1, t_2, \ldots, t_n\}$, and let $d_{\hat{A}}$ be a description. We define the \emph{weak redundancy of the description} $d_{\hat{A}}$ as:
\[
\rho(d_{\hat{A}}) =  1  - \frac{K \left( d_{\hat{A}} \right)}{l \left( d_{\hat{A}} \right)}
\]
\end{definition}

%
% Section: References
%

\section*{References}

The concept of redundancy has been also investigated in the context of information theory, since we are interested on using codes with low redundancy (see for example \cite{abramson1963information}).

