%
% CHAPTER: Surfeit
%

\chapterimage{Alexander_cuts_the_Gordian_Knot.pdf} % Chapter heading image

\chapter{Surfeit}
\label{chap:Redundancy}

\begin{quote}
\begin{flushright}
\emph{Everything should be made as simple as possible,\\
but not simpler.}\\
Albert Einstein 
\end{flushright}
\end{quote}
\bigskip

Surfeit is the final metric we will introduce to quantitatively measure our understanding of a research entity. It quantifies the presence of superfluous symbols in the description we use to model such an entity. Intuitively, our lack of knowledge about the entity is typically reflected in the length (number of symbols) of our prevailing description. Lengthy descriptions often include erroneous or redundant elements. As we enhance our understanding of the subject, we should be able to identify and eliminate these unnecessary symbols, leading to a more concise and precise description.

We define the surfeit of a description for an entity as the discrepancy in length between the given description and the optimal one for that entity. Within the framework of the theory of nescience, we operate under the assumption that the epitome of knowledge about an entity, or its perfect description, is encapsulated in the briefest description that enables the complete reconstruction of an entity’s representation. This notion of perfection is contingent upon the validity of the representation and the accuracy of the description.

The length of the most concise description of an entity is determined by the Kolmogorov complexity of a representation for that entity. As previously discussed, Kolmogorov complexity is not computable in general. Additionally, in practical terms, and considering that our knowledge about entities is typically incomplete, the most concise possible description remains unknown. As a result, surfeit is a metric that requires approximation in practice.

If we could devise a perfect description of an entity, such a description would have to be a random string; otherwise, it would embody redundant elements that could be omitted. In the context of the theory of nescience, attaining perfect knowledge is equivalent to achieving a state of randomness. This intrinsic randomness delineates a boundary on the depth of understanding attainable for a specific research topic. However, far from being a limitation, the acknowledgment and comprehension of this boundary unveil unprecedented opportunities in both science and technology. For instance, by assesing the deviation of our current description from a random string, we can estimate the closeness to a perfect description’s realization.

In this chapter, we will formally introduce the concept of surfeit and delve into its properties, including conditional surfeit. The concept of redundancy will also be unveiled as a practical approximation to surfeit. We will explore strategies to mitigate both surfeit and redundancy and scrutinize the relation between a reduction in surfeit and variations in accuracy or miscoding. Lastly, we will extend the application of the surfeit concept to incorporate the analysis of entire research areas.


%
% Section: Surfeit
%

\section{Surfeit}
\label{sec:Definition_redundancy}

Given the length of a description of a representation for an entity and the length of its shortest possible description, we can introduce a relative measure to quantify the unnecessary effort expended in explaining the entity with that particular description. We term this quantity \emph{surfeit}, and it constitutes an integral component of our definition of nescience, representing the extent of our ignorance about the research entity.

\begin{definition}[Surfeit]
Given a representation $r \in \mathcal{B}^\ast$, and $d \in \mathcal{D}$ a description for $r$, we define the \emph{surfeit of the description $d$ for the representation $r$}, denoted by $\sigma(d, r)$, as
\[
\sigma (d, r) = \frac{ | l(d) - K(r) |}{l(d)}
\]
\end{definition}

For the majority of descriptions, it will be the case that the length of the description $l(d)$ for $r$ exceeds the length of its shortest possible description $K(r)$. Intuitively, the more ignorant we are about an entity, the longer our description will be. A refined understanding of the entity should enable us to eliminate all redundant elements from its description. However, there might also be instances where the description is shorter than the optimal one. In such cases, we are oversimplifying the problem, which is equally detrimental. This is the rationale behind using the absolute value $| l(d) - K(r) |$ instead of simply $l(d) - K(r)$. Naturally, the current description might also be inaccurate, and the representation could be invalid. However, these issues are addressed by the metrics of inaccuracy and miscoding.

In our definition of surfeit, we opted for a relative measure rather than an absolute one (i.e., $| l(d) - K(r) |$) because we are interested not only in comparing the surfeit of different models for the same entity but also in comparing the surfeit across different entities. We prefer to use $K(r)$ instead of the equivalent $l \left( r^\star \right)$ to maintain consistency with the definition of inaccuracy provided in Section \ref{sec:inaccuracy:inaccuracy}.

\begin{example}
In a practical machine learning scenario, consider a task of classifying images of cats and dogs, being our representation a set of training images. An initial complex model, encumbered with excessive parameters and unnecessary features, represents a lengthy "description" of the problem. The optimal model, however, balances accuracy and simplicity. The surfeit measures the excess complexity of the initial model compared to this optimal one. It quantifies the "extra" elements that aren't essential for accurate classification. A high surfeit indicates an overly complicated model, signaling the need for refinement to achieve efficiency and effectiveness, a principle crucial in machine learning for developing models that generalize well to unseen data.
\end{example}

The surfeit of a description is a number between $0$ and $1$.

\begin{proposition}
\label{prop:range_redundancy}
Let $r \in \mathcal{B}^\ast$ be a representation, and $d \in \mathcal{D}^\star_r$ one of its {\color{red} XXX valid descriptions}, then we have that $0 \leq \sigma(d, r) \leq 1$.
\end{proposition}
\begin{proof}
Given that $l\left( d \right)>0$ and $K\left( r \right)>0$, as they are the lengths of non-empty strings, and $l\left( d \right) \geq K\left( r \right)$ because we are not considering {\color{red} XXX pleonastic descriptions}.
\end{proof}

The surfeit is zero when the length of the description \(l(d)\) is equal to the Kolmogorov complexity \(K(r)\) of the representation of the entity, signifying that the description has achieved theoretical conciseness.

\begin{proposition}
\label{prop:zero_surfeit}
Let $r \in \mathcal{B}^\ast$ be a representation, and $d \in \mathcal{D}$ a description for $r$, then we have that  $\sigma(d, r) = 0$ if and only if $l(d) = l(d^\star)$.
\end{proposition}
\begin{proof}
As a direct consequence of the definition of Kolmogorov complexity.
\end{proof}

It is essential to distinguish between conciseness and correctness. A zero surfeit highlights the optimal brevity of the description, but not necessarily its accuracy. The accuracy, or correctness, of the description is evaluated by the metric inaccuracy. Thus, while a zero surfeit points to a minimally lengthy, streamlined description, the inaccuracy metric is consulted to determine the extent to which this concise description is correct and reliable. Together, surfeit and inaccuracy offer a comprehensive assessment of the description's efficiency and validity.

\begin{example}
In a machine learning scenario, a model designed to classify emails as spam or not has achieved a surfeit of zero, indicating optimal conciseness with no superfluous elements. However, despite its streamlined complexity, the model is found to have high inaccuracy, frequently misclassifying emails. This illustrates the dichotomy between surfeit and inaccuracy. While the model is theoretically as concise as possible, signified by the zero surfeit, its practical application is hampered by incorrect classifications, highlighted by the inaccuracy metric. This underscores the necessity to balance a minimal surfeit with low inaccuracy to achieve a model that is both efficient and accurate.
\end{example}

{\color{red} Explain  that there could be more than one description that makes surfeit zero for the same representation.}

Our definition of surfeit compares the length of a description with the Kolmogorov complexity of the representation, not with the Kolmogorov complexity of the description itself (i.e., $K\left( d \right)$). That is, surfeit is not a measure of the redundancy of a description. It might happen that we come up with an incompressible description (no redundant elements to remove), that it is not the shortest possible one that describes the representation (see Example \ref{ex:description_neural}). Such a description would not be redundant in the traditional sense, but it still will present some surfeit in the sense of the theory of nescience. Moreover,it migth happen that the description $d$ we are considering does not describe the representation $r$, that is, $d \in \mathcal{D}_r$. In practice it is highly convenient to introduce the following alternative (we could say weaker) characterization of the concept of redundant model:

\begin{definition}[Redundancy]
Given a description $d \in \mathcal{D}$, we define the \emph{redundancy} of the description $d$, denoted by $\rho(d)$, as
\[
\rho(d) = 1 - \frac{K(d)}{l(d)}
\]
\end{definition}

The redundancy of a description $d$ is a quantity related to the description itself, and it does not depend on the representation $r$ being described.

\begin{example}
{\color{red} TODO: Provide an example to clarify the concept.}
\end{example}

We have that the redundancy of a description is  a number between $0$ and $1$.

\begin{proposition}
We have that $0 \leq \rho(d) \leq 1$ for all $d \in \mathcal{D}$.
\end{proposition}
\begin{proof}
Apply Proposition \ref{prop:kolmogorov_length}.
\end{proof}

{\color{red} Clarify when the redundancy is zero}

Finally, next proposition formalizes our intuition that the surfeit of a description is greater or equal than its redundancy.

\begin{proposition}
\label{prop:surfeit_comparison}
Let $r \in \mathcal{B}^\ast$ be a representation , and $d \in \mathcal{D}^\star_r$ one of its valid descriptions, then we have that $\rho(d) \leq \sigma(d, r)$.
\end{proposition}
\begin{proof}
Proving that $\rho(d) \leq \sigma(d, r)$ is equivalent to prove that $K(d) \geq K(r)$ for all $d$. Lets assume that there exist a $d$ such that $K(d) < K(r)$, that would mean there exists a Turing machine $\langle TM, a \rangle$ such that $TM(a)=r$ but $l(<TM, a>) < K(r)$. That is a contradiction with the fact that $K(r)$ is the length of the shortest possible Turing machine that prints $r$.
\end{proof}

It would be very nice if Proposition \ref{prop:surfeit_comparison} applies to all possible description. Unfortunately, the proposition is true only when we deal with valid descriptions (from $\mathcal{D}^\star_r$), as Example \ref{ex:surfeit_non_comparison} shows.

\begin{example}
\label{ex:surfeit_non_comparison}
{\color{red} TODO: Provide an example.}
\end{example}


%
% Section: Conditional Surfeit
%

\section{Conditional Surfeit}

We are interested into study how the surfeit of a description for a representation is affected when some background knowledge is assumed. That is, we want to know the surfeit of a conditional description for a representation, what we call \emph{conditional surfeit}.

\begin{definition}
Let $r \in \mathcal{B}^\ast$ be a representation, $s \in \mathcal{B}^\ast$ be a string, and $d \in \mathcal{D}$ be a description of $r$ given $s$. We define the \emph{conditional surfeit} of the conditional description $d_{r \mid s}$, denoted by $\sigma(d_{r \mid s})$, as: 
\[
\sigma(d_{r \mid s}) = 1 - \frac{K\left( r \mid s \right)}{l \left( d_{r \mid s} \right)}
\]
\end{definition}

This definition is required mostly for practical purposes, since given that our knowledge of $s$ is perfect, we can focus on studying what it is new in topic $t$, that is, in that part not covered by the assumed (perfect) background knowledge.

Conditional surfeit, being a relative measure, is a number between $0$ and $1$.

\begin{proposition}
Let $r \in \mathcal{B}^\ast$ be a representation, $s \in \mathcal{B}^\ast$ be a string, and $d \in \mathcal{D}$ be a description of $r$ given $s$. We have that $0 \leq \sigma(d_{r \mid s}) \leq 1$.
\end{proposition}
\begin{proof}
Given that $l \left( d_{r \mid s} \right) > 0$ and that $K\left( r \mid s \right) > 0$, since they are the lengths of non-empty strings, {\color{red} and that $l\left( d \right) \geq K\left( r \right)$ since we do not consider pleonastic descriptions}.

\end{proof}

Intuition tell us that the surfeit of a description could only decrease if we assume the background knowledge given by the description of another topic. This is because we require that this background knowledge must be a perfect description (it presents no surfeit). However, as it was the case of joint surfeit, we have to wait until Chapter \ref{chap:Nescience} to formalize this intuition.

In the same way we introduced the concept of redundancy of a description as a weaker version of the concept of surfeit, we can also introduce the concept of conditional redundancy as a weaker version of the concept of conditional surfeit.

\begin{definition}
Let $s \in \mathcal{B}^\ast$ be a string, and $d \in \mathcal{D}$ be a conditional description given $s$. We define the \emph{conditional surfeit} of the conditional description $d_{t \mid s^\star}$, denoted by $\sigma(d_{t \mid s^\star})$, as: 
\[
\rho(d_{r \mid s}) = 1 - \frac{K \left( d_{t \mid s^\star} \right)}{l \left( d_{t \mid s^\star} \right)}
\]
\end{definition}

Conditional surfeit is a relative measure, and so, a number between $0$ and $1$.

\begin{proposition}
We have that $0 \leq \rho(d_{t \mid s^\star}) \leq 1$ for all $t,s$ and all $d_{t,s}$.
\end{proposition}
\begin{proof}
Given that $K(d_{t \mid s^\star}) \leq l(d_{t \mid s^\star})$ we have that $\frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} \leq 1$ and so, $1 - \frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} \geq 0$. Also, since $\frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} > 0$ (both quantities are positive integers), we have that $1 - \frac{K(d_{t \mid s^\star})}{l \left( d_{t \mid s^\star} \right)} \leq 1$.
\end{proof}

Finally, we can extend our concepts of conditional surfeit and conditional redundancy to multiple, but fine, number of topics.

\begin{definition}
Let $t, s_1, s_2, \ldots, s_n \in \mathcal{T}$ be a finite collection of topics, and let $d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star}$ any conditional description of $t$ given $s_1, s_2, \ldots, s_n$. We define the \emph{conditional surfeit} of the description $d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star}$, denoted by $\sigma(d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star})$, as: 
\[
\sigma(d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star}) = 1 - \frac{K\left( t \mid s_1^\star, s_2^\star, \ldots,s_n^\star \right)}{l \left( d_{t \mid s_1^\star, s_2^\star, \ldots,s_n^\star} \right)}
\]
And the \emph{conditional redundancy} of the description $d_{t_1, t_2, \ldots, t_n}$, denoted by $\rho(d_{t_1, t_2, \ldots, t_n})$, as:
\[
\rho(d_{t_1, t_2, \ldots, t_n}) = 1 - \frac{K(d_{t_1, t_2, \ldots, t_n})}{l \left( d_{t_1, t_2, \ldots, t_n} \right)}
\]
\end{definition}

It is easy to show that the properties of conditional surfeit and conditional redundancy apply to the case of multiple topics as well.

%
% Section: Decreasing Surfeit
%

\section{Decreasing Surfeit}

{\color{red} Write this section}

%
% Section: Rate of Change
%

\section{Rate of Change}

{\color{red} Write this section}


%
% Section: Surfeit of Areas
%

\section{Surfeit of Areas}

{\color{red} Review this section}

The concept of surfeit can be extended to research areas, to quantitative measure the amount of extra effort we are using to describe the topics of the area.

\begin{definition}
Let $A \subset \mathcal{T}$ be an area with known subset $\hat{A} = \{t_1, t_2, \ldots, t_n\}$, and let $d_{\hat{A}}$ be a description. We define the \emph{surfeit of the description} $d_{\hat{A}}$ as:
\[
\sigma \left( d_{\hat{A}} \right) = 1  - \frac{K( \langle t_1, t_2, \ldots, t_n \rangle )}{l \left( d_{\hat{A}} \right)}
\]
\end{definition}

As it was the case of the concept of redundancy, in general we do not know the complexity of the area $K(\hat{A})$, and so, in practice, it must be approximated by the complexity of the descriptions themselves $K(\hat{d}_{\hat{A}})$. However, in the particular case of areas, we could have also problems with the quantity $\hat{d}_{\hat{A}}$, since it requires to study the conditional descriptions of the topics included in the area.

\begin{definition}
Let $A \subset \mathcal{T}$ be an area with known subset $\hat{A} = \{t_1, t_2, \ldots, t_n\}$, and let $d_{\hat{A}}$ be a description. We define the \emph{weak redundancy of the description} $d_{\hat{A}}$ as:
\[
\rho(d_{\hat{A}}) =  1  - \frac{K \left( d_{\hat{A}} \right)}{l \left( d_{\hat{A}} \right)}
\]
\end{definition}

%
% Section: References
%

\section*{References}

The concept of redundancy has been also investigated in the context of information theory, since we are interested on using codes with low redundancy (see for example \cite{abramson1963information}).

