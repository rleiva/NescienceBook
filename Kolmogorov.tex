%
% CHAPTER 4.- Kolmogorov Complexity
%

% TODO: Can we talk about independent topics? K(t|s) = K(t)
% TODO: Does K(t|s) != K(s|t)?


\chapterimage{Mandelbrot.pdf}

\chapter{Complexity}
\label{chap:Algorithmic_Information}

\begin{quote}
\begin{flushright}
\emph{Everything should be made as simple as possible, \\
but not simpler. \\}
Albert Einstein
\end{flushright}
\end{quote}
\bigskip

In Chapter \ref{chap:Coding}, the concept of the complexity of a string based on the lengths of the codewords of a prefix-free code was introduced. This definition is limited by two main factors: firstly, it necessitates prior knowledge of the set of possible strings, and secondly, it requires the definition of a probability distribution over this set a priori. It would be highly beneficial if we could expand the set of strings to encompass all strings (that is, $\mathcal{B}^\ast$) without necessitating a probability distribution, thereby providing an absolute notion of string complexity. Unfortunately, even if these issues are addressed, a more significant limitation exists in studying the complexity of strings using codes: certain strings, expected to be classified as simple, cannot be compressed. For instance, the binary expansion of the constant $\pi$ follows a uniform distribution over the set $\left\{0, 1\right\}$ and, as such, cannot be compressed. However, it can be fully and effectively described by a very short mathematical formula. This necessitates an alternative definition of string complexity.

\emph{Kolmogorov Complexity}, also known as \emph{Algorithmic Information Theory}, offers a definition of the complexity of a string that explicitly addresses these issues. Intuitively, the amount of information in a finite string is measured by the length of the shortest computer program capable of producing the string. This approach does not require prior knowledge of the set of valid strings or their probability distribution. Furthermore, objects like $\pi$ are appropriately classified as having low complexity. We could argue that Kolmogorov complexity provides a universal definition of the amount of information that closely aligns with our intuitive understanding. To compute the Kolmogorov complexity of a string, it is necessary to agree upon a universal description method or computer language, along with a universal computer. One might question whether, by doing so, the complexity of a string becomes dependent on the chosen computer language. Fortunately, it has been demonstrated that this is not the case, as all reasonable (and sufficiently powerful) computer languages yield the same description length, up to a fixed constant that depends on the languages chosen, but not on the string itself. Unfortunately, Kolmogorov complexity introduces a significant issue: it is a non-computable quantity, and as such, must be approximated in practice.

At this point, one might wonder if it is possible to compute the complexity of any object in general, not just strings. The answer is yes, at least theoretically. Given an object $x$, the task is to provide an encoding method that represents the object as a string  $x$. This encoding method would only be useful if we can losslessly and effectively reconstruct the original object from its encoded description. Unfortunately, providing such descriptions is not always feasible, either because the objects in question are abstract (as is often the case in mathematics) or because practical reconstruction of the object from its description is currently impossible (for example, with living organisms\footnote{As of now, it is not possible to recreate an animal solely based on its DNA.}).

%
% Section: String Complexity
%

\section{Strings Complexity}
\label{sec:strings_complexity}

In Section \ref{sec:Turing-Machines}, the concept of the Turing machine, an idealized model of computers, was introduced. We saw that Turing machines can be represented as partial computable functions $T:\mathcal{B}^\ast \rightarrow \mathcal{B}^\ast$, which assign to each input string $s \in \mathcal{B}^\ast$ an output string $T(s) \in \mathcal{B}^\ast$ (Definition \ref{def:computable-function}). We also introduced the concept of a universal Turing machine $U:\mathcal{B}^\ast \times \mathcal{B}^\ast \rightarrow \mathcal{B}^\ast$ (Definition \ref{def:Universal-Turing-Machine}), a machine that can simulate the behavior of any other Turing machine; that is, for all $(x,v) \in  \mathcal{B}^\ast \times \mathcal{B}^\ast$, we have that $U(x,v) = T_{x}(v)$. Later, in Section \ref{Codes}, the concept of a code, and in particular, the notion of a prefix-free code, was introduced (Definition \ref{def:Prefix-free-Code}). We saw that this kind of code presents important properties (Theorem \ref{th:Kraft-Inequality}). The next definition merges the best of both worlds, Turing machines and prefix-free codes, and introduces a new type of universal Turing machine.

\begin{definition}
A \emph{prefix-free universal Turing machine}\index{Prefix-free universal Turing machine} is a universal Turing machine $U:\mathcal{B}^\ast \times \mathcal{B}^\ast \rightarrow \mathcal{B}^\ast$ such that, for every $v \in \mathcal{B}^\ast$, the domain $U_{v}$ is prefix-free, where $U_{v}:\mathcal{B}^\ast \rightarrow \mathcal{B}^\ast$ and $U_{v}(p) = U(p, v)$ for all $p \in \mathcal{B}^\ast$.
\end{definition}

Using modern computer science terminology we could say that $U$ is the computer, $p$ is the program and $v$ is the input to the program. Intuitively, the above definition requires that no computer program can be a prefix of any other program. This is not a limitation from the point of view of string lengths, since, by applying McMillan's theorem (Theorem \ref{th:Kraft-Inequality}), given a uniquely decodable program, we could always find a prefix-free one that computes exactly the same function and has the same length. Moreover, in practice, real computer programs are usually prefix-free; for example, the C programming language requires that all functions must be enclosed by braces \{\}.

The concept of a prefix-free universal Turing machine allows us to introduce a new definition of the complexity of a string that aligns more closely with our intuitive understanding of the amount of computational information contained in an object (encoded as a string).

\begin{definition}[Kolmogorov Complexity]
\label{def:Kolmogorov-Complexity}\index{Kolmogorov Complexity}
Fix a prefix-free universal Turing machine $U:\mathcal{B}^\ast \times \mathcal{B}^\ast \rightarrow \mathcal{B}^\ast$. The \emph{Kolmogorov complexity} of a string $s \in \mathcal{B}^\ast$, denoted by $K(s)$, is defined as:
\[
K(s)=\min_{p,v \in \mathcal{B}^\ast}\left\{l(p) + l(v)\,:\, U(p,v)=s\right\}
\]
\end{definition}

Intuitively, the shortest description of a string $s$ is given by two elements: a program $p$ (a self delimited Turing machine) that compress all the regular patterns of the string, and a new string $v$ that comprises those parts of $s$ that do not present any regularity. We have to find the optimum balance between increasing the complexity of the program, trying to grasp more regularities, or increase the size of the non-compressible part\footnote{In the literature the Kolmogorov complexity of the string $s$ is defined as $K(s)=\min_{p \in \mathcal{B}^\ast}\left\{l(p)\,:\, U(p,\lambda)=s\right\}$, that is, the length of the shortest computer program that without any additional input can print the string $s$. We prefer to use the two parts definition $l(p) + l(v)$ because it is more in line with the requirements of the theory of nescience.}.

\begin{example}
Consider the string composed of one thousand times the substring "10", that is "$\underbrace{1010\ldots1010}_{1.000\,\mathrm{times}}$". We could write the following program:

\begin{verbatim}
    example(char *v) {
        for (int i=1; i<=1000; i++)
            printf("%s", v);
    }
\end{verbatim}
and then run it with:
\begin{verbatim}
    example("10");
\end{verbatim}

in order to print it. The length of the original string is 2.000 bits, but the length of the program is 480 bits (assuming that every symbol is encoded using an uniform code of 8 bits), and the length of the input is 2 bits, so we can conclude that the string has a low complexity. Of course, in order to compute the real Kolmogorov complexity of the string we should find the shortest Turing machine that prints that string.

On the contrary, a string composed of two thousands random 0's and 1's would have a high complexity, since it does not exists any program shorter than the program that prints the string itself.
\end{example}

As we mentioned in the preface of this chapter, Kolmogorov complexity would not be particularly useful if the complexity of strings depended on the choice of universal Turing machine. The following theorem demonstrates that this concern is unfounded, up to a constant that depends on the choice of machines, but not on the strings themselves. This establishes Kolmogorov complexity as an inherent property of the strings.

\begin{theorem}[Invariance theorem]
\label{def:Invariance-theorem}\index{Invariance Theorem}
Let $U$ and $U'$ two universal Turing machines. Then, there exists a constant $C_{U, U'}$, that depends on $U$ and $U'$, such that for each string $s \in \Sigma^{\ast}$ we have that:
\[
K_{U}(s) \leq K_{U'}(s) + C_{U, U'}
\]
\end{theorem}
\begin{proof}
Let $p, v$ be the shortest strings that $U'(p,v)=s$, then we have that $U(\langle U',p, v \rangle, \lambda) = U'(p, v) = s$, and that $l(\langle U',p \rangle) = \log(l(\langle U' \rangle)) + 1 + l(\langle U' \rangle) + l(p) = K_{U'}(s) + C$, where $l(\langle U' \rangle)$ is the length of the machine $U'$ using the encoding described in Section \ref{sec:Universal-Turing-Machines}.
\end{proof}

\begin{example}
Consider a universal programming language, such as Java, and an alternative language, such as Python. We can write a Python interpreter in Java, that is, a Java program that takes a Python script as input and executes it. Then, to compute the complexity of a string $s \in \mathcal{B}^\ast$ using Java, $C_J(s)$, it would be no greater than the complexity of the string using Python, $C_P(s)$, plus the length of the Python interpreter written in Java, $C_{J,P}$. Importantly, the length of the interpreter, $C_{J,P}$, does not depend on the string $s$.
\end{example}

Although we have proved that Kolmogorov complexity does not depend on the selected universal Turing machine, the size of the constant \(C_{U, U'}\) could pose a limitation in practical applications, especially when computing the complexity of short strings where the constant might significantly exceed the complexity of the string itself. This challenge is addressed by the Minimum Description Length principle, as described in Section \ref{sec:MML}.

\begin{notation}
We denote by \(s^\ast\) the shortest program that outputs the string \(s\) on the universal Turing machine \(U\), that is, \(s^\ast = \langle p,v \rangle\), \(U(s^\ast) = s\) and \(l(s^\ast) = K(s)\). If more than one program satisfies these properties, we select the first one using a lexicographical order induced by \(0 < 1\).
\end{notation}

The size of the \(C_{U, U'}\) constant is not the only challenge presented by Kolmogorov complexity; another issue is its non-computability, that is, there is no algorithm capable of determining the shortest program that generates any given string. The following theorem on the uncomputability of Kolmogorov complexity marks a pivotal insight into the intrinsic limits of complexity theory. By exploring the theorem, we delve into the heart of computability theory, confronting the paradoxical reality that some of the most fundamental aspects of information are inherently beyond the reach of algorithmic computation.

\begin{theorem}
The function \(K: \mathcal{B}^\ast \rightarrow \mathbb{N}\) that assigns to each string \(s\) its Kolmogorov complexity \(K(s)\) is not computable.
\end{theorem}
\begin{proof}
{\color{red} TODO: Review}
Assume for the sake of contradiction that \(K\) is computable. This means there exists a Turing machine \(T\) such that for any string \(s\), \(T\) halts with \(K(s)\) on its tape. Consider the following process, which uses \(T\) to construct a string \(s\) of arbitrary complexity: i) enumerate all binary strings using a shortlex ordering: \(s_1, s_2, \ldots\), ii) for each string \(s_i\), compute \(K(s_i)\) using \(T\) and select \(s_i\) such that \(K(s_i) > l(s_i) + C\), where \(C\) is a constant representing the length of this description plus the operation to add \(C\). Such a process would effectively produce a string \(s\) whose Kolmogorov complexity is greater than its length plus a constant, which contradicts the definition of Kolmogorov complexity as the length of the shortest description. The contradiction arises because our assumption that \(K\) is computable allows us to construct a string that defies the bounds set by \(K\) itself. Therefore, the function \(K\) is not computable.
\end{proof}

If \(K\) were computable, we could solve the Halting Problem by constructing a program that, for any input program and input, computes whether the program halts by checking if its Kolmogorov complexity is finite. Since the Halting Problem is known to be undecidable, this provides a contradiction.

In practice, we approximate the Kolmogorov complexity using the compressed version of the string, employing standard compression algorithms, such as the Huffman algorithm described in Section \ref{sec:Huffman-Algorithm}.

%
% Section: Properties of Kolmogorov Complexity
%

\section{Properties of Complexity}

In this section, we delve into the properties of Kolmogorov complexity. We will explore the foundational principles that govern this complexity measure, including its invariance, symmetry, and non-computability. Through examining these properties, we gain deeper insights into the complex interplay between information, computation, and randomness.

Kolmogorov complexity is a finite positive natural number.

\begin{proposition}
For all $s\in\mathcal{B}^{\ast}$ we have that $0 < K(s) < \infty$.
\end{proposition}
\begin{proof}
Since $K(s)$ is the length of a string, it must be greater than $0$. The property $K(s) < \infty$ is a consequence of Proposition \ref{prop:kolmogorov_length} and the fact that we are only dealing with finite strings.
\end{proof}

The Kolmogorov complexity of a string cannot surpass the sum of its own length and a constant.

\begin{proposition}
\label{prop:kolmogorov_length}
There is a constant $c$ such that for all $s\in\mathcal{B}^{\ast}$ we have that $K(s) \leq l(s) + c$.
\end{proposition}
\begin{proof}
Let $s \in \mathcal{B}^\ast$ be an arbitrary string, and consider the encoding of a Turing machine $p$ such that for any input $v = s$, it halts and outputs $s$. The program $p$ is designed to simply reproduce its input. Given this setup, when $p$ is executed on a universal Turing machine $U$ with $s$ as input, it satisfies the condition $U(p, s) = s$. The length of \(p\) is a constant $c$ across all strings $s$. This constancy arises because $p$'s mechanism (accepting an input and outputting it unchanged) does not vary with the size or content of $s$. By the definition of Kolmogorov Complexity $K(s)$, which seeks the minimum length of a program-input pair that generates $s$, the combination of $p$ and $s$ presents a feasible solution. Therefore, we have $K(s) \leq l(s) + l(p) = l(s) + c$.
\end{proof}

The size of the constant $c$ depends on the specific encoding schema used by the selected universal Turing machine $U$, but it is independent of the string $s$. In Section \ref{sec:incompressibility_randomness}, we will explore the characteristics of random strings, which are defined as strings that cannot be compressed. Such strings exhibit a Kolmogorov complexity that exceeds their own length, that is, $K(s) > l(s)$.

The absolute difference in Kolmogorov complexity between any string \(x\) and its transformed counterpart \(f(x)\), via a computable bijection, is bounded by a constant $c$. That is, not only does $f$ not increase the complexity of $x$ by more than a constant, but also $f$ does not decrease the complexity by more than a constant.

\begin{proposition}
Let $f:\mathcal{B}^{\ast} \to \mathcal{B}^{\ast}$ is a computable biyection, then there exists a constant $c$ such that $| K\left( f(x) \right) -  K(x) | < c$.
\end{proposition}
\begin{proof}
Let $P_f$ be the program that computes $f$ and $P_{f^{-1}}$ the program that computes the inverse of $f$. For any string $x$, let $P_x$ be the shortest program that generates $x$. Then, a program $P_{f(x)}$ that generates $f(x)$ can be constructed by concatenating $P_x$ with $P_f$. The length of this program is $|P_{f(x)}| = |P_f| + |P_x|$. Since $|P_f|$ is a constant that does not dependent on $x$, we can say that $K(f(x)) \leq K(x) + |P_f|$. Similarly, given $f(x)$, we can construct a program $P'_{x}$ to generate $x$ by applying $P_{f^{-1}}$ to $f(x)$. The length of this program is $|P'_{x}| = |P_f| + |P_{f^{-1}}|$. Thus, $K(x) \leq K(f(x)) + |P_{f^{-1}}|$. The two inequalities combined implies that $|K(f(x)) - K(x)| \leq \max(|P_f|, |P_{f^{-1}}|) = c$, where \(c\) is a constant that represents the maximum of the lengths of the programs that compute \(f\) and \(f^{-1}\). This constant \(c\) does not depend on \(x\), but rather on the complexity of the functions \(f\) and \(f^{-1}\).
\end{proof}

This proposition shows a remarkable stability of informational content under computable bijections, underscoring the intrinsic robustness of Kolmogorov complexity in the face of such transformations.


%
% Section: Joint Kolmogorov Complexity
%

\section{Joint Kolmogorov Complexity}

The joint Kolmogorov complexity of two strings $s$ and $t$ is defined as the length of the shortest is the shortest program $p$ that, when executed on an universal Turing machine $U$, outputs the pair $\langle s, t \rangle$, that is, in such a way that allows both strings to be unambiguously retrieved.

\begin{definition}[Joint Kolmogorov Complexity]
\label{def:Joint-Kolmogorov-Complexity}\index{Joint Kolmogorov complexity}
The \emph{Joint Kolmogorov complexity} of the strings $s, t \in \mathcal{B}^\ast$, denoted by $K(s, t)$, is defined as:
\[
K(s, t)=\min_{p,v \in \mathcal{B}^\ast}\left\{l(p) + l(v)\,:\, U(p,v)=\langle s, t \rangle \right\}
\]
\end{definition}

The notation $K(s, t)$ and $K(st)$ represent two different concepts in the context of Kolmogorov complexity. $K(s, t)$ refers to the joint Kolmogorov complexity of two strings $s$ and $t$ as per Definition \ref{def:Joint-Kolmogorov-Complexity}, meanwhile $K(st)$ represents the Kolmogorov complexity of the concatenation of $s$ and $t$, without any additional structure to distinguish between them, and so, Defintion \ref{def:Kolmogorov-Complexity} is applied. The choice between $K(s, t)$ and $K(st)$ depends on whether it's important to preserve and utilize the distinction and relationship between $s$ and $t$. If analyzing the interplay or the shared characteristics of $s$ and $t$ is relevant, $K(s, t)$ is more appropriate. If the focus is on the information content of the combined sequence without regard to its origin from two separate strings, $K(st)$ is used.

\begin{example}
{\color{red} TODO: Pending}
\end{example}

Our first proposition highlights a fundamental symmetry in Kolmogorov complexity, illustrating that the complexity of describing a pair of strings in either order differs by at most a constant. This reflects the intrinsic property that the information content is independent of the specific arrangement of the strings being described. The constant \(c\) encapsulates the overhead associated with the operations needed to reverse the order of the strings.

\begin{proposition}
\label{prop:kolmogorov_order}
There is a constant $c$ such that for all $x, y \in\mathcal{B}^{\ast}$ we have that $K(x, y) \leq K(y, x) + c$.
\end{proposition}
\begin{proof}
The key to this proof lies in the symmetry of information and the properties of a universal Turing machine. Let $U$ be a universal Turing machine, and let $p_{xy}$ and $p_{yx}$ be the shortest programs that output $x$ followed by $y$, and $y$ followed by $x$, respectively, when executed on $U$. To convert $p_{xy}$ into a program that outputs $yx$, we need to add or modify a finite set of instructions to swap the output order. This set of instructions has a fixed length, denoted by $c$. Therefore, the program that first computes $x$ and $y$ and then swaps their order can be at most $c$ bits longer than the program that directly computes $y$ and $x$.
\end{proof}

Next proposition underscores the subadditive nature of Kolmogorov complexity, proving that the total complexity of describing two strings jointly cannot exceed the sum of their individual complexities by more than a fixed constant, irrespective of the strings' content.

\begin{proposition}
\label{prop:additive_kolmogorov}
There is a constant $c$ such that for all $s, t \in\mathcal{B}^{\ast}$ we have that $K(s, t) \leq K(s) + K(t) + c$.
\end{proposition}
\begin{proof}
Let $s^\ast$ and $t^\ast$ be the shortest computer programs that generate $s$ and $t$, respectively. Given that $s^\ast$ and $t^\ast$ are designed to be prefix-free, a universal Turing machine $U$ can unambiguously decode each program without needing any additional information and subsequently generate the strings $s$ and $t$. The constant $c$ accounts for the requirement of $U$ to prepend the concatenated program for $st$ with the length $l(s)$ of $s$. This step ensures that the concatenated sequence $st$ can be decoded unambiguously.
\end{proof}

The last proposition states the relationship between the complexity of individual strings and their joint complexity, by establishing a lower bound on the joint complexity of two strings relative to their individual complexities.

\begin{proposition}
\label{prop:excess_kolmogorov}
There is a constant $c$ such that for all $s, t \in\mathcal{B}^{\ast}$ we have that $K(s, t) \geq K(s) + c$ and $K(s, t) \geq K(t) + c$.
\end{proposition}
\begin{proof}
Assume for the sake of contradiction that $K(s, t) < K(s) + c$. Let $p$ be the shortest computer program that outputs the pair $\langle s, t \rangle$, meaning that $K(s, t) = l(p)$. Since program $p$ unambiguously generates $s$ as part of the output pair, it follows that the complexity of generating $s$ alone, $K(s)$, should not exceed the length of $p$. which is a contradiction with respect the intial assumption of $K(s, t) < K(s) + c$. 
\end{proof}


%
% Section: Conditional Kolmogorov Complexity
%

\section{Conditional Kolmogorov complexity}

{\color{red} TODO: Section Pending!}

Sometimes, the description of a string can be greatly reduced if we assume we know about another string. In this section we are going to introduce the concept of \emph{conditional Kolmogorov complexity}, that is, the complexity of a string $s$ when another string $s'$ is given as input.

\begin{definition}[Conditional Kolmogorov Complexity]
Given the universal Turing machine $U:\Sigma^{\ast}\times\Sigma^{\ast}\rightarrow\Sigma^{\ast}$ we define the \emph{conditional Kolmogorov complexity} of a string $s\in\Sigma^{\ast}$ given the string $s'\in\Sigma^{\ast}$ as:
\[
K(s|s')=\min_{p,v\in\Sigma^{\ast}}\left\{l(p) + l(v)\,:\, U(p,\langle v, s' \rangle)=s\right\}
\]
\end{definition}

As it was the case of the non-conditional Kolmogorov complexity, we have that the conditional complexity of a string $s$ is a quantity that depends on the string itself and the provided string $v$, but not on the reference machine used for the computation.

\begin{exercise}
Given $U$ and $U'$, two universal Turing machines, prove that there exists a constant $C_{U, U'}$ such that for each strings $s$ and $s' \in \Sigma^{\ast}$ we have that $K_{U}(s|s') \leq K_{U'}(s|s') + C_{U, U'}$.
\end{exercise}

{\color{red} TODO: Multiple conditional complexity}

\begin{proposition}
For all $s\in\mathcal{B}^{\ast}$ we have that $0 < K(s | v) < \infty$.
\end{proposition}
\begin{proof}
{\color{red} TODO}
\end{proof}


\begin{proposition}
There is a constant $c$ such that for all $s\in\mathcal{B}^{\ast}$ we have that $K(s | s ) = c$.
\end{proposition}
\label{prop:self_conditional}
\begin{proof}
{\color{red} TODO}
\end{proof}

The complexity of a string $s$ does not increase when another string

\begin{proposition}
\label{prop:kolmogorov_conditional}
There is a constant $c$ such that for all $s, s'\in\mathcal{B}^{\ast}$ we have that $K(s | s' ) \leq K(s) + c$.
\end{proposition}
\begin{proof}
{\color{red} TODO}
\end{proof}

{\color{red} TODO: Introduce this propostion.}

\begin{proposition}
\label{prop:kolmogorov_joint_conditional}
There is a constant $c$ such that for all $r ,s, s'\in\mathcal{B}^{\ast}$ we have that $K(s | rs' ) \leq K(s \mid s') + c$.
\end{proposition}
\begin{proof}
{\color{red} TODO}
\end{proof}


{\color{red} TODO: Introduce this propostion.}

\begin{proposition}
\label{prop:kolmogorov_relations}
For all $s, s'\in\mathcal{B}^{\ast}$ we have that
\[
K(s | s' ) \leq K(s) \leq K(s, s')
\]
\end{proposition}
\begin{proof}
{\color{red} TODO}
\end{proof}

{\color{red} TODO: Introduce this propostion.}

\begin{proposition}
\label{prop:kolmogorov_relations}
For all $X, Y\in\mathcal{B}^{\ast}$ we have that
\[
K(XY) = K(X) + K(Y \mid X)
\]
\end{proposition}
\begin{proof}
{\color{red} TODO}
\end{proof}




%
% Section: Information Distance
%

\section{Information Distance}
\label{sec:information_distance}

{\color{red} TODO: Section Pending!}

In this section we are going to introduce a universal measure of absolute information distance between individual objects. Intuitively, the information distance between two strings $x$ and $y$ would be the length of the shortest computer program that allow us to transform $x$ into $y$ and $y$ into $x$. This notion of distance is universal in the sense that it contains all the other notions of computable distance as special cases.

We might be tempted to use as information distance the conditional Kolmogorov coplexity of $x$ given $y$, $K(x \mid y)$, however conditional complexity is not suitable as a measure of information distance due to its assymetry, that is, $K( \lambda \mid y)$ is always small, even in the case of $y$ is not close to the empty string. Equally erroneously would be to use $K(x \mid y) + K(y \mid x)$ since that would not take into account the redundancy existing between the information required to transform $x$ into $y$ and $y$ into $x$.

\begin{definition}
The \emph{information distance} between two binary strings $x$ and $y$, denoted by $E(x, y)$, is defined as
\[
E_U(x, y) = \min \{ l(p) : U(p, x) = y, U(p, y) = x \}
\]
\end{definition}

{\color{red} Say something about that information distance is up to a fixed universal Turing machine. Perhaps insists that it is not computable.}

{\color{red} Introduce the following proposition.}

\begin{proposition}
Let $x$ and $y$ two binary strings, then we have that:
\[
E_U(x, y) = \max\{ K(x \mid y), K(y \mid x) \} + O ( \log \max\{ K(x \mid y), K(y \mid x) \}) 
\]
\end{proposition}
\begin{proof}
{\color{red} TBD (based on the maximal overlap).}
\end{proof}

{\color{red} Introduce the concept of max distance as:}
\[
E(x, y) = \max\{ K(x \mid y), K(y \mid x) \}
\]

\begin{example}
{\color{red} TODO: Give an example based on the bitwise exclusive or.}
\end{example}

{\color{red} Introduce the following proposition.}

\begin{proposition}
{\color{red} $E(x, y)$ is a metric.}
\end{proposition}
\begin{proof}
\end{proof}

{\color{red} TODO: Introduce the concept of admisible information distance. Prove that $E(x, y)$ is an admisible information distance. Prove that it is minimal for every admisible information distance. Explain this notion of universality.}

{\color{red} Explain the problem of using non-normilized distance, perhaps with an example. "Normaliztion refers to the fact that the transformation description size must be seen in relation to the size of the participating objects"}

{\color{red} TODO: Perhaps say something about the options of normalization, and why we choose max.}

\begin{definition}
The \emph{normalized information distance} between two binary strings $x$ and $y$, denoted by $e(x, y)$, is defined as:
\[
e(x, y) = \frac{\max\{ K(x \mid y), K(y \mid x) \}}{\max \{ K(x), K(y) \} }
\]
\end{definition}

\begin{definition}
The \emph{normalized compression distance} between two strings $x$ and $y$, given the compressor $Z$, and denoted by $e_Z(x, y)$, is defined as:
\[
e_Z(x, y) = \frac{ Z(xy) - \min\{ Z(x), Z(y) \}}{\max \{ Z(x), Z(y) \} }
\]
\end{definition}

{\color{red} Introduce the following proposition.}

\begin{proposition}
\label{prop:ncd_between_zero_and_one}
The normalized information distance $e(x, y)$ takes values in the range $[0, 1]$.
\end{proposition}
\begin{proof}
{\color{red} TODO: Pending}
\end{proof}

{\color{red} Introduce the following proposition.}

\begin{proposition}
The normalized information distance $e(x, y)$ is a metric, up to negligible errors.
\end{proposition}
\begin{proof}
{\color{red} TODO: Pending}
\end{proof}

{\color{red} TODO: Explain that NID is not computable in the general case, and how wonderfull would be to have a computable equivalent.}

{\color{red} Introduce the following proposition.}

\begin{proposition}
\[
E(x, y) = \max\{ K(x \mid y), K(y \mid x) \} = K(xy) - \min\{ K(x), K(y) \} + O(\log K(xy) )
\]
\end{proposition}
\begin{proof}
{\color{red} TODO: Pending}
\end{proof}

{\color{red} Introduce the following definition.}

\begin{definition}
Normalize compression distance.
\end{definition}

{\color{red} And its properties.}


%
% Section: Incompressibility and Randomness
%

\section{Incompressibility and Randomness}
\label{sec:incompressibility_randomness}

{\color{red} TODO: Section Pending!}

It is easy to prove that the mayority of strings cannot be compressed. For each $n$ there are $r^n$ string of length $n$, but only Sum possible shorter descriptions. Therefore, there is at least one string $x$ of length $n$ such that K(n) >= n. We call such strings \emph{incompressible}

\begin{definition}
For each constant $c$ we say that a string $s$ is \emph{c-incompressible} if $K(s)\geq l(s)-c$.
\end{definition}

We will use the term \emph{incompressible string} to refer to all the strings that are c-incompressible with small c. Those string are characterized because they do not contain any patterns, since a pattern could be used to reduce the description length. Intuitively, we think of such patternless sequences as being \emph{random}.

Th number of strings o length n thare are c-incompressible is ast least $2^n - 2^(n-c) + 1$. Hence there is ate least one 0-incompressible string of length $n$, at least one-half of all strings of length n are 1-incompressible, at lesat three-fourths of all strings of length $n$ are 2-incompressible, ..., and at leas the $(1-1/2^c)$ th part of all $2^n$ strings of length $n$ are c-incompressible. This means that for each constant c>1 the majority of all strings of length $n$ with n>c are c-incompressible.


%
% Section: Bibliography and References
%

\section*{Bibliography and References}

{\color{red} TODO: Section Pending!}

{\color{red} Kolmogorov Complexity, a fundamental concept in the field of information theory and computer science, emerged in the 1960s from the work of three pioneering researchers: Andrei Kolmogorov, Ray Solomonoff, and Gregory Chaitin. Independently arriving at similar concepts, these scholars sought to formalize a measure of the information content or complexity of an object (specifically, finite strings) in a way that is independent of the specific language or encoding used. Kolmogorov, a prominent Soviet mathematician, introduced his formulation in 1965, aiming to provide a theoretical foundation for understanding the complexity of individual objects beyond the probabilistic framework of Shannon's information theory. Solomonoff, an American information theorist, introduced a related concept as part of his work on algorithmic probability, emphasizing its implications for prediction and inductive inference. Chaitin, working in the United States as well, contributed by exploring the mathematical properties and implications of this notion, including its non-computability. The motivations behind the development of Kolmogorov Complexity were multifaceted, encompassing the desire to better understand the nature of information, randomness, and the limits of computation and prediction, thus laying the groundwork for numerous applications in theoretical computer science, mathematics, and beyond. }


The idea of measuring the amount of information contained in a string based on the shorter computer program that can reproduce it was introduced independelntly by Solomonoff \cite{solomonoff1964formal}, Kolmogorov \cite{kolmogorov1965three} and Chaiting \cite{chaitin1969simplicity}.

In the Kolomogorov complexity literature, the concept of Kolmogorov complexity is defined in terms of computable functions, i.e. Turing machines. Then it is shown that there are some machines that do not provide worse descriptiosn than others, up to a constant. And finally, it is proved that unversal Turing machines are not worse than any other machines. We have provided here an easier to understand short-cut to this approach. Also, we do not consider the case of of non-prefix Kolmogorov complexity. And we prefer to split code and data. See XX, XX, XX, or XX for a more classical introduction to Kolmogorov complexity.

