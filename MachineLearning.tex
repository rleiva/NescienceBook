%
% CHAPTER: Machine Learning
%

\chapterimage{Capek_play.pdf}

\chapter{Machine Learning}
\label{chap:Machine-Learning}

\begin{quote}
\begin{flushright}
\emph{There are no difficult problems,\\
only lack of imagination.}\\
Antonio García \\
\end{flushright}
\end{quote}
\bigskip

We have seen that the most challenging problems to which we can apply the results of the theory of nescience arise when the set of entities $\mathcal{E}$ under study is composed of abstract elements. The difficulty with abstract entities is that there is no straightforward way to encode them as finite strings of symbols from which they can be effectively reconstructed. Without such an encoding, it becomes impossible to measure their redundancy, accuracy, or any other property required by the theory of nescience.

In practice, a common approach to overcome this limitation is to design an experiment and collect observable outcomes, as is typically done in physics or other empirical sciences. Another possible approach is to collect a set of indirect measurements that serve as observable proxies for the abstract entities, for example, by recording the behavior of users in an online social network to study social interaction patterns.

This chapter is devoted to applying the concept of minimum nescience to the field of machine learning. We assume that the entities under study have been encoded as a dataset $\mathbb{X}$ consisting of $n$ training vectors, each with $p$ predictors (or features), and an associated response variable $\mathbf{y}$ (see Section \ref{sec:machine_learning}).

We will begin by presenting practical approximations for the concepts of miscoding, inaccuracy, and surfeit when the entities are represented as datasets. We will then show how these quantities can be combined into the single measure of nescience. These approximations will allow us to introduce the minimum nescience principle, a method designed to automate the process of selecting optimal models in machine learning (a form of automated machine learning, or AutoML).

In addition to introducing these approximations, we will demonstrate how to apply them to solve practical machine learning problems. The examples provided will be based on the \texttt{nescience}\footnote{\texttt{https://github.com/rleiva/nescience}} library, an open-source Python library that implements the ideas presented in this chapter.

%
% Section: Nescience Library
%

\section{Nescience Python Library}
\label{sec:nescience_library}

The \texttt{nescience} library is an open-source Python package that implements the ideas presented in this book, specifically their application to the field of machine learning. The library follows the API design and conventions of the widely used \texttt{scikit-learn} toolkit, which allows it to be seamlessly combined with the methods and workflows provided by that package.

The \texttt{nescience} library can be installed with the \texttt{pip} utility:

\begin{sourcecode}
{\scriptsize \begin{verbatim}
pip install nescience
\end{verbatim}}
\end{sourcecode}

On the companion website for the library, the reader will find a collection of \texttt{jupyter-lab} notebooks that illustrate how the library works. Each subsection of this chapter is accompanied by a notebook containing implementations of all the examples discussed, so that the reader can reproduce the results and experiment with them interactively. Additional information about the \texttt{nescience} library, including a complete API reference, is also available on the library's website.

%
% Section: A Note About Compression
%

\section{A Note About Compression}
\label{sec:note_about_compression}

As is customary, the Kolmogorov complexity $K(s)$ of a string $s$ will be approximated by the length of the compressed version of that string using a standard compressor. In this way, we can estimate the normalized compression distance as follows:
\[
E_Z(\mathbf{x}_j, \mathbf{y}) = \frac{\max\{ \hat{K}_Z(\mathbf{x}_j \mid \mathbf{y}), \hat{K}_Z(\mathbf{y} \mid \mathbf{x}_j) \}}{\max \{ \hat{K}_Z(\mathbf{x}_j), \hat{K}_Z(\mathbf{y}) \} },
\]
where $\hat{K}_Z(s)$ denotes the length of the compressed version of the string $s$ using the compressor $Z$.

In the particular case of having a vector $\mathbf{x} = \{ x_1, \ldots, x_n \}$ of measurements, the string $s$ to be compressed will be the concatenation of the encoded values $s = \langle x_1, \ldots, x_n \rangle$. In practice, it is usually easier to compute the joint encoding of two vectors than their conditional encoding. For this reason, we prefer the following equivalent definition of normalized compression distance:
\[
E_Z(\mathbf{x}_j, \mathbf{y}) = \frac{ \hat{K}_Z(\mathbf{x}_j, \mathbf{y}) - \min\{ \hat{K}_Z(\mathbf{x}_j), \hat{K}_Z(\mathbf{y}) \} } { \max\{ \hat{K}_Z(\mathbf{x}_j), \hat{K}_Z(\mathbf{y}) \} }.
\]
As the compression technique, we will use a code $C$ of minimal expected length, given the relative frequencies of the observed values (see Section \ref{sec:Optimal-Codes}). If $\mathbf{x}$ is a qualitative vector (either a feature or the target variable) taking values from a set of labels $\mathcal{G} = \{g_1, \ldots, g_\ell\}$, that is, $\mathbf{x} \in \mathcal{G}^n$, the quantity $\hat{K}_C(\mathbf{x})$ can be computed as:
\[
\hat{K}_C(\mathbf{x}) = - \sum_{i=1}^\ell \log_2\!\left( \frac{ \sum_{j=1}^n I(x_j = g_i)} {n} \right).
\]
If $\mathbf{x}$ is based on a continuous random variable, we cannot directly calculate the probability of each $x_j$ since, in general, the underlying probability distribution of $\mathbf{x}$ is unknown, and moreover $P(x_j)=0$ for all $j$. To approximate $K(\mathbf{x})$ using a minimal-length code $C$, we must first discretize the vector $\mathbf{x}$ into a finite set of intervals.

A discretization algorithm maps a (potentially huge) set of numeric values to a reduced set of discrete categories, inevitably losing some information. The choice of discretization algorithm can have a significant impact on the practical computation of nescience. Ideally, we want a discretization method that balances bias and variance: it should produce enough intervals to capture relevant variation (low bias), while ensuring enough observations per interval to obtain reliable frequency estimates (low variance).

Common techniques include \emph{equal-width discretization}, \emph{equal-frequency discretization}, and \emph{fixed-frequency discretization}. However, these methods require selecting a hyperparameter (the number of intervals), which makes them unsuitable for fully automated model selection.

In the \texttt{nescience} library, we use a proportional discretization approach (see Section \ref{sec:discretization_algorithms}), where both the number of intervals $m$ and the number of observations per interval $s$ are proportional to the total number of observations $n$. In particular, the library sets $s = m = \sqrt{n}$, which grows adaptively with the dataset size without introducing external hyperparameters.

Using this discretization procedure, we can approximate the Kolmogorov complexity of a vector $\mathbf{x}$ by:
\[
\hat{K}_C(\mathbf{x}) = - \sum_{i=1}^m \log_2\!\left( \frac{ \sum_{j=1}^n I(x_j \in D_i)} {n} \right),
\]
where $D_i$ is the $i$-th interval defined by the endpoints $(i-1, i)$.

This quantity $\hat{K}_C$ can be generalized to an arbitrary number of $m$ vectors $\hat{K}_C(\mathbf{x_1}, \ldots, \mathbf{x_m})$ composed of $n$ samples each, by considering the joint encoded vector
\[
\langle \mathbf{x_1}, \mathbf{x_2}, \ldots, \mathbf{x_m} \rangle = \{ \langle x_{11}, x_{12}, \ldots, x_{1m} \rangle, \ldots, \langle x_{n1}, x_{n2}, \ldots, x_{nm} \rangle \}.
\]

\begin{proposition}
The normalized compression distance of two vectors $\mathbf{x}$ and $\mathbf{y}$, when computed using a compressor based on optimal codes, is equivalent to the normalized mutual information between these two vectors, that is:
\[
NCD_C\left(\mathbf{x},\mathbf{y}\right)=1-\frac{I\left(\mathbf{x};\mathbf{y}\right)}{\max\left\{ H\left(\mathbf{x}\right),H\left(\mathbf{y}\right)\right\} }.
\]
\end{proposition}
\begin{proof}
We must prove that
\begin{align*}
NCD\left(\mathbf{x},\mathbf{y}\right) 
&= \frac{C\left(\mathbf{x},\mathbf{y}\right)-\min\left\{ C\left(\mathbf{x}\right),C\left(\mathbf{y}\right)\right\} }{\max\left\{ C\left(\mathbf{x}\right),C\left(\mathbf{y}\right)\right\} } \\[4pt]
&= \frac{nH\left(\mathbf{x},\mathbf{y}\right)-\min\left\{ nH\left(\mathbf{x}\right),nH\left(\mathbf{y}\right)\right\} }{\max\left\{ nH\left(\mathbf{x}\right),nH\left(\mathbf{y}\right)\right\} } \\[4pt]
&= \frac{H\left(\mathbf{x},\mathbf{y}\right)-\min\left\{ H\left(\mathbf{x}\right),H\left(\mathbf{y}\right)\right\} }{\max\left\{ H\left(\mathbf{x}\right),H\left(\mathbf{y}\right)\right\} }.
\end{align*}

Consider two cases.  

\textbf{Case 1:} $H\left(\mathbf{x}\right) > H\left(\mathbf{y}\right)$.
\begin{align*}
NCD\left(\mathbf{x},\mathbf{y}\right)
&= \frac{H\left(\mathbf{x},\mathbf{y}\right)-H\left(\mathbf{y}\right)}{H\left(\mathbf{x}\right)} 
= \frac{H\left(\mathbf{y}\right)+H\left(\mathbf{x}\mid\mathbf{y}\right)-H\left(\mathbf{y}\right)}{H\left(\mathbf{x}\right)} 
= \frac{H\left(\mathbf{x}\mid\mathbf{y}\right)}{H\left(\mathbf{x}\right)} \\[4pt]
&= \frac{H\left(\mathbf{x}\right)-I\left(\mathbf{x};\mathbf{y}\right)}{H\left(\mathbf{x}\right)}
= 1-\frac{I\left(\mathbf{x};\mathbf{y}\right)}{H\left(\mathbf{x}\right)}.
\end{align*}

\textbf{Case 2:} $H\left(\mathbf{y}\right) > H\left(\mathbf{x}\right)$.
\begin{align*}
NCD\left(\mathbf{x},\mathbf{y}\right)
&= \frac{H\left(\mathbf{x},\mathbf{y}\right)-H\left(\mathbf{x}\right)}{H\left(\mathbf{y}\right)} 
= \frac{H\left(\mathbf{x}\right)+H\left(\mathbf{y}\mid\mathbf{x}\right)-H\left(\mathbf{x}\right)}{H\left(\mathbf{y}\right)}
= \frac{H\left(\mathbf{y}\mid\mathbf{x}\right)}{H\left(\mathbf{y}\right)} \\[4pt]
&= \frac{H\left(\mathbf{y}\right)-I\left(\mathbf{x};\mathbf{y}\right)}{H\left(\mathbf{y}\right)}
= 1-\frac{I\left(\mathbf{x};\mathbf{y}\right)}{H\left(\mathbf{y}\right)}.
\end{align*}

Combining both cases, we obtain:
\[
NCD_C\left(\mathbf{x},\mathbf{y}\right)=1-\frac{I\left(\mathbf{x};\mathbf{y}\right)}{\max\left\{ H\left(\mathbf{x}\right),H\left(\mathbf{y}\right)\right\} }.
\]
\end{proof}

% % Subsection: Compression of strings

% \subsection{Compression of Strings} 

% {\color{red} TODO: Pending}

% % Subsection: Compression of datasets

% \subsection{Compression of Datasets}

% Let $\mathbf{x} = { x_1, \ldots, x_n }$ be a vector to be compressed, representing either a feature or the target variable. As the compression technique, we employ a code $C$ of minimal length, determined by the relative frequencies of the observed values (see Section \ref{sec:Optimal-Codes}). We must distinguish between two cases: when the vector consists of discrete values and when it consists of continuous values.

% If the variable is discrete, its values do not require further discretization. Suppose $\mathbf{x}$ is a qualitative vector taking values from a set of labels $\mathcal{G} = {g_1, \ldots, g_\ell}$, that is, $\mathbf{x} \in \mathcal{G}^n$. In this case, the empirical frequencies of the labels provide a natural basis for constructing the code. Specifically, let
% \[
% f_i = \frac{1}{n}\sum_{j=1}^n I(x_j = g_i)
% \]
% be the observed relative frequency of label $g_i$.

% In the case where $\mathbf{x}$ is generated by a continuous random variable, we cannot directly assign probabilities to individual values $x_j$, since the underlying distribution of $\mathbf{x}$ is unknown and, moreover, $P(x_j)=0$ for all $j$. To approximate the Kolmogorov complexity $K(\mathbf{x})$, we first discretize $\mathbf{x}$ into a finite set of intervals and then apply the same optimal coding scheme used in the discrete case.

% A discretization algorithm maps a (potentially vast) set of numeric values into a reduced set of discrete categories, inevitably losing some information in the process. The choice of discretization method strongly affects the practical computation of nescience. Ideally, the procedure should balance bias and variance: it should use enough intervals to capture the structure of the data (low bias), but not so many that most bins remain empty (which would lead to high variance).

% Common approaches include \emph{equal-width discretization}\index{Equal-width discretization}, \emph{equal-frequency discretization}\index{Equal-frequency discretization}, and \emph{fixed-frequency discretization}\index{Fixed-frequency discretization}. In the \texttt{mnplib} library we adopt an equal-width discretization strategy, enhanced with additional techniques that remove the need to manually optimize the hyperparameter controlling the number of bins, thereby enabling full automation of the discretization process.

% Our approach begins by selecting the number of bins according to \emph{Rice's rule},
% \[
% B \;\approx\; 2 \, n^{1/3},
% \]
% which is asymptotically motivated in the context of histogram density estimation and offers a good balance between resolution and robustness. This rule avoids the excessive granularity of the square-root heuristic (which can result in too few samples per bin when building joint encodings), while remaining less conservative than Sturges’ logarithmic rule (see Section \ref{sec:discretization_continuous_variables}).

% Equal-width binning performs poorly in the presence of outliers, since extreme values can stretch the binning range, leaving large portions of the histogram empty. To address this issue, we adopt a \emph{trimmed core range}\index{Trimmed core range}: rather than spanning from $\min(x)$ to $\max(x)$, we restrict the binning interval to
% \[
% [a, b] \;=\; \bigl[ Q_\alpha(x), \; Q_{1-\alpha}(x) \bigr],  
% \]
% where $Q_\alpha(x)$ denotes the $\alpha$-quantile (e.g., $\alpha = 0.005$). Two additional overflow bins, $(-\infty, a)$ and $(b, +\infty)$, capture the extreme tails, ensuring robustness against outliers.

% To further prevent sparsity within the core, we impose a minimum occupancy threshold $m_{\min}$. If any bins fall below this threshold, the number of core bins is reduced until all bins contain at least $m_{\min}$ samples. This guarantees that each symbol represents a meaningful portion of probability mass (see Section \ref{sec:discretization_continuous_variables}).

% With this procedure, the estimated Kolmogorov complexity of a dataset is given by the length of its compressed representation under an optimal code. The discretization process combines Rice's rule for bin selection, robust trimming to handle outliers, and minimum occupancy enforcement to avoid empty bins. Together, these components ensure that the complexity estimates are (i) faithful to the observed distribution, (ii) robust to extreme values, and (iii) comparable across single and joint variables.

% Given either a discretized continuous variable or a discrete variable with categorical labels, let $c_i$ denote the number of samples in category or bin $i$, and let $B$ be the total number of bins (or distinct labels). Instead of relying on the naive plug-in estimator $p_i = c_i/n$, we adopt the Krichevsky-Trofimov (or Jeffreys) smoothing:
% \[
% p_i \;=\; \frac{c_i + \tfrac{1}{2}}{n + \tfrac{B}{2}}.  
% \]
% This Bayesian correction eliminates zero-probability events, stabilizes the estimation of code length, and achieves minimax-optimal redundancy in universal coding (see Section \ref{sec:discretization_continuous_variables}). Intuitively, it can be interpreted as adding a "half-count" to every category or bin, ensuring that even rarely observed or unobserved outcomes contribute non-zero probability mass. The resulting compressed length is then
% \[
% \hat{K}_C(\mathbf{x}) \;=\; - \sum_{i=1}^B c_i \log_2 p_i.  
% \]

% \begin{example}
% Consider the sample
% \[
% \mathbf{x} = \{-1.2,\,-0.8,\,-0.5,\,-0.1,\,0.0,\,0.2,\,0.4,\,0.9,\,1.1,\,1.3,\,1.5,\,4.5\}  
% \]
% with $n=12$, which contains an outlier at $4.5$. We discretize using equal-width bins, with the bin count determined by Rice's rule:
% \[
% B_{\text{core}} \;\approx\; \lceil 2\,n^{1/3}\rceil \;=\; \lceil 2 \cdot 12^{1/3}\rceil \;\approx\; \lceil 4.58\rceil \;=\; 5.  
% \]
% Next, we apply a small trimming factor $\alpha=0.05$. The core range is then defined as
% \[
% [a,b] \;=\; \bigl[ Q_{0.05}(x),\,Q_{0.95}(x) \bigr].  
% \]
% For this sample, the outlier $4.5$ falls into the right tail, and the core can be taken as $[a,b] = [-1.2,,1.5]$. We divide this interval into $B_{\text{core}}=5$ equal-width bins of size $h=(b-a)/5=0.54$:
% \[
% [-1.2,-0.66),\;[-0.66,-0.12),\;[-0.12,0.42),\;[0.42,0.96),\;[0.96,1.5].  
% \]
% We also add two overflow bins, $(-\infty,a)$ and $(b,+\infty)$. With a minimum occupancy threshold $m_{\min}=1$, all bins contain at least one observation, so no adjustment of $B_{\text{core}}$ is required. Assigning each $x_j$ to its bin (core bins are half-open except for the last) yields:
% \[
% \text{Left tail}=0,\quad  
% \text{Core}=(2,\,1,\,4,\,1,\,3),\quad  
% \text{Right tail}=1.  
% \]
% Thus, the 7-bin count vector is
% \[
% (c_1,\ldots,c_7) \;=\; (0,\,2,\,1,\,4,\,1,\,3,\,1), \qquad B=7,\; n=12.  
% \]
% Applying KT smoothing gives
% \[
% p_i \;=\; \frac{c_i + \tfrac12}{n + \tfrac{B}{2}}  
% \;=\; \frac{c_i + \tfrac12}{12+3.5}  
% \;=\; \frac{c_i + \tfrac12}{15.5},  
% \]
% so the smoothed probabilities are approximately
% \[
% p \;\approx\; \bigl(0.0323,\,0.1613,\,0.0968,\,0.2903,\,0.0968,\,0.2258,\,0.0968 \bigr).  
% \]
% Finally, the optimal data code length is
% \[
% \widehat{K}_C(\mathbf{x}) \;=\; -\sum_{i=1}^{B} c_i \log_2 p_i  
% \;\approx\; 28.9 \;\text{bits.}  
% \]
% \end{example}

% When compressing multiple variables jointly, for example $\mathbf{x}$ and $\mathbf{y}$, we first discretize each variable independently (if needed) using its own binning scheme, and then combine them into joint symbols. To preserve comparability, we use a \emph{mixed-radix encoding}\index{Mixed-radix encoding}: if $\mathbf{x}$ has $B_x$ bins and $\mathbf{y}$ has $B_y$, a pair $(i,j)$ is mapped to the integer
% \[
% z \;=\; i + B_x j.  
% \]
% This approach generalizes naturally to higher dimensions, providing a bijection between joint bins and integers while preserving the underlying probability distributions.

% \begin{example}
% Let $\mathbf{x}$ and $\mathbf{y}$ be two samples of size $n$. Each axis is discretized independently, following the procedure described above, yielding $B_x$ and $B_y$ bins respectively (with the same trimming and occupancy rules applied). The joint space is then partitioned into $B_x \times B_y$ cells. To encode these joint cells, we use a mixed-radix mapping, where each pair $(i,j)$ is mapped to a single index $z = i + B_x j$.

% Let $c_{ij}$ denote the joint counts and $c_z$ the corresponding counts over the mapped indices. With KT smoothing, the joint code length is given by
% \[
% \widehat{K}_C(\mathbf{x},\mathbf{y})  
% \;=\;  
% -\sum_{z=1}^{B_x B_y} c_z \log_2\!\left(\frac{c_z+\tfrac12}{n+\tfrac{B_x B_y}{2}}\right).  
% \]
% By using the same per-axis binning for $\mathbf{x}$, $\mathbf{y}$, and their joint grid, we ensure comparability under a consistent encoding scheme (the same “machine”). In practice, the relation
% \[
% \widehat{K}_C(\mathbf{x},\mathbf{y}) \;\lesssim\; \widehat{K}_C(\mathbf{x}) + \widehat{K}_C(\mathbf{y})  
% \]
% holds up to small modeling constants, in accordance with the chain rule (see Section \ref{prop:kolmogorov_chain_rule}).
% \end{example}

%
% Section: Miscoding
%

\section{Miscoding}
\label{sec:machine_learning_miscoding}

In Section \ref{def:miscoding} we introduced the concept of miscoding as a quantitative measure of how well a string based encoding $r \in \mathcal{R}$ represents a research entity from $\mathcal{E}$. The miscoding of a representation $r$ was defined as:
\[
\mu(r) = \overset{o}{ \underset{s \in \mathcal{R}_\mathcal{E}} \min} \frac{ \max\{ K(s \mid r), K(r \mid s) \} } { \max\{ K(s), K(r) \} }
\]
and we saw that this quantity cannot be computed in practice in the general case: first, because it would require computations from an abstract oracle machine; second, because it relies on the uncomputable Kolmogorov complexity; and third, because it does not directly take into account the particular entity $e$ we are interested in. 

In this section, we will show how this concept can be adapted in practice to estimate the error made when using a dataset $\mathbf{X}$ as a representation of a response variable $\mathbf{y}$ (see Section \ref{sec:machine_learning}). Our goal is twofold: on one hand, we are interested in measuring the overall quality of the dataset $\mathbf{X}$ as a predictor of the variable $\mathbf{y}$; on the other, we want to identify those individual features $\mathbf{x}_j$ of $\mathbf{X}$ that have the highest predictive power for $\mathbf{y}$. This is precisely the problem addressed by discriminative models (see Section \ref{sec:generative_discriminative}), in which we want to estimate the conditional distribution $P( \mathbf{y} \mid \mathbf{X} )$.

Given a training dataset $\mathbf{X}$, we can approximate the miscoding of a feature $\mathbf{x}_j$ for the target variable $\mathbf{y}$ by computing the normalized information distance between $\mathbf{x}_j$ and $\mathbf{y}$ (see Section \ref{sec:information_distance}):
\[
E(\mathbf{x}_j, \mathbf{y}) = \frac{\max\{ K(\mathbf{x}_j \mid \mathbf{y}), K(\mathbf{y} \mid \mathbf{x}_j) \}}{\max \{ K(\mathbf{x}_j), K(\mathbf{y}) \} }
\]
The Kolmogorov complexity $K(\mathbf{v})$ of a vector $\mathbf{v}$ will be approximated by the length of the compressed version of that vector $\hat{K}_C(\mathbf{v})$ using as compressor a minimal length code $C$ (see Section \ref{sec:note_about_compression}).

\begin{definition}
Let $\mathbf{y}$ be a response variable, $\mathbf{X}$ a dataset composed by $p$ features, and $\mathbf{x}_j$ the $j-th$ feature. We define the regular feature miscoding of $\mathbf{x}_j$ as a representation of $\mathbf{y}$, denoted by $\hat\mu(\mathbf{x}_j, \mathbf{y})$, as:
\[
\hat\mu(\mathbf{x}_j, \mathbf{y}) = \frac{ \hat{K}_C(\mathbf{x}_j, \mathbf{y}) - \min\{ \hat{K}_C(\mathbf{x}_j), \hat{K}_C(\mathbf{y}) \} } { \max\{ \hat{K}_C(\mathbf{x}_j), \hat{K}_C(\mathbf{y}) \} }
\]\end{definition}

Intuitively, the quantity $\hat\mu(\mathbf{x}_j, \mathbf{y})$ measures, in relative terms, the additional description length required to fully encode $\mathbf{y}$ given knowledge of $\mathbf{x}_j$, and vice versa. The lower this value, the better $\mathbf{x}_j$ serves as a predictor for $\mathbf{y}$.

\begin{example}

Let $\mathbf{y}$ be a target variable composed of $1{,}000$ random samples from a normal distribution $N(3,1)$ with mean $\mu = 3$ and standard deviation $\sigma = 1$; let $\mathbf{x}_1$ be a predictor feature approximately equal to $\mathbf{y}$ with some added noise, that is $\mathbf{x}_1 = \mathbf{y} + N(3, 1) / 10$; and let $\mathbf{x}_2$ be a second predictor consisting of random samples from an exponential distribution with rate $\lambda = 1$.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from scipy.stats import norm, expon

y  = norm.rvs(loc=3, scale=1, size=10000)
x1 = y + norm.rvs(loc=3, scale=1, size=10000) / 10
x2 = expon.rvs(size=10000)
\end{verbatim}}
\end{sourcecode}

We can use the Nescience library to compute the miscoding of the features $\mathbf{x}_1$ and $\mathbf{x}_2$ when they encode the target variable $\mathbf{y}$.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from fastautoml.miscoding import Miscoding
import numpy as np

X = np.column_stack((x1, x2))

miscoding = Miscoding()
miscoding.fit(X, y)
miscoding.miscoding_features(mode="regular")
\end{verbatim}}
\end{sourcecode}

The output of the library would be something similar to the following\footnote{Since we are generating a list of $1.000$ random samples, the reader could get a slightly different result when running this example.}:

\begin{sourcecode}
{\scriptsize \begin{verbatim}
array([0.27445364, 0.9934222])
\end{verbatim}}
\end{sourcecode}

As it was expected the miscoding of $\hat\mu(\mathbf{x}_1, \mathbf{y})$ is much smaller than the miscoding of $\hat\mu(\mathbf{x}_2, \mathbf{y})$. In this case, we should prefer $\mathbf{x}_1$ over $\mathbf{x}_2$ as a predictor of $\mathbf{y}$.

\end{example}

Sometimes, we will use the normalized version of the complements of the individual miscodings, that is:
\[
\frac{ 1 - \hat\mu(\mathbf{x}_i, \mathbf{y}) } { \sum_{j=1}^p \big(1 - \hat\mu(\mathbf{x}_j, \mathbf{y})\big) },
\]
instead of the raw values $\hat\mu(\mathbf{x}_i, \mathbf{y})$. This alternative, which we call the \emph{adjusted feature miscoding}, is useful because it allows easier comparison with other feature selection techniques, and it has a visually intuitive interpretation as relative importance weights.

\begin{example}
\label{example:gaussian_blob_miscoding}
In this example we generate a synthetic dataset where the target variable $\mathbf{y}$ is a collection of normally distributed clusters of points, and the training set $\mathbb{X}$ is composed of both relevant and irrelevant predictors. In particular, we generate $1{,}000$ samples with $20$ features describing $10$ clusters; only $4$ of the features are relevant for prediction, and the remaining $16$ are just random noise.

Figure \ref{figure:gaussian_blob_cluster} shows a two-dimensional projection of this dataset along the hyperplane composed of features 8 and 10.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{gaussian_blob_cluster.png}
\caption{Gaussian Blob Cluster.}
\label{figure:gaussian_blob_cluster}
\end{figure}

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from fastautoml.miscoding import Miscoding
from sklearn.datasets.samples_generator import make_classification

X, y = make_classification(n_samples=1000, n_features=20, n_informative=4,
       n_redundant=0, n_classes=10, n_clusters_per_class=1, flip_y=0)

miscoding = Miscoding()
miscoding.fit(X, y)
msd = miscoding.miscoding_features(mode='adjusted')
\end{verbatim}}
\end{sourcecode}

We use the adjusted version of miscoding for easier comparison with other feature selection techniques. If we plot the results (see Figure \ref{figure:miscoding_make_classification}) we can see that the library has successfully identified the four relevant predictors ($\mathbf{x}_3$, $\mathbf{x}_8$, $\mathbf{x}_{10}$ and $\mathbf{x}_{16}$). Since we are using the adjusted version, higher values are better; note that the actual values should be interpreted only in relative terms. 

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{feature_miscoding.png}
\caption{Miscoding of a Synthetic Dataset.}
\label{figure:miscoding_make_classification}
\end{figure}

We can compare miscoding with correlation, a common technique used in machine learning to identify relevant features. Figure \ref{figure:correlation_make_classification} shows the correlation between the individual features of $\mathbf{X}$ and the target variable $\mathbf{y}$. As we can observe, correlation fails to properly identify one of the relevant features ($\mathbf{x}_3$).

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{feature_correlation.png}
\caption{Correlation of a Synthetic Dataset.}
\label{figure:correlation_make_classification}
\end{figure}

\end{example}

Feature miscoding allows us to identify the most relevant features of a training dataset $\mathbf{X}$, but it cannot be used directly to compute the miscoding of the dataset itself. If we start with a miscoding of $1$ (complete ignorance) and subtract the miscodings of the individual features, we may end up with a negative miscoding, which is not allowed by our theory. If we use the adjusted version, the dataset miscoding would be $0$ for all datasets, which goes against the intuition that not all datasets $\mathbf{X}$ represent a target variable $\mathbf{y}$ equally well. According to the theory of nescience, we expect non-relevant features to \emph{add}, rather than subtract, to the global miscoding of the dataset.

To address this, we introduce the concept of partial miscoding of a feature, defined as the difference between the adjusted and the normalized miscodings.

\begin{definition}
Let $\mathbf{y}$ be a target variable, $\mathbf{X}$ a dataset composed by $p$ features, and $\mathbf{x}_j$ the $j-th$ feature. We define the partial miscoding of $\mathbf{x}_j$ as a representation of $\mathbf{y}$, denoted by $\tilde\mu(\mathbf{x}_j, \mathbf{y})$, as:
\[
\tilde\mu(\mathbf{x}_i, \mathbf{y}) = \frac{ 1 - \hat\mu(\mathbf{x}_i, \mathbf{y}) } { \sum_{j=1}^p 1 - \hat\mu(\mathbf{x}_j, \mathbf{y}) } - \frac{\hat\mu(\mathbf{x}_i, \mathbf{y}) } { \sum_{j=1}^p \hat\mu(\mathbf{x}_j, \mathbf{y}) }
\]
\end{definition}

A positive partial miscoding indicates that the feature contributes to describing the target variable; a negative value indicates that the feature is not relevant (or is counterproductive) in representing $\mathbf{y}$.

\begin{example}
\label{example:partial_feature_miscoding}
We reuse the synthetic dataset of Example \ref{example:gaussian_blob_miscoding}, but we increase the number of relevant features from $4$ to $14$. Then, we compute the list of partial miscodings.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from fastautoml.miscoding import Miscoding
from sklearn.datasets.samples_generator import make_classification

X, y = make_classification(n_samples=1000, n_features=20, n_informative=14,
       n_redundant=0, n_classes=10, n_clusters_per_class=1, flip_y=0)

miscoding = Miscoding()
miscoding.fit(X, y)
msd = miscoding.miscoding_features(mode="partial")
\end{verbatim}}
\end{sourcecode}

As shown in Figure \ref{figure:partial_feature_miscoding}, the library not only correctly identifies the relevant features, but also assigns negative contributions to non-relevant features, thereby increasing the global miscoding when they are included.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{partial_miscoding.png}
\caption{Partial Feature Miscoding.}
\label{figure:partial_feature_miscoding}
\end{figure}

\end{example}

Given the definition of partial feature miscoding, we can now define the miscoding of a target variable given a subset of predictors in a way that is closer to the original notion of miscoding in the theory of nescience.

\begin{definition}
Let $\mathbf{y}$ be a target variable, $\mathbf{X} = \{ \mathbf{x}_1, \ldots, \mathbf{x}_p \}$ a dataset composed by $p$ features, and $\mathbf{Z} = \{ \mathbf{z}_1, \ldots, \mathbf{z}_k \}$ a subset of features, that is, $\{ \mathbf{z}_1, \ldots, \mathbf{z}_k \} \subseteq \{ \mathbf{x}_1, \ldots, \mathbf{x}_p \}$. We define the miscoding of $\mathbf{Z}$ as a representation of $\mathbf{y}$, denoted by $\hat\mu(\mathbf{Z}, \mathbf{y})$, as:
\[
\hat\mu(\mathbf{Z}, \mathbf{y}) = \sum_{i=1}^k \tilde\mu (\mathbf{z}_i, \mathbf{y})
\]
\end{definition}

\begin{example}
\label{example:accumulated_partial_feature_miscoding}
Using the dataset and partial feature miscodings computed in Example \ref{example:partial_feature_miscoding}, Figure \ref{figure:accumulated_partial_feature_miscoding} shows the evolution of the miscoding of the training subset $\mathbf{Z}$ as we add more features to the study.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{accumulated_partial_miscoding.png}
\caption{Accumulated Partial Feature Miscoding.}
\label{figure:accumulated_partial_feature_miscoding}
\end{figure}

\end{example}

In the next example we compare the performance of a classifier when using the full dataset and a reduced version containing only those features identified as relevant (i.e., with positive partial miscoding).

\begin{example}
We train a neural network on the \texttt{scikit-learn} digits dataset to classify handwritten digits. The evaluation criterion is the classifier's accuracy on a held-out test set. The neural network is trained and evaluated first on all features, and then on a reduced dataset containing only those features with positive partial miscoding.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
import numpy  as np
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import load_digits
from fastautoml.fastautoml import Miscoding

data = load_digits()
X_raw = data.data
y_raw = data.target

miscoding = Miscoding()
miscoding.fit(X_raw, y_raw)
mscd = miscoding.miscoding_features(miscoding='partial')
X_red = X_raw[:,np.where(mscd > 0)[0]]
y_red = y_raw

X_raw_train, X_raw_test, y_raw_train, y_raw_test = train_test_split(X_raw,
             y_raw, test_size=.3)
X_red_train, X_red_test, y_red_train, y_red_test = train_test_split(X_red,
             y_red, test_size=.3)

clf = MLPClassifier(alpha=1, max_iter=1000)

clf.fit(X_raw_train, y_raw_train)
score_raw = clf.score(X_raw_test, y_raw_test)
        
clf.fit(X_red_train, y_red_train)
score_red = clf.score(X_red_test, y_red_test)
        
reduction = 1 - X_red_train.shape[1] / X_raw_train.shape[1]

print("Score raw:", score_raw, " Score Miscoding:", score_red,
      " Reduction:", reduction)

Score raw: 0.9833333333333333  Score Miscoding: 0.9814814814814815  Data Reduction: 0.46875
\end{verbatim}}
\end{sourcecode}

Running the above code typically shows that the classifier’s accuracy is essentially the same for both datasets (about $98\%$ on the test set). However, the reduced dataset used for training (selected via positive partial miscoding) is about $47\%$ smaller than the original dataset. This size reduction can substantially decrease training time. Smaller datasets are also beneficial when training ensembles (e.g., random forests), where many models must be fitted.

\end{example}

Intuitively, as Example \ref{example:accumulated_partial_feature_miscoding} suggests, we should prefer the subset $\mathbf{Z} \subseteq \mathbf{X}$ composed of those features whose partial miscodings are greater than zero. However, as we will see in the following sections, this might not always be the case. Feature selection is only one of the criteria used when searching for an optimal model for an entity represented by a dataset; other components of nescience, such as inaccuracy or surfeit, may suggest a different subset of predictors. The overall optimization criterion is the nescience itself. A sensible approach to using partial miscoding is to incrementally add to the model those features with the highest (positive) partial miscoding until all positive features are included or an external optimality criterion is reached.

In the case of a generative model (see Section \ref{sec:generative_discriminative}), i.e., a machine learning algorithm designed to estimate the joint probability $P(\mathbf{X}, \mathbf{y})$, we can use miscoding to assess how individual features relate to each other, by computing $\hat\mu(\mathbf{x}_i, \mathbf{x}_j)$ for each pair $i, j \leq p$. The result is a \emph{miscoding matrix} (see Example \ref{example:miscoding_boston}).

\begin{example}
\label{example:miscoding_boston}
The Boston dataset included in the \texttt{scikit-learn} library contains a collection of variables that (potentially) explain the price of houses in the Boston area. In this example, instead of identifying the factors that contribute most to price, we study the interdependence among factors using a miscoding matrix.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from fastautoml.fastautoml import Miscoding
from sklearn.datasets import load_boston

data = load_boston()

miscoding = Miscoding(X_type="numeric", y_type="numeric")
miscoding.fit(data.data, data.target)
mscd_matrix = miscoding.features_matrix(mode='regular')
\end{verbatim}}
\end{sourcecode}

Figure \ref{figure:miscoding_matrix} shows a heatmap of the miscoding matrix computed over the features. Darker values represent lower miscoding (note we use the regular version here). In particular, the diagonal entries are equal to zero. 

\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{miscoding_matrix.png}
\caption{Regular Miscoding Matrix.}
\label{figure:miscoding_matrix}
\end{figure}

The minimum value of $0.52$ is obtained for the pair $(8, 9)$, corresponding to the features "index of accessibility to radial highways" and "full-value property-tax rate per \$10,000". These features are good candidates to evaluate jointly in a predictive model, since they contain non-redundant information. The maximum value of $0.99$ is achieved for the pair $(3, 12)$, corresponding to "Charles River dummy variable" and "\% lower status of the population". These features contain almost the same information; including both in a model adds little new information while increasing model complexity and the risk of overfitting.

\end{example}

%
% Section: Inaccuracy
%

\section{Inaccuracy}
\label{sec:machine_learning_inaccuracy}

In Section \ref{sec:inaccuracy:inaccuracy} we defined the inaccuracy of a description $d \in \mathcal{D}$ for a representation $r \in \mathcal{R}$ as the normalized information distance between the representation $r$ and the string $\Gamma(d)$ printed out by a universal Turing machine when given the description as input:
\[
\iota(d, r) = \frac{ \max\{ K \left(r \mid \Gamma(d) \right), K \left( \Gamma(d) \mid r \right) \} } { \max\{ K(r), K \left(\Gamma(d) \right) \} }
\]
Inaccuracy, being based on Kolmogorov complexity, is not computable in the general case and must be approximated in practice. In this section we show how this concept can be estimated for a model trained on a dataset. The approach parallels the one used for miscoding (see Section \ref{sec:approx-miscoding}).

For discriminative models, which estimate $P(\mathbf{y} \mid \mathbb{X})$, the natural proxy compares predicted targets $\hat{\mathbf{y}}$ with true targets $\mathbf{y}$ via normalized compression distance.

\begin{definition}
Let $\mathbb{X}$ be a dataset, $\mathbf{y}$ a response variable, $m$ a model, and $\mathbf{\hat{y}} = m(\mathbb{X})$ the predicted values by $m$ given $\mathbb{X}$. We define the \emph{inaccuracy}\index{Inaccuracy} of the model $m$ for the target values $\mathbf{y}$, denoted by $\hat\iota(\mathbf{\hat{y}}, \mathbf{y})$, as:
\[
\hat\iota(\mathbf{\hat{y}}, \mathbf{y}) = \frac{ \hat{K}_C(\mathbf{\hat{y}}, \mathbf{y}) - \min\{ \hat{K}_C(\mathbf{\hat{y}}), \hat{K}_C(\mathbf{y}) \} } { \max\{ \hat{K}_C(\mathbf{\hat{y}}), \hat{K}_C(\mathbf{y}) \} }
\]
\end{definition}

Intuitively, $\hat\iota(\hat{\mathbf{y}}, \mathbf{y})$ measures how far the predictions are from the true values. The lower this quantity, the better $m$ is as a predictor of $\mathbf{y}$. Unlike standard accuracy metrics, this measure accounts not only for how difficult it is to reconstruct $\mathbf{y}$ from $\hat{\mathbf{y}}$, but also for how much additional information $\hat{\mathbf{y}}$ contains that is unrelated to $\mathbf{y}$.

For generative models, which aim to capture the joint distribution $P(\mathbb{X}, \mathbf{y})$, we compare the observed joint sample with a synthetic joint sample generated by the model.

\begin{definition}
Let $m$ be a generative model for $(\mathbb{X}, \mathbf{y})$. Let $(\hat{\mathbb{X}}, \hat{\mathbf{y}})$ be a synthetic sample of the same size as $(\mathbb{X}, \mathbf{y})$ generated by $m$ (after applying the same encoding/discretization scheme). The \emph{generative inaccuracy} of $m$ for $(\mathbb{X}, \mathbf{y})$ is:
\[
\hat\iota_g\!\left(m; \, (\mathbb{X}, \mathbf{y})\right)
= \frac{ \hat{K}_C\!\left(\langle \mathbb{X}, \mathbf{y} \rangle , \langle \hat{\mathbb{X}}, \hat{\mathbf{y}} \rangle\right)
- \min\!\left\{ \hat{K}_C\!\left(\langle \mathbb{X}, \mathbf{y} \rangle\right), \hat{K}_C\!\left(\langle \hat{\mathbb{X}}, \hat{\mathbf{y}} \rangle\right) \right\} }
{ \max\!\left\{ \hat{K}_C\!\left(\langle \mathbb{X}, \mathbf{y} \rangle\right), \hat{K}_C\!\left(\langle \hat{\mathbb{X}}, \hat{\mathbf{y}} \rangle\right) \right\} } .
\]
\end{definition}

When a model provides explicit probabilities, one may also define $\hat{K}$ using the model code length $-\sum \log_2 \hat{p}_m(\cdot)$; we keep the compression-based proxy here for consistency with the rest of the chapter.

\begin{example}
\label{ex:machine_learning:inaccuracy:inaccuracy_DT}
According to the minimum nescience principle, inaccuracy is the normalized compression distance between the actual targets $\mathbf{y}$ and the predicted targets $\hat{\mathbf{y}}$. We compare our inaccuracy metric with a classical score (we use $1 - \text{accuracy}$) on the MNIST\index{MNIST} digits dataset.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from fastautoml.fastautoml import Inaccuracy
from sklearn.datasets import load_digits

X, y = load_digits(return_X_y=True)

inacc = Inaccuracy()
inacc.fit(X, y)
\end{verbatim}}
\end{sourcecode}

We train a decision-tree classifier with maximum depth $i \in \{1,\ldots,20\}$:

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from sklearn.tree import DecisionTreeClassifier

scores       = list()
inaccuracies = list()

for i in range(20):
    
    tree = DecisionTreeClassifier(max_depth=i, random_state=42)
    tree.fit(X, y)
    
    scores.append(1 - tree.score(X, y))
    inaccuracies.append(inacc.inaccuracy_model(tree))
\end{verbatim}}
\end{sourcecode}

We compare $1-\text{accuracy}$ and inaccuracy. As shown in Figure \ref{figure:machine_learning:inaccuracy:inaccuracy_DT}, both behave similarly, with inaccuracy typically taking larger values due to its stronger emphasis on incorrectly predicted values.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{inaccuracy_DT.png}
\caption{Inaccuracy vs. Score of Decison Trees}
\label{figure:machine_learning:inaccuracy:inaccuracy_DT}
\end{figure}

\end{example}

In Example \ref{ex:machine_learning:inaccuracy:inaccuracy_DT} we observed that deeper trees yield smaller training error. Of course, larger depth increases the risk of overfitting\index{Overfitting}. However, within the nescience framework, overfitting is handled by \emph{surfeit} (see Section \ref{sec:machine_learning:surfeit}), so inaccuracy need not penalize complexity directly.

We can interpret inaccuracy as the effort, measured in description length, required to \emph{fix} a model's predictions. Consequently, a model that makes one hundred \emph{identical} errors is easier to correct than a model that makes one hundred \emph{distinct} errors (see Example \ref{ex:machine_learning:inaccuracy:one_hundred_errors}).

\begin{example}
\label{ex:machine_learning:inaccuracy:one_hundred_errors}

We again use a decision-tree classifier, now with the hyperparameter \texttt{min\_samples\_leaf}=5 (a common regularization to reduce overfitting).

\begin{sourcecode}
{\scriptsize \begin{verbatim}
tree = DecisionTreeClassifier(min_samples_leaf=5)
tree.fit(X, y)
\end{verbatim}}
\end{sourcecode}

The inaccuracy of this new trained model is $0.17$, and its score $0.08$. Next we will artificially introduce one hundred errors in the dataset, simulating the case that the tree is not able to model correctly these data points. In this particular case all the errors are exactly the same.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
X2 = X.copy()
y2 = y.copy()
for i in range(100):
    X2 = np.append(X2, [X[0]], axis=0)
    y2 = np.append(y2, (y[0]+1) % 10)
\end{verbatim}}
\end{sourcecode}

The inaccuracy of the decision tree, given this new dataset, has increased\footnote{Note that we had to \texttt{fit()} again the class Inaccuracy in order to use the new dataset. Normally this is not the way we use this class; instead what we should do is to fit once a dataset, and then compute the inaccuracy of different models. We are doing here in this way to demonstrate an interesting property of the concept of inaccuracy.} from $0.17$ to $0.21$.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
inacc.fit(X2, y2)
inacc.inaccuracy_predictions(pred)
\end{verbatim}}
\end{sourcecode}

Score has also increased, in this case from $0.08$ to $0.13$.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
1 - tree.score(X2, y2)
\end{verbatim}}
\end{sourcecode}

Finally, we are going to repeat exactly the same experiment, but this time instead of adding one hundred times the same error, adding one hundred different errors.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
X3 = X.copy()
y3 = y.copy()
for i in arange(100):
    index = np.random.randint(X.shape[0])
    X3    = np.append(X3, [X[index]], axis=0)
    y3    = np.append(y3, (y[index]+1) % 10)
\end{verbatim}}
\end{sourcecode}

In this last case the inaccuracy of the model has increased up to $0.25$, meanwhile score remained the same.
\end{example}

In line with Example \ref{ex:machine_learning:inaccuracy:one_hundred_errors}, consider an extreme binary case: a model that always flips the label (predicts False when the truth is True, and vice versa). Classical accuracy deems this the worst possible model; our inaccuracy, however, is low because the predictions can be perfectly "fixed" by a simple transformation. In some applications (e.g., a hedge fund\index{Stock market} trading strategy), such a model would be extremely valuable.

In highly imbalanced datasets\index{Imbalanced dataset}, standard accuracy can be misleading: a high score may simply reflect correct classification of the majority class while failing on the minority class. Practitioners address this with specialized metrics. With our inaccuracy metric, as Example \ref{ex:machine_learning:inaccuracy:unbalanced_dataset} shows, a model that fails to classify the minority class is penalized even if that class has few samples.

\begin{example}
\label{ex:machine_learning:inaccuracy:unbalanced_dataset}
We create a synthetic dataset with two classes, using the \texttt{make\_classification} utility of scikit-learn, where one has $95\%$ of the samples and the other $5\%$:

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from sklearn.datasets import make_classification

depth = list()
score = list()
inacc = list()

inaccuracy = Inaccuracy()

for i in np.arange(1, 100):
                    
    X, y = make_classification(n_samples=1000, n_features=2,
                               n_informative=2, n_redundant=0,
                               class_sep=2, flip_y=0, weights=[0.95,0.05])

    inaccuracy.fit(X, y)
        
    tree = DecisionTreeClassifier(min_samples_leaf=i)
    tree.fit(X, y)

    depth.append(i)        
    score.append(1 - tree.score(X, y))
    inacc.append(inaccuracy.inaccuracy_model(tree))
\end{verbatim}}
\end{sourcecode}

We vary the minimum number of samples per leaf $i \in \{1,\ldots,100\}$. In Figure \ref{figure:machine_learning:inaccuracy:inaccuracy_DT2}, for large $i$ the accuracy-based error (i.e., $1-\text{accuracy}$) may stay near $0.05$, yet the inaccuracy remains high because the model effectively ignores the minority class.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{inaccuracy_DT2.png}
\caption{Inaccuracy of Decision Tree.}
\label{figure:machine_learning:inaccuracy:inaccuracy_DT2}
\end{figure}

\end{example}

%
% Section: Surfeit
%

\section{Surfeit}
\label{sec:machine_learning_surfeit}

In Section \ref{sec:Definition_redundancy} we defined the surfeit of the model $m \in \mathcal{M}$ for a representation $r \in \mathcal{R}$ as:
\[
\sigma (m, r) = 1 - \frac{K(r)}{l(m)}
\]
Since the length $K(r)$ of the shortest possible description of $r$ (its Kolmogorov complexity) is in general unknown, we must approximate this concept in practice. In the setting of a training dataset $\mathbb{X}$ and a target variable $\mathbf{y}$, we approximate the surfeit of a model $m$ for the representation $\mathbf{y}$ by
\[
\hat\sigma(m, \mathbf{y}) = 1 - \frac{\hat{K}_C(\mathbf{y})}{l(m)} ,
\]
where $\hat{K}_C(\mathbf{y})$ is the length of the compressed version of the vector $\mathbf{y}$ using a minimal-length code $C$ determined by the empirical relative frequencies of the values observed in $\mathbf{y}$ (see Section \ref{sec:approx-miscoding}).

\begin{definition}
Let $\mathbf{y}$ be a response variable, and $\mathbb{X}$ a dataset composed of $p$ features and $n$ samples. We define the surfeit of the model $m \in \mathcal{M}$ as a representation of $\mathbf{y}$, denoted by $\hat\sigma(m, \mathbf{y})$, as
\[
\hat\sigma(m, \mathbf{y}) = 1 - \frac{\hat{K}_C(\mathbf{y})}{l(m)} .
\]
\end{definition}

This definition requires a method to encode models as strings of symbols so that their length $l(m)$ can be computed. Ideally, one would encode models as Turing machines and agree on a universal Turing machine to interpret them. However, this would make it very difficult to add new models to the \texttt{nescience} library. Instead, we use a restricted subset of the Python language as the model-encoding scheme: only a limited set of constructions is allowed, and third-party libraries are disallowed.

Surfeit can help avoid overfitted models. The higher the surfeit of a model, the greater the risk that the model is an overfit to the training dataset, as Example \ref{ex:surfeit_overfit} illustrates.

\begin{example}
\label{ex:surfeit_overfit}
We generate $900$ samples from a sinusoidal curve and fit $n$-degree polynomials with $n \in \{1,\ldots,15\}$.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

from Nescience.Nescience import Surfeit
from Nescience.Nescience import Inaccuracy

n_samples = 900
degrees = np.arange(1, 15)

X = np.sort(np.random.rand(n_samples) * 3)
y = np.cos(1.5 * np.pi * X)

linacc   = list()
lsurfeit = list()

for i in degrees:
        
    poly = PolynomialFeatures(degree=i, include_bias=False)
    newX = poly.fit_transform(X[:, np.newaxis])
    
    linear_regression = LinearRegression()
    linear_regression.fit(newX, y)

    inacc.fit(newX, y)
    inaccuracy = inacc.inaccuracy_model(linear_regression)
    
    sft.fit(newX, y)
    surfeit = sft.surfeit_model(linear_regression)
    
    linacc.append(inaccuracy)
    lsurfeit.append(surfeit)
\end{verbatim}}
\end{sourcecode}

In Figure \ref{figure:surfeit_vs_inaccuracy} we see that, as expected, higher-degree polynomials yield lower training error, but at the cost of higher surfeit. The preferred model is one achieving low inaccuracy \emph{and} low surfeit.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{surfeit_vs_inaccuracy.png}
\caption{Surfeit vs Inaccuracy}
\label{figure:surfeit_vs_inaccuracy}
\end{figure}

\end{example}

Another advantage of surfeit is that it allows us to compare models from different families. For example, if two models have the same accuracy, should we prefer a decision tree to a neural network, or a naive Bayes classifier to a support vector machine? The next example shows how surfeit can guide such choices.

\begin{example}
\label{ex:dt_vs_nn}

We compare a decision tree with a neural network on synthetic data consisting of two isotropic Gaussian blobs. First, with standard deviation $1$ and two dimensions, the two clusters are easily separable (Table \ref{tab:isotropic_gaussian_blobs}, left).

\begin{table}
\begin{center}

\begin{tabular}{ c c }

\includegraphics[scale=0.4]{blobs_easy_split} & \includegraphics[scale=0.4]{blobs_difficult_split}

\end{tabular}
\end{center}
\caption{\label{tab:isotropic_gaussian_blobs}Isotropic Gaussian blobs.}
\end{table}

% \raisebox{.4\height}{\includegraphics[scale=0.4]{blobs_difficult_split}}

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from Nescience.Nescience import Surfeit
from Nescience.Nescience import Inaccuracy
from sklearn.datasets.samples_generator import make_blobs

X, y = make_blobs(n_samples=1000, centers=2, n_features=2, cluster_std=1)

tree = DecisionTreeClassifier()
tree.fit(X, y)
tree.score(X, y)

nn = MLPClassifier()
nn.fit(X, y)
nn.score(X, y)

sft = Surfeit()
sft.fit(X, y)

sft.surfeit_model(tree)
sft.surfeit_model(nn)

\end{verbatim}}
\end{sourcecode}

If we run the code, both models typically achieve accuracy $1$. However, the decision tree often has substantially lower surfeit than the neural network (e.g., $0.25$ vs.\ $0.73$ in one run), so we would prefer the decision tree in this setting.

If we repeat the experiment with standard deviation $3$ and higher dimensionality (Table \ref{tab:isotropic_gaussian_blobs}, right), the situation can change:

\begin{sourcecode}
{\scriptsize \begin{verbatim}
X, y = make_blobs(n_samples=10000, centers=2, n_features=8, cluster_std=3)
\end{verbatim}}
\end{sourcecode}

In this second case, both models can again achieve very high accuracy, but the surfeit of the decision tree typically increases markedly (e.g., to $\approx 0.82$), while the neural network's surfeit may remain similar (e.g., $\approx 0.76$). For this dataset, we would then prefer the neural network.

\end{example}

In Example \ref{ex:dt_vs_nn} we assumed equal accuracy. When accuracies differ, we must appeal to the overall concept of \emph{nescience} to decide between models, jointly considering inaccuracy and surfeit (and, where relevant, miscoding).

%
% Section: Nescience
%

\section{Nescience}
\label{sec:machine_learning_nescience}

In Chapter \ref{chap:Nescience} we defined the concept of nescience as the solution to a nonlinear multiobjective optimization problem, in which we minimize the miscoding, inaccuracy, and surfeit of representations and models. The solution to this problem is, in general, not unique: we can often find multiple pairs (representation, model) such that none of the three quantities can be improved without degrading at least one of the others (Pareto optimality). However, in practice we expect a machine-learning library to provide a single solution when training a model on a dataset. To provide a unique solution, we resort to a utility function that selects one element from the Pareto-optimal set. The \texttt{nescience} library offers several utility functions; the default is the arithmetic mean of the three metrics.

\begin{remark}
The \texttt{nescience} library implements the following utility functions to approximate the concept of nescience, i.e., to compute $\hat\nu\!\left(\mathbb{Z}, m, \mathbf{y} \right)$:

\begin{itemize}
\item Euclidean distance: $\left( \hat\mu(\mathbb{Z}, \mathbf{y})^2 + \hat\iota(\hat{y}, \mathbf{y})^2  + \hat\sigma(m, \mathbf{y})^2 \right)^{1/2}$
\item Arithmetic mean: $\frac{\hat\mu(\mathbb{Z}, \mathbf{y}) + \hat\iota(\hat{y}, \mathbf{y}) + \hat\sigma(m, \mathbf{y})}{3}$
\item Geometric mean: $\left( \hat\mu(\mathbb{Z}, \mathbf{y}) \times \hat\iota(\hat{y}, \mathbf{y}) \times \hat\sigma(m, \mathbf{y}) \right)^{1/3}$
\item Product: $\hat\mu(\mathbb{Z}, \mathbf{y}) \times \hat\iota(\hat{y}, \mathbf{y}) \times \hat\sigma(m, \mathbf{y})$
\item Addition: $\hat\mu(\mathbb{Z}, \mathbf{y}) + \hat\iota(\hat{y}, \mathbf{y}) + \hat\sigma(m, \mathbf{y})$
\item Weighted mean: $w_\mu \hat\mu(\mathbb{Z}, \mathbf{y}) + w_\iota \hat\iota(\hat{y}, \mathbf{y}) + w_\sigma \hat\sigma(m, \mathbf{y})$
\item Harmonic mean: $\frac{3}{ \hat\mu(\mathbb{Z}, \mathbf{y})^{-1} + \hat\iota(\hat{y}, \mathbf{y})^{-1} + \hat\sigma(m, \mathbf{y})^{-1} }$
\end{itemize}

Euclidean distance and addition can produce nescience values greater than one, which conflicts with the intended $[0,1]$ range. The geometric mean, product, and harmonic mean have the issue that the resulting nescience is zero (or undefined, in the harmonic case) whenever any of the three components is zero. The weighted mean introduces three additional hyperparameters $(w_\mu, w_\iota, w_\sigma)$ that must be chosen (often subject to $w_\mu + w_\iota + w_\sigma = 1$). It remains an open question which utility function is most appropriate in general; by default we use the arithmetic mean.
\end{remark}

Example \ref{ex:nescience_decisiontree} shows how to use the \texttt{nescience} library to compute the nescience of a dataset and a model.

\begin{example}
\label{ex:nescience_decisiontree}

We compute the nescience of a decision-tree classifier on the \texttt{digits} dataset (MNIST handwritten digits) from \texttt{sklearn}.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_digits
from Nescience.Nescience import Nescience

data = load_digits()

tree = DecisionTreeClassifier()
tree.fit(data.data, data.target)
tree.score(data.data, data.target
[ ] 1

nescience = Nescience()
nescience.fit(data.data, data.target)

nescience.nescience(tree)
[ ] 0.5895603819965907
\end{verbatim}}
\end{sourcecode}

The training accuracy of the decision tree is $1.0$, meaning all samples are correctly classified on the training set. This is symptomatic of overfitting. In practice we would use a train/test split or cross-validation. Nonetheless, the nescience value of about $0.59$ flags that something is wrong with the model-data pairing.

\end{example}

As Example \ref{ex:nescience_decisiontree} illustrates, a key advantage of nescience is that it can evaluate model quality without requiring computationally expensive procedures such as cross-validation or holding out a test set. Another advantage is that nescience allows comparisons across different model families, as shown next.

\begin{example}
\label{ex:nescience_comparison}

We compare two model families on the Breast Cancer Wisconsin (Diagnostic) dataset from \texttt{sklearn}: a decision tree and a neural network.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import load_breast_cancer
from Nescience.Nescience import Nescience

data = load_breast_cancer()
X = data.data
y = data.target

tree = DecisionTreeClassifier(max_depth=3)
tree.fit(X, y)
tree.score(X, y)
[ ] 0.9789103690685413

nescience = Nescience()
nescience.fit(X, y)
nescience.nescience(tree)
[ ] 0.5945936419010083

nn = MLPClassifier()
nn.fit(X, y)
nn.score(X, y)
[ ] 0.9261862917398945

nescience.nescience(nn)
[ ] 0.7860523786210711
\end{verbatim}}
\end{sourcecode}

In one run, both models achieve high and broadly similar training accuracy. In this case, not only does the decision tree achieve the better score, but its nescience is also much lower than that of the multilayer perceptron; hence we should prefer the former to the latter.

\end{example}

Nescience can also be used to optimize hyperparameters within a (parametric) family of models. A practical advantage is that we can adopt a greedy search for a hyperparameter that is monotone with accuracy: select the value at which nescience stops decreasing and starts to increase, since this is the point at which we are no longer learning anything new from the dataset (see Example \ref{ex:nescence_hyperparameter}).

\begin{example}
\label{ex:nescence_hyperparameter}

We revisit a decision tree on the breast cancer dataset. We train $10$ trees with \texttt{max\_depth} from $1$ to $10$. The deeper the tree, the higher the training accuracy, but also the higher the risk of overfitting. For each tree we compute the model's nescience and compare it to a cross-validation score. Figure \ref{figure:nescience_cancer} shows typical behavior: both nescience and the cross-validation error decrease as depth increases, up to an inflection point after which they increase, indicating overfitting. The \texttt{nescience} library suggests a maximum depth of $7$, whereas cross-validation indicates an optimum near $6$.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{nescience_cancer.png}
\caption{Evolution of Nescience with Tree Depth.}
\label{figure:nescience_cancer}
\end{figure}

\end{example}

It is instructive to examine the three components in Figure \ref{figure:nescience_cancer}. As expected, deeper trees reduce inaccuracy and increase surfeit. By contrast, miscoding can evolve nonmonotonically because each candidate tree may use a different subset of features at internal nodes. It would be desirable to have a tree-building algorithm that accounts for miscoding when selecting split features; such an algorithm is described in Section \ref{sec:decision_trees}.

Finally, we consider hyperparameter searches where a greedy approach is not applicable—for example, when searching over multiple (often conflicting) hyperparameters. Grid search can be computationally expensive due to the number of combinations, and even more so if each candidate requires cross-validation. Since nescience can flag overfitting without cross-validation, it can substantially speed up hyperparameter search. Example \ref{ex:hyper_search} shows how to integrate nescience into \texttt{GridSearchCV}.

\begin{example}
\label{ex:hyper_search}

In this example we are going to see how we can use the \texttt{nescience} library to find the optimal hyperparameters for a model using a grid search. In particular, we are going to select the best hyperparameters for a multilayer perceptron classifer, including the number of hidden layers, and the size of those layers (what it is called Neural Architecture Search). The procedure will be demonstrated using the \texttt{digits} dataset.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from Nescience.Nescience import Nescience
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
\end{verbatim}}
\end{sourcecode}

First of all we have to provide a custom loss function based on the concept of nescience to be integrated with the search procedure. The next code shows how to implement such a function.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
def my_custom_loss_func(estimator, X, y):
    
    nsc = Nescience()
    nsc.fit(X, y)
    nescience = nsc.nescience(estimator)
    
    # scikit-learn expect that higher numbers are better
    score = -nescience
    
    return score
\end{verbatim}}
\end{sourcecode}

Second, we have to define the grid of hyperparameters over which we are going to do the search. The larger the grid, the better the result, but also, the more computer time is required to evaluate all possible combinations.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
parameters = {'solver': ['lbfgs'],
              'max_iter': [1000, 1500, 2000 ], 
              'alpha': 10.0 ** -np.arange(1, 10, 3),
              'hidden_layer_sizes':[(60,), (100,), (60, 60,), (100, 100,), 
                                    (60, 60, 60,), (100, 100, 100,)]}
\end{verbatim}}
\end{sourcecode}

Next code show how to do a classical grid search using the score of the models. The search will be evaluated using a train/test split of the dataset.
 
\begin{sourcecode}
{\scriptsize \begin{verbatim}
clf_std = GridSearchCV(estimator=MLPClassifier(), param_grid=parameters,
                       cv=3, iid=True, n_jobs=-1)
clf_std.fit(X_train, y_train)
clf_std.best_params_

[] {'alpha': 0.1,
[]  'hidden_layer_sizes': (100,),
[]  'max_iter': 1000,
[]  'solver': 'lbfgs'}

y_true, y_pred = y_test, clf_std.predict(X_test)
print(classification_report(y_true, y_pred))

[] precision    recall  f1-score   support
[] avg / total       0.98      0.97      0.97       540
\end{verbatim}}
\end{sourcecode}

Next code show how to perform exactly the same search, but using the concept of nescience instead of the metric score.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
clf_nsc = GridSearchCV(estimator=MLPClassifier(), param_grid=parameters,
                       cv=3, scoring=my_custom_loss_func, iid=True)
clf_nsc.fit(X_train, y_train)
clf_nsc.best_params_

{'alpha': 0.1,
 'hidden_layer_sizes': (60,),
 'max_iter': 1500,
 'solver': 'lbfgs'}

y_true, y_pred = y_test, clf_nsc.predict(X_test)
print(classification_report(y_true, y_pred))

                  precision    recall    f1-score    support
avg / total       0.98         0.98      0.98        540
\end{verbatim}}
\end{sourcecode}

In a typical run, both searches yield strong generalization. Notably, the nescience-based search selects a \emph{smaller} model (e.g., a single hidden layer with $60$ neurons instead of $100$) and compensates with a higher \texttt{max\_iter}. This reflects the role of surfeit: nescience tends to select the smallest model that attains high accuracy without overfitting the training data.

\end{example}

%
% Section: Auto Classification
%

\section{Auto Classification}
\label{sec:machine_learning_classification}

The \texttt{nescience} library also includes a module for automated machine learning (for both classification and regression problems). The AutoML module returns the single model, chosen from a collection of model families, that yields the smallest nescience. For each family, the class performs a greedy search over the hyperparameters required by that family, using nescience as the objective.

The next example shows how to apply the AutoML tools.

\begin{example}
\label{ex:automl}

In this example we use the \texttt{nescience} library to find the model that best describes the \texttt{digits} dataset.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

from Nescience.Nescience import AutoClassifier

(X, y) = load_digits(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

model = AutoClassifier()
model.fit(X_train, y_train)

model.score(X_test, y_test)
[] 0.9622222222222222
\end{verbatim}}

\end{sourcecode}
\end{example}

If we evaluate \texttt{type(model.model)}, we will see that the library has selected a linear support vector machine as the best model for this dataset.

A key difference between the \texttt{nescience} library and other AutoML libraries is that it returns a \emph{single} model as the best candidate, rather than an ensemble of models. In this way, the data scientist can directly reuse the result of \texttt{AutoClassifier} and proceed with the subsequent analysis.

%
% Section: Auto Regression
%

\section{Auto Regression}

The \texttt{AutoRegressor} class automatically selects the best model for a regression problem. In particular, it (i) computes an optimal subset of features, (ii) selects the most suitable family of models, and (iii) tunes the model's hyperparameters—using nescience as the objective. 

\begin{example}
\label{ex:auto_regression}

In this example we apply our auto-regression class to estimating house prices, using the \texttt{boston} dataset included with \texttt{scikit-learn}.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from fastautoml.fastautoml import AutoRegressor
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

(X, y) = load_boston(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

model = AutoRegressor()
model.fit(X_train, y_train)
AutoRegressor()

model.score(X_test, y_test)
0.8763987309111113
\end{verbatim}}

\end{sourcecode}
\end{example}

If we evaluate \texttt{type(model.model)}, we see that the library has selected a linear support vector machine or a decision tree (depending on the run and dataset version) as the best model for this dataset.

A key difference between the \texttt{nescience} library and other AutoML libraries is that it returns a single model as the best candidate, rather than an ensemble of models. In this way, the data scientist can directly reuse the result of \texttt{AutoRegressor} and proceed with the analysis.

%
% Section: Time Series
%

\section{Time Series}

In this section we study the application of the concept of miscoding to a time series and a delayed version of itself, or to a delayed version of a second time series.

Automiscoding applies miscoding to a time series and a lagged version of itself, as a function of the lag. Automiscoding estimates to what \emph{extent} past observations of the series can explain (or help forecast) future observations. In this sense, automiscoding has a similar objective to autocorrelation in classical time-series analysis (see Section \ref{sub:autocorrelation}).

\begin{definition}
Let $\{\mathbf{x}_t\}$ be a time series composed by $n$ samples. We define lag $k$ \emph{regular automiscoding} of $\{\mathbf{x}_t\}$ as $\hat\mu(\mathbf{x}_{x_{k+1}, x_{k+2}, \ldots, x_n}, \mathbf{x}_{x_0, x_1, \ldots, x_{(n-k)}})$. We define in the same way the concepts of \emph{adjusted automiscoding} and \emph{partial automiscoding}.
\end{definition}

In contrast to autocorrelation, automiscoding is defined for all time series, including those with a trend. Moreover, automiscoding remains interpretable in the presence of trends and can reveal seasonal components without requiring prior decomposition.

\begin{example}
We examine cycles in the number of passengers of a US airline. Figure \ref{figure:air_passengers} shows monthly passengers from 1949 to 1960 (AirPassengers dataset, see References below). There is a clear yearly cycle. We apply automiscoding to confirm this analytically.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{airpassengers.png}
\caption{Air Passengers.}
\label{figure:air_passengers}
\end{figure}

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from nescience.timeseries import TimeSeries

data = pd.read_csv("data/AirPassengers.csv", index_col=["Month"], parse_dates=True)
X = np.array(data["#Passengers"]).reshape(-1, 1)

ts = TimeSeries(auto=False)
ts.fit(data)
mscd = ts.auto_miscoding(max_lag=36)
\end{verbatim}}
\end{sourcecode}

As shown in Figure \ref{figure:auto-miscoding}, the adjusted automiscoding exhibits a prominent peak every twelve months: the distance between the aligned sequences is minimized when the lag is a multiple of one year.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{auto-miscoding.png}
\caption{Auto-miscoding of Air Passengers.}
\label{figure:auto-miscoding}
\end{figure}

\end{example}

Crossmiscoding measures the relationship between a time series and a lagged version of a second time series. The objective is to detect whether the first series has temporal predictive power over the second.

\begin{definition}
Let $\{\mathbf{x}_t\}$ and $\{\mathbf{y}_t\}$ be two time series composed by $n$ samples each. We define lag $k$ \emph{regular crossmiscoding} of $\{\mathbf{x}_t\}$ and $\{\mathbf{y}_t\}$ as $\hat\mu(\mathbf{x}_{x_{k+1}, x_{k+2}, \ldots, x_n}, \mathbf{y}_{x_0, x_1, \ldots, x_{(n-k)}})$. We define in the same way the concepts of \emph{adjusted crossmiscoding} and \emph{partial crossmiscoding}.
\end{definition}

\begin{example}
We investigate whether it is possible to predict household appliance energy consumption. The dataset (Appliances Energy Prediction; see References) contains temperature and humidity measurements from rooms every ten minutes, the appliance energy consumption, and weather variables from a nearby station.

For each feature we compute the lag in $[1,30]$ that maximizes crossmiscoding with the target:

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from fastautoml.fastautoml import Miscoding

X = pd.read_csv("../data/energydata_complete.csv", parse_dates=["date"], index_col="date")
y = X["Appliances"]
X = X.drop(["Appliances", "lights"], axis=1)

miscoding = Miscoding()
miscoding.fit(X, y)

best_lag  = list()
for i in np.arange(X.shape[1]):
    mscd = miscoding.cross_miscoding(attribute1=i, min_lag=1, max_lag=30)
    best_lag.append(np.where(mscd == np.max(mscd))[0][0] + 1)
\end{verbatim}}
\end{sourcecode}

Figure \ref{figure:cross-miscoding} shows the results. In general, indoor measurements favor small lags, whereas weather variables (which influence indoor conditions with delay) favor larger lags.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{cross_miscoding_lag.png}
\caption{Cross Miscoding Lag}
\label{figure:cross-miscoding}
\end{figure}

\end{example}

\begin{remark}
The approximation to miscoding introduced in this chapter estimates (i) the quality of individual features as predictors of a target and (ii) the quality of the training dataset as a whole. However, it does not explicitly account for redundancy among the features themselves. For instance, two features $\mathbf{x}_i$ and $\mathbf{x}_j$ may each have low miscoding with respect to $\mathbf{y}$ yet be largely redundant with one another. It remains an open question how to extend miscoding to incorporate feature redundancy in a way that stays close to the theoretical definition, is computationally efficient, and does not require prohibitively large samples.
\end{remark}

% Auto Time Series

% \subsection{Auto Time Series}

% {\color{red} TODO: Introduce the auto-time series models, and clearly state what it can be expect from such models (short story: nothing)}

% {\color{red} TODO: Introduce structural time series models, the state space representation, and the Kalman filter}

% {\color{red} structura approach [...] different unobserved components or building blocks responsible for the dynamics of the series such as trend, seasonal, cycle, and the effects of explanatory and intervention variables are identified separately before being put together in a state space model.}

% {\color{red} State space methods originated in the eld of control engineering, starting with the groundbreaking
% paper of Kalman (1960). They were initially (and still are) deployed for the purpose
% of accurately tracking the position and velocity of moving objects such as ships, airplanes,
% missiles, and rockets [...] these ideas could well be applied to time series analysis
% generally as well.}

% {\color{red} In a state space
% analysis the time series observations are assumed to depend linearly on a state vector that
% is unobserved and is generated by a stochastically time-varying process (a dynamic system).
% The observations are further assumed to be subject to measurement error that is independent
% of the state vector. The state vector can be estimated or identified once a sufficient set of
% observations becomes available.}

% \begin{definition}
% A linear Gaussian state space model for the multivariate time series $\mathbf{y} = \mathbf{y}_1, \ldots, \mathbf{y_n}$, where each observiation is a $p$ dimensional vector $\mathbf{y}_i = \{ y_{i1}, \ldots, y_{ip} \}$, is given by
% \begin{equation}
% \mathbf{y}_t = \mathbf{Z}_t \mathbf{\alpha}_t + \mathbf{d}_t + \mathbf{\epsilon}_t \quad \mathbf{\epsilon}_t \sim N \left( 0, \mathbf{H}_t \right)
% \end{equation}
% called {\color{red} space?} \emph{orbservation or measurement equation}, and
% \begin{equation}
% \mathbf{\alpha}_t = \mathbf{T}_t \mathbf{\alpha}_{t-1} + \mathbf{c}_t + \mathbf{R}_t \mathbf{\eta}_t \quad \mathbf{\eta}_t \sim N \left( 0, \mathbf{Q}_t \right)
% \end{equation}
% called \emph{state or transition equation}, where the individual summands correspond to:
% \begin{align*}
%  & \mathbf{y}_t \quad \text{observed or measured values,} \\
%  & \mathbf{Z}_t \quad \text{design matrix,} \\
%  & \mathbf{\alpha}_t \quad \text{unobserved state,} \\
%  & \mathbf{d}_t \quad \text{observation intercept,} \\
%  & \mathbf{\epsilon}_t \quad \text{observational disturbance,} \\
%  & \mathbf{H}_t \quad \text{observational disturbance covariance matrix,} \\
%  & \mathbf{T}_t \quad \text{transition matrix,} \\
%  & \mathbf{c}_t \quad \text{state intercept,} \\
%  & \mathbf{R}_t \quad \text{selection matrix,}  \\
%  & \mathbf{\eta}_t \quad \text{state disturbance, and} \\
%  & \mathbf{Q}_t \quad \text{state disturbance covariance matrix}
% \end{align*}
% \end{definition}

% {\color{red} The $p \times m$ matrix $Z_t$ links the observation vector $y_t$ with the unobservable state vector $\alpha_t$ and may consist of regression variables.  The $m \times m$ transition matrix $T_t$ determines the dynamic evolution fo the state vector [...] the observation and state disturbances $\epsilon_t$ and $eta_t$ are assumed to be serially independent and independent of each other at all time points [...] matrix $R_t$ is an $m \times r$ selection matrix with $r < m$.}

% {\color{red} The initial state vector $\alpha_1$ is assumed to be generated as $\alpha_1 \sim NID \left( a_1, P_1 \right)$, independen of the observation and estate disturbances $\epsilon_t$ and $\eta_t$. Mean $a_1$ and variance $P_1$ can be treated as given konw.}

% {\color{red} Talk about initialization?}

% For example, if the time series $\mathbf{y}$ is unidimensional and the state space model is time invariant (only $\mathbf{y}_t$ and $\mathbf{\alpha}_t$ depends on $t$, being the rest of the summands constant), a model with $m$ unobserved stares will be given by
% \[
% y_t = [ z_1 \ldots z_m ]
% \begin{bmatrix}
%   z_{1} \\
%   \vdots \\
%   z_{n}
% \end{bmatrix}
% + d_{t} + \epsilon_{t}
% \]

% Some of the most common time series models are particular cases of the state-space model (see Example XX).

% \begin{example}
% \end{example}

% {\color{red} TODO: Explain the Kalman filter}

% The \emph{Kalman filter} is a recursive formula that provides an optimal estimate for the unknown state in a state space model. At each time step $t$, the Kalman filter computes the predicted state conditional to the observations up to time $t-1$.

% {\color{red} Kalman filter can be use for filtering, prediction and smoothing. Here we are only interested in prediction [...] forward pass [...] recursive formulas}

% \begin{definition}
% \begin{equation}
% \begin{split}
%   & \mathbf{a}_{t+1} = \mathbf{T}_t \mathbf{a}_t + \mathbf{K}_t \mathbf{v}_t  \\
%   & \mathbf{K}_t = \mathbf{T}_t \mathbf{P}_t \mathbf{Z}_t^T \mathbf{F}_t^{1} \\
%   & \mathbf{v}_t = \mathbf{y}_t  - \mathbf{Z}_t \mathbf{a}_t
% \end{split}
% \end{equation}
% called \emph{prediction equations}, and
% \begin{equation}
% \begin{split}
%   & \mathbf{F}_t = \mathbf{Z}_t \mathbf{P}_t \mathbf{Z}_t^T + \mathbf{H}_t \\
%   & \mathbf{L}_t = \mathbf{T}_t - \mathbf{K}_t \mathbf{Z}_t \\
%   & \mathbf{P}_{t+1} = \mathbf{T}_t \mathbf{P}_t \mathbf{L}_t^T + \mathbf{R}_t \mathbf{Q}_t \mathbf{R}_t^T
% \end{split}
% \end{equation}
% called \emph{updating equations}.
% \end{definition}

% \begin{example}
% {\color{red} TODO: Provide an example where we can see how the Kalman filter integrates the predicted probability distribution and observed probability distribution to make a prediction}
% \end{example}

% {\color{red} TODO: Examplain the space search algorithm}

% \begin{example}
% {\color{red} TODO: Provide an example of auto-time series}
% \end{example}

%
% Section: Anomaly Detection
%

\section{Anomaly Detection}

As we have seen in Section XXX, the main problem in anomaly detection is that we do not have a precise mathematical definition of what an anomaly is. In this book we propose to equate \emph{abnormal} samples with \emph{incompressible} samples, and to study the consequences. The essence is that learning is about finding regularities in a dataset, and data compression is about exploiting regularities. We have also seen that the best model minimizes the sum of the length of the model plus the length of the data given the model. This optimal model partitions the dataset into two disjoint subsets: a compressible part and an incompressible part. It is the latter that interests us here: being incompressible means the samples cannot be explained by the model, i.e., they are model-based anomalies under the best available model.

\begin{definition}
Let $\emph{X}$ be a dataset composed by $p$ features and $n$ samples, $\mathbf{y}$ the target variable, and $\mathit{M}$ a model such that the nescience $N \left( \mathbf{X}, \mathit{M} \right)$ is minimal. Let $\hat{y} = \mathit{M} \left( \mathbf{X} \right)$ be the predicitons made by the model $\mathit{M}$ over the vectors of $\mathbf{X}$. We define the \emph{anomaly subset} of $\mathbf{X}$, denoted by $\mathcal{A}_\mathit{M}^\mathbf{X}$, to the set of $X$ such that $y \neq \hat{y}$.
\end{definition}

The class \texttt{nescience.anomalies} allows us to identify the anomaly subset, i.e., the samples that do not match the regularity patterns found in the rest of the dataset. In Example \ref{ex-abnormal_boston_houses} we apply this class to identify houses with abnormally low prices and to explain why they are cheaper.

\begin{example}
\label{ex-abnormal_boston_houses}
We use the Boston House Prices dataset from \texttt{scikit-learn}. The dataset contains 13 predictive features (both numeric and categorical) describing different characteristics of the houses (number of rooms, age, etc.) and the target is the median value of owner-occupied homes. The dataset has 506 samples. We aim to identify houses whose prices are \emph{abnormally low}, i.e., houses that, given their characteristics, should have a higher price.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from sklearn.datasets import load_boston

data = load_boston()
X    = data.data
y    = data.target
\end{verbatim}}
\end{sourcecode}

We first train a "knowledge model," i.e., the best model that explains the target variable given the predictors, without overfitting. By default, the \texttt{anomalies} class uses the AutoML capabilities provided by the \texttt{nescience} library.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from nescience.anomalies import Anomalies

model = Anomalies(X_type="mixed", y_type="numeric")
model.fit(X, y)
\end{verbatim}}
\end{sourcecode}

Finally, we select those samples for which the actual price is smaller than the price predicted by the model.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
anomalies = model.get_anomalies("smaller")
X.shape, anomalies.shape
((506, 13), (25,))
\end{verbatim}}
\end{sourcecode}

Thus, there are 25 houses with abnormally low price.

\end{example}

The \texttt{anomalies} class also allows us to group the identified anomalies according to shared characteristics.

Let us examine which attributes best describe those abnormal houses.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
model.get_classes(n_dims=1, an_type="smaller", filter_balancedness=True,
                  filter_redundancy=False, filter_repeated_attrs=False)

Attribute1	Attribute2	Inertia	        N Class 0	N Class 1	Ratio
2	        None	        154.953975	9	        16	        0.36
4	        None   	        0.090593	6	        19	        0.24
5	        None	        5.002437	17	        8	        0.68
7	        None	        20.352117	6	        19	        0.24
8	        None	        77.611111	18	        7	        0.72
9	        None	        41737.11111	7	        18	        0.28
10	        None	        26.577436	13	        12	        0.52
12	        None	        430.294828	18	        7	        0.72
\end{verbatim}}
\end{sourcecode}

According to the inertia, the best attribute to classify the anomalies is attribute 4 (nitric oxides concentration). This attribute divides the abnormal houses into two clusters of sizes 6 and 19. Let us examine how price varies with this dimension.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
        
lr = LinearRegression()
lr.fit(X[:,4].reshape(-1, 1), y)
        
plt.scatter(X[:,4].reshape(-1, 1), y)
plt.plot(X[:,4], lr.intercept_ + lr.coef_ * X[:,4], color="red")
plt.xlabel(data.feature_names[4])
plt.ylabel("Price")
\end{verbatim}}
\end{sourcecode}

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{nox_price.png}
\caption{Price as a function of NOX.}
\label{figure:nox_vs_price}
\end{figure}

The regression line suggests that house prices are lower in areas with higher nitric oxides concentration. Let us see how the anomalies split along this dimension.

\begin{sourcecode}
{\scriptsize \begin{verbatim}
class0, class1 = model.get_class_points(attribute1=4, attribute2=None, an_type="smaller")

plt.hist(class0)
plt.hist(class1)
plt.ylabel("Count")
plt.xlabel(data.feature_names[4])
plt.show()
\end{verbatim}}
\end{sourcecode}

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{nox_histogram.png}
\caption{Histogram of anomalies NOX.}
\label{figure:nox_histogram}
\end{figure}

The analysis suggests that six of the houses have abnormally low prices because they are located in areas with high pollution levels. We can repeat the same analysis with the other attributes; however, their higher inertia indicates that class separation will be less evident. Note that there may be multiple reasons why a house's price is abnormally low. 

%
% Section: Decision Trees
%

\section{Decision Trees}
\label{sec:decision_trees}

In the last sections we have seen how to use the concepts of miscoding, inaccuracy, surfeit and nescience to evaluate the quality of datasets and models, and to automatically select a family of models and search over its hyperparameters to find the best possible description of a topic. In particular, we have studied in detail the family of binary decision trees. The procedure used in the \texttt{fastautoml} library with trees was a mix between a classical approach (a CART algorithm combined with a cost-complexity pruning), and an evaluation of candidate trees using the minimum nescience principle. In this section we are going to see a new algorithm to derive optimal trees, both for classification and regression problems, that is entirely based on the theory of nescience. The new algorithm, by design, avoids the overfitting of the training dataset without losing accuracy, it does not require the optimization of hyperparameters, thus significantly reducing the training time, and it produces much smaller and more shallow trees than traditional algorithms, facilitating the interpretability of the results.

\subsection{Algorithm Description}
\label{sub:tree_algorithm_description}

The following pseudocode shows the proposed algorithm to build a decision tree given a training dataset $(\mathbf{X}, \mathbf{y})$. The procedure is based on a breadth first traversal of trees combined with a greedy approach. It requires a function called \textsc{bestSplit()} that returns the best split of a given subset of the data into two subsets; and a second function, called \textsc{Nescience()} that provides an estimation of the nescience of the current tree. The algorithm is based on two nested loops: the external \textbf{while} loop keeps a list of the candidate nodes to grow, whereas the internal \textbf{for} loop finds the best node to grow the tree. The latter operation requires to check all possible growing options and select the one that minimizes the nescience. The exit point of the algorithm is when there are no more branches to grow. We keep track of the best nescience achieved during the building process and return the associated tree.

\begin{sourcecode}
\label{algorithm:decision_tree}
{\scriptsize \begin{verbatim}

def BUILD_TREE(data)

    nodesList <- list()
    tree <- BESTSPLIT(data)
    bestNescience <- NESCIENCE(tree)
    nodesList.append(tree)

    while not nodesList.empty()
    
        nescience <- bestNescience
        bestNode <- None
        childNode <- None
        
        for i <- 1, nodesList.length()

            node <- nodesList[i]
            
            node.child <- BESTSPLIT(node.ldata)
            tmp <- NESCIENCE(tree)
            if tmp < nescience
                nescience <- tmp
                bestNode <- i
                
            node.left <- None

        if nescience < bestNescience
            node <- nodeList[bestNode]
            bestNescience <- nescience
            nodesList.append(node.left)
                        
            if not node.left.empty() and not node.right.empty()
                nodesList.remove(bestNode)
    
    return tree
\end{verbatim}}
\end{sourcecode}

The main difference of our algorithm from other decision tree building algorithms is in the way the tree is evaluated. Instead of using only accuracy as most of the algorithms do, in addition, we take into account the complexity of the tree (surfeit) to avoid overfitting, and the quality of the subset of data used during the training process (miscoding).

\subsubsection*{Nescience}
\label{sub:tree_nescience}

The calculation of the nescience implemented in the algorithm is based on a Euclidean distance utility function (see Section \ref{sec:machine_learning_nescience}), because that one was the one that produced the best results in the tests we have performed. For the computation of miscoding and inaccuracy, we use the same techniques that the one used in the \texttt{fastautoml} library, described in Section \ref{sec:machine_learning_miscoding} and Section \ref{sec:machine_learning_inaccuracy} respectively. For the implementation of surfeit, we use the same template to describe trees that was used in the \texttt{DecisionTreeClassifier} of the \texttt{AutoClassifier} class, and that was described in Section \ref{subsec:surfeit_decision_trees}. The only difference is that we also allow equalities in the nodes (\texttt{if [attr] = [thresh]}), something not supported by the \texttt{DecisionTreeClassifier} algorithm of the \texttt{scikit-learn} library.

The generic problem of the instability of inaccuracy due to very short models, also applies to this algorithm (see Section \ref{sec:machine_learning_surfeit}), and the particular problem of the algorithms to build decision trees, in which the best local split might not be that one that minimizes the error (see Section \ref{sec:sec:machine_learning_classification}) is also relevant in this case.

The concept of nescience is used in two different ways in our algorithm. For every iteration of the \texttt{for} loop we have to decide which one of the candidate branches of the tree we should develop. Recall that the order in which we develop the branches is important, since it might happens that one branch does not get develped because that would mean increase the sufeit without a sufficiently large decrease of the inaccuracy. The second place is a the end of the \texttt{while} loop, we we keep trac of the nescience of the different building steps, to decide at the end of the algorithm with wich tree we return.

We treat regression problems as classification problems in which we discretizes the continuous target variable $\mathbf{y}$ into $n$ intervals given the number of samples, and using a uniform discretization (see Section \label{sec:codes_continuous_data}). Once the target variable has been discretized, we train a regular classification tree.

\subsubsection*{Splitting Criteria}
\label{sub:tree_splitting_criteria}

Given a subset $\mathbf{Q} \subseteq \mathbf{X}$ we have to find an split for $\mathbf{Q}$ such that the values of $\textbf{y}$ are grouped together. Recall that a split is a pair $\theta = (j,w)$, were $1 \leq j \leq p$ is a feature index and $w$ is the partition point (see Section \ref{subsec:learning_decision_trees}). A split divides the set $\mathbf{Q}$ into two disjoint subsets $\mathbf{Q}_l$ and $\mathbf{Q}_r = \mathbf{Q} \backslash \mathbf{Q}_l$. In case of a continuous variable we have that $\mathbf{Q}_l = \{\mathbf{x}_i \in \mathbf{Q} : x_{ij} \leq w\}$, and if the feature is categorical we define $\mathbf{Q}_l = \{\mathbf{x}_i \in \mathbf{Q} : x_{ij} = w\}$\footnote{Ideally, for the categorical case, instead of a single feature $w$ we should search over all the elements of the power set of the set of features $\mathcal{P} \{1, 2, \ldots, p \}$. Unfortunately, that would imply to check $2^p$ cases, something that is time-expensive from the computational point of view.}.

In Section \ref{subsec:learning_decision_trees} we saw that a common splitting criteria used in practice is to minimize the weighted entropy $\tilde{H}$ of the subsets $\mathbf{Q}_l$ and $\mathbf{Q}_r$, that is, to find an split that it is minimal $\theta^\star = \argmin_\theta \tilde{H}(\mathbf{Q},\theta)$. More explicitly, if $\mathbf{y}$ is a target vector taking values from a set of $k$ labels $\mathcal{G} = \{g_1, \ldots, g_k \}$ (either because is a categorical target or a continuous target that has been discretized into $k$ intervals), and denoting the subsets of $\mathbf{y}$ as $\mathbf{y}^l = \{y_i : \mathbf{x}_i \in \mathbf{Q}_l \}$ and $\mathbf{y}^r = \{y_i : \mathbf{x}_i \in \mathbf{Q}_r \}$, and $n_l$ and $n_r$ are the number of elements of $\mathbf{y}^l$ and $\mathbf{y}^r$ respectively, we have that
\begin{multline}
\tilde{H}(\mathbf{Q},\theta) = \frac{n_l}{n} \left( - \sum_{i=1}^k \frac{ \sum_{j=1}^{n_l} I(y^l_j = g_i)} {n_l} \log_2{ \frac{ \sum_{j=1}^{n_l} I(y^l_j = g_i)} {n_l} } \right) \\
+ \frac{n_r}{n} \left( - \sum_{i=1}^k \frac{ \sum_{j=1}^{n_r} I(y^r_j = g_i)} {n_r} \log_2{ \frac{ \sum_{j=1}^{n_r} I(y^r_j = g_i)} {n_r} } \right)
\end{multline}

In our nescience based decision tree algorithm, the splitting criteria is to minimize the total length of encoding the subsets $\mathbf{Q}_l$ and $\mathbf{Q}_r$ using optimal codes. We have to find the optimal split $ \theta^\star = \argmin_\theta \hat{K}_C(\mathbf{Q} \mid \theta) = \argmin_\theta \{ \hat{K}_{C_l}(\mathbf{Q}_l) + \hat{K}_{C_r}(\mathbf{Q}_r) \}$ where $C_l$ and $C_r$ are the optimal codes given the relative frequencies of the observed values of $\mathbf{y}^l$ and $\mathbf{y}^r$ respectively. The quantity $\hat{K}_C(\mathbf{Q} \mid \theta)$ is computed as:
\begin{multline}
\hat{K}_C(\mathbf{Q} \mid \theta) = \hat{K}_{C_l}(\mathbf{Q}_l) + \hat{K}_{C_r}(\mathbf{Q}_r) = \\
- \sum_{i=1}^k \log_2{ \frac{ \sum_{j=1}^{n_l} I(y^l_j = g_i)} {n_l} } - \sum_{i=1}^k \log_2{ \frac{ \sum_{j=1}^{n_r} I(y^r_j = g_i)} {n_r} }
\end{multline}
I this particular case (if we use as compression algorithm a code with optimal lengths, and continuous variables have been discretized) it turns out that both expressions are equivalent given the following relation:
\[
\tilde{H}(\mathbf{Q},\theta) = \frac{1}{n} \hat{K}_C(\mathbf{Q} \mid \theta)
\]
We prefer to talk of encoding length instead of weighted entropy because it has an easier interpretation in the context of the theory of nescience.

\begin{remark}
Strictly speaking, if we want to implement a decision trees search algorithm fully compliant with the minimum nescience principle, instead of using a total length encoding as splitting criteria, we should had computed the nescience at each split and select that one that makes it minimal. However, early experiments have shown that at local level it works better to group the values of $y$ than to reduce the nescience. Further research is required to confirm and explain this point.
\end{remark}

\subsubsection*{Practical Implementation}

In the web page that accompanies this book\footnote{http://www.mathematicsunknown.com} we provide an open-source implementation of our algorithm in Python. Our software can be used together with other machine learning tools from the \texttt{scikit-learn} library, since we adhere to the their API guidelines. For example, our algorithm can be used as part of an ensemble of classifiers, like the \texttt{BaggingClassifier} meta-estimator, or the results of the classification could be cross-validated with tools like \texttt{cross\_val\_score}. As an example, to provide a model for the breast cancer dataset, we could do something like the following:

\begin{sourcecode}
{\scriptsize \begin{verbatim}
from NescienceDecisionTree import NescienceDecisionTreeClassifier
from sklearn.datasets import load_breast_cancer

data = load_breast_cancer()

model = NescienceDecisionTreeClassifier()
model.fit(data.data, data.target)
print("Score: ", model.score(data.data, data.target))
\end{verbatim}}
\end{sourcecode}

\subsection{Algorithm Evaluation}
\label{sub:algorithm_evaluation}

In this section we are going to evaluate our new algorithm, and compare its performance against the well-known algorithm CART. CART, \emph{Classification and Regression Trees}, is the de-facto standard algorithm used in the machine learning industry for the derivation of decision trees. For this particular experiment we have used the CART implementation provided by \texttt{scikit-learn}.

Figure \ref{figure:data_error_cart} shows a synthetic dataset consisting of $1000$ random points lying on a two dimensional plane, where all the points with an $X1$ attribute less than $50$ are colored blue, and the rest as red. We have artificially introduced a red point, simulating a measurement error, in the blue area. The black lines correspond to the decisions performed by CART. Since the CART algorithm will not stop until all the points have been properly classified, we have to specify an expected count condition to limit the number of splits. The figure correspond to the tree generated by CART setting the \texttt{min\_samples\_leaf} hyperparameter to $5$.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{data_error_cart.jpg}
\caption{Synthetic dataset with CART algorithm splits.}
\label{figure:data_error_cart}
\end{figure}

The tree obtained by applying our algorithm to the dataset of Figigure \ref{figure:data_error_cart} can be seen in Figure \ref{figure:data_error_nes}. The nescience based algorithm does not try to model the error point, since the gain due to an increment in the accuracy does not compensate the surfeit introduced in the model. Recall that the algorithm stops when the total nescience of the tree, based on the measures of miscoding, inaccuracy and surfeit, does not decrease when adding new nodes to the tree. Our algorithm presents a lower sensitivity to the errors found in datasets, at least if the number of errors is small compared with the number of valid points.

\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{small_error.jpg}
\caption{Decision tree obtained by the nescience algorithm.}
\label{figure:data_error_nes}
\end{figure}

Our second experiment, again with synthetic dataset, is depicted in Figure \ref{figure:blobs}. There, we create two isotropic Gaussian blobs that partially overlap. We start with a standard deviation of $2.5$ for each cluster, so they are easy to separate, and we increase the standard deviation in increments of $0.01$, until we reach $4.5$, which causes significant overlaps. For each value of the standard deviation, we run the experiment $100$ times and we compute the average accuracy for the two algorithms using different datasets for training (70\% of the data) and testing (30\% of the data). The results of this experiment are shown in Figure \ref{figure:accuracy_blobs}.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{blobs.png}
\caption{Isotropic Gaussian Blobs.}
\label{figure:blobs}
\end{figure}

As we can see, the performance of both algorithms, in terms of accuracy, is similar. However we should note that the hyperparameter \texttt{minimum\_leaf\_size} of the CART algorithm has been optimized to achieve the best accuracy. For this particular experiment, the best value was achieved with a minimum leaf size of 26 points. By definition, given the fact that CART has one degree of freedom more than the nescicence algorithm, it should produce better accuracy; something that it is not observed (both algorithms have a mean accuracy of $0.87$.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{accuracy_blobs.png}
\caption{Accuracy of Isotropic Gaussian Blobs.}
\label{figure:accuracy_blobs}
\end{figure}

For each iteration of the experiment, we have also computed the average number of nodes, including internal and leaf nodes, required by the models to properly classify the clouds in the dataset. The results of this measurement are show in Figure \ref{figure:length_nodes}. Our algorithm requires an average of $4$ nodes compared to $23$ nodes for the CART algorithm. Moreover, our algorithm is more stable than CART, in the sense that it produces models of similar complexity when it gets similar input datasets (a standard deviation of $0.31$ compared to $3.77$ for CART).

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{nodes_blobs.png}
\caption{Number of Nodes.}
\label{figure:length_nodes}
\end{figure}

In Figure \ref{figure:blob_max_depth} we show the maximum depth of the tree, defined as the longest path from the root of the tree to any of its leaves. The maximum depth of the tree is a good measure of the average time it will require for the model to provide a classification. The nescience algorithm has an average depth of $1.6$ nodes, whereas the average depth yielded by the CART algorithm is $4.8$ nodes.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{max_depth.png}
\caption{Maximum depth of the model.}
\label{figure:blob_max_depth}
\end{figure}

The last part of the evaluation consists in comparing the performance of our algorithm and CART with a collection real datasets. More specifically, we have selected $12$ well known datasets from the UCI Machine Learning Repository. The selected datasets are: diagnosis of breast cancer (\texttt{cancer}), optical recognition of handwritten digits (\texttt{digits}), predicting protein localization sites in gram-negative bacteria (\texttt{yeast}), classification of NASA space shuttle data (\texttt{shuttle}), classification of blocks in web pages (\texttt{page}), segmentation of outdoor images (\texttt{image}), predicting the age of abalones from physical measurements (\texttt{abalone}), predicting the quality of red and white variants of Portuguese wine (\texttt{wine}), filter spam emails (\texttt{spam}), wall-following robot navigation (\texttt{wall}), classification of land use based on Landsat satellite images (\texttt{landsat}), and distinguishing signals from background noise in the MAGIC gamma telescope images (\texttt{magic}). For each dataset, we have repeated the experiment 100 times, by randomly selecting the training (70\%) and testing (30\%) subsets at each iteration.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{cart_nescience_accuracy.png}
\caption{Maximum depth of the model.}
\label{figure:cart_nescience_accuracy}
\end{figure}

In Figure \ref{figure:cart_nescience_accuracy} we compare the accuracy of the resulting models obtained by applying the CART algorithm and the nescience algorithm to the above datasets. In $4$ of the $12$ datasets, our algorithm provides better accuracy than CART. In the remaining $8$ cases, the accuracy is, on average, less than $1\%$ smaller.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{cart_nescience_nodes.png}
\caption{Maximum depth of the model.}
\label{figure:cart_nescience_nodes}
\end{figure}

In Figure \ref{figure:cart_nescience_nodes} it is shown a comparison of the total number nodes (internal nodes plus leaf nodes) of the resulting models. Only for one of the datasets (\texttt{digits}), our model produces a slightly more complex tree that those generated by CART. In the rest of the cases, the number of nodes in the trees generated by the nescience algorithm have between two and three orders of magnitude fewer nodes (in this figure the $y$ axis is in logarithmic scale).

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{cart_nescience_depth.png}
\caption{Maximum depth of the model.}
\label{figure:cart_nescience_depth}
\end{figure}

Finally, in Figure \ref{figure:cart_nescience_depth} we provide a comparison of the depth of the tree of the resulting models. Our algorithm always yields a shallower tree than the CART algorithm.

We would like to mention that the nescience algorithm is hihgly robust with respect to the compressor selected or the nescience function implemented. In Table \ref{table:nescience_functions}, we have apply the nescience algorithm to the datasets described above, and evaluate different alternatives for the definition of the nescience function $N (X , M )$: arithmetic mean $(\mu(M , D) + \iota(X , M ) + \sigma(M, D))/3$, geometric mean $(\mu(M , D) + \iota(X , M ) + \sigma(M, D)) 1/3$ , harmonic mean $3 / (\mu(M , D) + \iota(X , M ) + \sigma(M, D)) -1 )$, Euclidean distance $(\mu(M , D) + \iota(X , M ) + \sigma(M, D)) 1/3$, sum $\mu(M , D) + \iota(X , M ) + \sigma(M, D)$, and product $\mu(M , D) + \iota(X , M ) + \sigma(M, D)$. The table shows limited difference between the different functions. 

\begin{table}[h]
\label{table:nescience_functions}
\centering
\begin{tabular}{l l l l l l l l}
\toprule
 & \small{\textbf{Euclid}} & \small{\textbf{Arithm.}} & \small{\textbf{Geometric}} & \small{\textbf{Product}} & \small{\textbf{Addition}} & \small{\textbf{Harmonic}} \\
\midrule
Acc. & 0.758 & 0.784 & 0.803 & 0.803 & 0.784 & 0.81 \\
Stdev. & 0.051 & 0.041 & 0.033 & 0.033 & 0.041 & 0.038 \\
\bottomrule
\end{tabular}
\caption{Comparison of nescience functions}
\end{table}

Similarly, Table \ref{table:compressor} shows the performance of our algorithm when using the LZMA, zlib, and bz2 compressors. We observe that all of them yield similar performance. The above results suggest that the performance our algorithm is independent of the specific choice made for either implementation aspect.

\begin{table}[h]
\label{table:compressor}
\centering
\begin{tabular}{l l l l}
\toprule
 & \textbf{bz2} & \textbf{lzma} & \textbf{zlib} \\
\midrule
Accuracy & 0.813 & 0.804 & 0.81 \\
Stdev & 0.03 & 0.045 & 0.038 \\
\bottomrule
\end{tabular}
\caption{Comparison of compressors}
\end{table}

We emphasize that the CART algorithm requires to optimize a configuration hyperparameter in order to obtain good results, whereas the algorithm proposed in this book does not require from this optimization.

Shallower trees means faster forecasting times when the models used in production, since the number of \texttt{if-else} conditions to be evaluated is smaller. Moreover, smaller trees makes easier to interpret the results by human analysts. and much shorter training times, something very relevant in case of training ensembles of trees, like random forest or boosted trees (although the use of ensembles of models is highly discouraged by the theory of nescience, given their high surfeit).

%
% Section: Algebraic Model Selection
%

\section{Algebraic Model Selection}

As it was the case for the definition of nescience based on the encyclopedic description of research topics, the nescience of structured datasets can be used to evaluate alternative descriptions of research topics (mathematical models), and to identify how far these descriptions are from an ideal perfect knowledge. This evaluation could be used to identify those topics which require further research. Moreover, the same methodology could be applied to collections of datasets to identify our current knowledge of research areas (collections of topics).

If we combine the concept of nescience of a model, with our concepts of relevance and applicability of research topics, we could apply our methodology for the assisted discovery of interesting questions to collections of datasets; a very useful methodology now that big datasets are becoming widely available.

In order to evaluate the methodology developed, we are going to apply it to a particular research topic: \emph{Multipath Wave Propagation and Fading}. The problem at hand is to understand the effect of a propagation environment on a radio signal, such as the one used by wireless devices. The signals reaching the receiving antenna could follow multiple paths, due to atmospheric reflection and refraction, and reflection from water and objects such as buildings. The effects of these multiple wave paths include constructive and destructive interference (fading), and phase shifting of the original signal, resulting a highly complex received signal (see Figure \ref{fig:Multipath-Signal-Propagation}).

\begin{figure}[h]
\centering\includegraphics[scale=0.5]{fading}
\caption{\label{fig:Multipath-Signal-Propagation}Multipath Signal Propagation}
\end{figure}

In many circumstances, it is too complicated to describe all reflection, diffraction and scattering processes that determine the different paths the signal will follow. Rather, it is preferable to describe the probability (stochastic model) that the received signal attains a certain value. We are interested in to analyze how well these stochastic models (our current knowledge) are able to describe what happen in reality.

The \emph{Rayleigh fading model} assumes that the magnitude of a signal will vary randomly, or fade, according to a Rayleigh distribution (the radial component of the sum of two uncorrelated Gaussian random variables). The Rayleigh probability density function of the power signal is given by:

\[
P_{\sigma}\left(x\right)=\frac{1}{\sigma}\exp\left[-\frac{x}{\sigma}\right]
\]

where $\sigma$ is the mean of the received signals. Rayleigh fading is viewed as a reasonable model for the effect of heavily built-up urban environments, when there is no dominant propagation along a line of sight between the transmitter and receiver.

The Rice or \emph{Rician distribution} describes the power of the received signal when the target consists in many small scatterers of approximately equal strength, plus one dominant scatterer whose individual received signal equals all that of all the small scatterers combined (there is a dominant line of sight). The probability density function of the power of the received signal is given by:

\[
P(x)=\frac{1}{\bar{\sigma}}\left(1+a^{2}\right)\exp\left[-a^{2}-\frac{x}{\bar{\sigma}}\left(1+a^{2}\right)\right]I_{0}\left[2a\sqrt{\left(1+a^{2}\right)\frac{x}{\bar{\sigma}}}\right]
\]


where $\bar{\sigma}$ is the mean of the received signals, and it is equal to $\bar{\sigma}=\left(1+a^{2}\right)\bar{\sigma}_{R}$, being $a^{2}\bar{\sigma}_{R}$ the power of the dominant scatterer, and $I_{0}$ is the modified zeroth order Bessel function of the first kind.

An experiment (see Figure \ref{fig:Experimental-Set-Up}) was set up to collect a real dataset to analyze. The experiment was run on a $135 m^2$ office full of obstacles (interacting objects). The transmitter was an Odroid C1 Linux computer with a Ralink RT5370 USB Wifi adapter. The receiver was a (fixed in space) Motorla Moto G mobile phone. Data was collected using the Kismet \footnote{https://www.kismetwireless.net/index.shtml} platform (an 802.11 layer2 wireless network detector, sniffer, and intrusion detection system), with some ad hoc, home made, software extensions, mostly for data aggregation. A total of 3,177 samples (power level measured in dBm) were collected during one hour experiment.

\begin{figure}[h]
\centering\includegraphics[scale=0.2]{experiment}
\caption{\label{fig:Experimental-Set-Up}Experimental Set Up}
\end{figure}

Next table summarizes the results of applying the three considered models (uniform, Raylegh and Rice) and the optimal encoding using a Huffman code:

\begin{table}[h]
\centering
\begin{tabular}{l l l}
\toprule
\textbf{Model} & \textbf{LDM} & \textbf{Nescience} \\
\midrule
Uniform & 17,351 & 1.30 \\
Rayleigh & 13,229 & 0.75 \\
Rice & 11,118 & 0.47 \\
Huffman & 7,541 & - \\
\bottomrule
\end{tabular}
\caption{Nescience of Models}
\end{table}

The uniform model, that is, assuming zero knowledge about the topic covered by the dataset, has a nescience of 1.30. This value is a kind of upper level for the nescience associated with that particular topic and dataset; any model with a higher nescience should be classified as zero knowledge model. If we introduce the knowledge that in a environment with multiple obstacles the signal propagation can be described as a Gaussian process (Rayleigh distribution), we are able to decrease our nescience to 0.75, that is, there were a 43\% improvement in our understanding of the topic. If we add the knowledge that there is usually a strongly dominant signal seen at the receiver caused by a line of sight between the antenna and the mobile phone (Rician distribution), the nescience decreases to 0.47, and so, we have achieved an additional 23\% gain in our understanding. Given that numbers we can conclude that the Rayleigh model increases our knowledge with respect to the uniform model, and that the Rice model does so with respect to Rayleigh. However, the nescience of this last model is 0.47. That means that there still patterns in the dataset that are not explained by the Rice model, or what it is equivalent according to our methodology, there is still some knowledge to discover and learn.

The methodology has been applied to a dataset gathered in a single experiment under a controlled environment, since the goal of this Chapter was to provide a methodology to quantify the nescience of structured datasets, not to evaluate models for signal propagation and fading. In order to to conclude that, in general, the Rice model is an improvement over Rayleigh, a more realistic experiment is required, with multiple datasets gathered in real environments.

%
% Section: The Analysis of the Incompressible
%

% \section{The Analysis of the Incompressible}

% As we have said in Chapter {chap:Introduction}, one of the reasons to understand how things work is to understand the cause-effect relation in systems. We are interested in this cause/effect relation in two ways. That is, if we want to see an effect in a system, we want to understand wich causes trigger that effect. Also, and perhaps more interesting, if we have observed an (probably undesired effect) in a system we would like to discover what has caused that effect, so we can fix it, and revert the normal situation.

% We could use the theory of nescience to model, and modify, those uncommon effect, by means of training a model and looking at the incompressible part of the data.


% A model $\mathcal{M}$ for a dataset $\mathcal{D} = (X, y)$ is a compressed version of that dataset, since the length of the dataset given the model $l(\mathcal{D} \mid \mathcal{M})$ is smaller that then length of the original dataset $l(\mathcal{D})$. The model $\mathcal{M}$ is composed by the regularities found in the dataset (subject to the algorithm used and the families of models considered). What it is left, $\mathcal{D} \mid \mathcal{M}$ is the incompressible part of the dataset, that is, those samples that have no regularity at all, or present a regularity that requires a description longer than the length of the raw data.

% In this section we are going to show the practical applications of analyzing what it is left, that is, the incrompressible samples of a dataset. An element that is incompressible represents a very unlikely, or uncommon, situation of the entity being studied. A incompressible element does not necessarily means a problem, since if a problem is sufficiently common, it can be compressed. An incompressible element is something than cannot be explained given the normal behaviour of the system. Of course, all of this is assuming that our dataset has no errors.

% Once we have found a model that has the lowest possible nescience for a dataset, we could separate those elements that have not been compresed, denoted by XX, and fit a second model. We could argue that it does not make any sense to model the incompressible part, since, it is incompressible. However, the incompressible part is incompressible with respect to the original entity under study, that is, the global systen. And in this new case, we are studying a different entity, namely, the uncommon parts of an entity. It might happen that we can find regularities in this new entity.



%
% Section: References
%

\section*{References}
\label{sec:ML_references}

The quantity $1 - I(\mathbf{x};\mathbf{y}) / \max\left\{ H(\mathbf{x}),H(\mathbf{y})\right\}$ has previously been proposed in~\cite{ferri2009experimental} as a candidate definition of \emph{normalized mutual information}. However, to the best of our knowledge, it has not been used in practice.

A insightful description of the differences between explanation models and predictive models, how these models are used in different scientific disciplines, and what are the implications for the process of statistical modeling can be found in \cite{shmueli2010explain}.

The application of the minimum description length principle to the identification of optimal decision trees have been proposed in \cite{quinlan1989inferring}, further refined and clarified in \cite{wallace1993coding}; however the coding method proposed by those authors is different from the one used in this book.

The Minimum Description Length \cite{grunwald2007minimum} and the Minimum Message Length \cite{wallace2005statistical} techniques have been applied to the problem of inferring decision trees in \cite{quinlan1989inferring}, later on clarified and extended in~\cite{wallace1993coding}, in \cite{mehta1995mdl} as a technique for pruning, and in \cite{rastogi1998public}, among others. Although the underlining concepts behind the cost function proposed in this chapter are the same (namely, that learning is equivalent to the capability to compress), our approach is very different from the ones described in these works.


% {\color{red} TODO: Find the original reference of the AirPassengers dataset.}

% {\color{red} TODO: Find the original reference of the Appliance energy consumption dataset. https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction}


% \begin{verbatim}

% The normalized compression distance of two vectors $x$ and $y$ computed using a compressor based on optimal codes is equivalent to the normalized mutual information of these two vectors, that is:
% \[
% NCD_C\left(\mathbf{x},\mathbf{y}\right)=1-\frac{I\left(\mathbf{x};\mathbf{y}\right)}{\max\left\{ H\left(\mathbf{x}\right),H\left(\mathbf{y}\right)\right\} }
% \]
% The quantity $1 - \frac{I\left(\mathbf{x};\mathbf{y}\right)}{\max\left\{ H\left(\mathbf{x}\right),H\left(\mathbf{y}\right)\right\} }$ has been already proposed in~\cite{ferri2009experimental} as a candidate definition of the concept of Normalized Mutual Information. However, to the best of our knowledge, it has not been used in practice.

% \end{verbatim}