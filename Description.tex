%
% CHAPTER: Entities, Represenations and Descriptions
%

\chapterimage{owl.pdf} % Chapter heading image

\chapter{Entities, Representations and Descriptions}
\label{cha:Topics-and-Descriptions}

\begin{quote}
\begin{flushright}
\emph{We are all agreed that your theory is crazy. \\
The question which divides us is whether it is crazy enough.} \\
Niels Bohr
\end{flushright}
\end{quote}
\bigskip

The first step to quantitatively measure how much we do not know is to identify clearly the collection of research entities under study. The exact elements of this collection is something that depends on the particular application in which the theory of nescience is being used. Different applications require different collections of entities (mathematical objects, living things, human needs, etc.). Fortunately, the procedure to compute how much we do not know is the same in all cases.

The second step is to provide a method to encode the set of identified entities as strings of symbols, what we call representations. How to properly encode a research entity with symbols is a difficult, still unsolved, epistemological problem. The solution we propose in the theory of nescience is based in the concept of oracle Turing machine. How easy is to implement this solution in practice is something that depends on how abstract are the entities under study. For example, the collection of all abstract mathematical objects is a very difficult set to encode, and so, an approximation has to be found; the collection of all possible computer programs (given that our area of interest is software quality, see Chapter \ref{chap:Software-Engineering}) is far easier to encode, since computer programs are strings themselves.

The final step, once we have found a way to properly encode the original set of entities as string-based representations, is to provide a description, as accurate and succinct as possible, of what we already know about those representations. In the theory of nescience we require that descriptions must be computable, that is, given a description, a computer should be able to fully reconstruct the original representation. A difficult problem that arise with descriptions is that they characterize representations, that is, the encoding of entities, not the entities themselves, and so, the quality of a description for an entity is conditional to the quality of the representation used.

In this chapter we will formalize all these concepts: entities, representations, descriptions, and many others. We will also see what we mean by perfect knowledge, how to compute the combined representation of multiple entities, and the description of a representation assuming a perfect knowledge of another one.

{\color{red} Is perfect knowledge covered in the chapter? If not, remove this sentence.}

%
% Section: Entities
%

\section{Entities}
\label{sec:descriptions_entities}

What exactly is a research entity is a difficult, still unsolved, philosophical problem. Our approach to address this complex issue is eminently practical. Our theory starts by assuming there exists a non-empty collection of \emph{entities}\index{Entities} we would like to understand.

\begin{notation}
Denote by $\mathcal{E}$ the set of research entities.
\end{notation}

The exact elements that compose $\mathcal{E}$ is something that depends on the particular domain in which the theory of nescience is being applied, but they usually corresponds to an area or subarea of knowledge. Examples of sets of entities could be: research elements in mathematics (abstract); the kingdom of animalia (living things); known and unknown human needs (abstract); all possible computer programs (strings), etc. Technically speaking $\mathcal{E}$ is not a well defined set, since in general we cannot tell what is and what is not a member of this set.

The abstract nature of $\mathcal{E}$ has some advantages, but also introduces important limitations. The main limitation is that our definition of nescience is, in general, a non-computable quantity, and so, it must be approximated in practice. The main advantage is that we can apply the new concepts and methods introduced in this book to other problems, not only to the discovery of new scientific knowledge.

In the theory of nescience we do not allow universal sets, that is, we cannot assume the existence of a set $\xi$ that contains everything. The problem of universal sets is that they violate Cantor's theorem (see Example \ref{cantor_theorem}). Cantor's theorem states that the power set $\mathcal{P}(\xi)$ composed by all the possible subsets of $\xi$ has more elements than the original set $\xi$, and this is a contradiction with the fact that $\xi$ contains everything. In the theory of nescience the set $\mathcal{E}$ must be the set of "something".

\begin{example}[Cantor's theorem]
\label{cantor_theorem}
The Cantor's theorem states that for every set $A$ we have that $d(A) < d\left(\mathcal{P}(A)\right)$. Let $f: A \rightarrow \mathcal{P}(A)$ a function that maps every element of $x \in A$ to the set $\{x\} \in \mathcal{P}(A)$; clearly $f$ is injective, and so, $d(A) \leq d\left(\mathcal{P}(A)\right)$. In order to prove that the inequality is strict assume there exist a surjective function $g: A \rightarrow \mathcal{P}(A)$, and consider the set $B = \{ x \in A : x \notin g(x) \}$. Since $g$ is surjective there must exists an $y \in A$ such that $g(y) = B$. However this rises the contraction $y \in B \Leftrightarrow y \notin g(y) = B$. Consequently, the function $g$ does not exists, and so $d(A) < d\left(\mathcal{P}(A)\right)$.
\end{example}

Not all possible sets are valid sets in the theory of nescience, because some sets lead us to paradoxes. For example, the Russel paradox propose to consider the set $R$ composed by all those sets that are not members of themselves; the paradox arises when we try to answer the question of whether $R$ is a member of itself (see Example \ref{ex:russell_paradox}). In order to avoid these kind of problems, the theory of nescience is based on the Zermelo-Fraenkel set of axioms together with the axiom of choice (see Appendix \ref{apx:math}). Russell's paradox is due to a misuse of the set builder notation $\{ : \}$. The \emph{axiom of separation} (if $P$ is a property with parameter $p$, then for any set $x$ and parameter $p$ there exists a set $y=\{u \in x : P(u) \}$ that contains all those sets $u \in x$ that have property $P$) only allows the use of this notation to construct sets that are subsets of already existing sets. A more general \emph{axiom of comprehension} (if $P$ is a property, the there exist a set $y=\{u : P(u) \}$) would be required to allow sets like the one proposed by Russell's paradox. In the ZFC axioms, and in the theory of nescience, the axiom of comprehension is considered to be false.

\begin{example}[Russell's Paradox]
\label{ex:russell_paradox}
Let $R$ be the set composed by all sets that are not members of themselves, that is, $R = \{ x : x \notin x \}$. The contradiction arises when we ask if $R$ is member of itself. If $R$ is not a member of itself, according to its own definition it should be; however if we say that $R$ is a member of itself, the definition tell us that it should not be. Symbolically $R \in R \Leftrightarrow R \notin R$.
\end{example}

In the theory of nescience we do not deal with classical ontological questions, that is, about the classes of things that there exists in the world and that can be known. Also, we do not try to answer any kind of epistemological issues, like for example, how scientific knowledge is validated by appeal to evidence, or what it is the nature of that evidence.

Once we have selected a set $\mathcal{E}$ of entities, the next step is to uniquely encode their elements as strings of symbols, otherwise it would not be possible to properly describe them (unless entities are strings of symbols themselves). How to properly perform this encoding is described in the next section.

%
% Section: Representations
%

\section{Representations}
\label{sec:representations}

How to represent the, possibly abstract, entities of a set $\mathcal{E}$ is a difficult epistemological problem that has been the subject of research for more than two thousand years (see Section \ref{sec:scientific_representation} for a short review of problems and open questions). In this section we describe a novel solution to this problem, and we will study its advantages and drawbacks. We propose to split the scientific representation problem it two complementary subproblems: how descriptions characterize representations, and how representations encode entities. In this sense, scientific descriptions are models of entities by virtue of an indirect relation through representations. In this section we will focus on the encoding part, and Section \ref{sec:descriptions_models} will be about descriptions.

In the discipline of Kolmogorov complexity, researchers solve the problem of representation by assuming that the set $\mathcal{E}$ is well defined, countable, and that there exists a total encoding function $f:\mathcal{E} \rightarrow \mathbb{N}$ from the set of entities to the set of natural numbers (a kind of GÃ¶del numbering). In the theory of nescience we borrow this idea of encoding entities with numbers, or equivalently, strings of symbols. The main difference is that we address the problem of encoding any arbitrary set $\mathcal{E}$ by turning it around, that is, by defining a function $f_\mathcal{E}:\mathcal{S}^\ast \rightarrow \mathcal{E}$ from the well defined set $\mathcal{S}^\ast$ of all possible finite strings from an alphabet $\mathcal{S}$ to the, perhaps not countable and even not well defined, set $\mathcal{E}$ of entities under study.

Our function $f_\mathcal{E}$, that depends on the set $\mathcal{E}$, is a kind of oracle (inspired by the concept of oracle Turing machine from Definition \ref{def:Oracle-Turing-Machine}) that is able to fully reconstruct the entity $f_\mathcal{E} (x) \in \mathcal{E}$ given its encoding $x \in \mathcal{S}^\ast$, without requiring any external information. Of course, this oracle is a conceptual idea that has no equivalence in the real world for the majority of the sets $\mathcal{E}$, but assuming its existence will allow us to explain and prove important properties of how the process of scientific discovery works. We assume that there exists a single, unique, physical world, that world is independent of observers, and that for some collections of entities $\mathcal{E}$ there exists an oracle $f_\mathcal{E}$ (scientific representation problem).

Without any loss of generality, for the rest of this book we will only consider binary strings as encoding of entities.

\begin{definition}
\label{def:descriptions_topic}
Let $\mathcal{E}$ be a set of entities, and let $f_\mathcal{E}:\mathcal{B}^\ast \rightarrow \mathcal{E}$ be the an oracle, called \emph{representation function}, from the set $\mathcal{B}^\ast$ of all possible finite binary strings to the set $\mathcal{E}$ of entities under study.
\end{definition}

Only the elements of the set $\mathcal{B}^\ast$, that is, binary strings, can serve as representations (problem of ontology). We do not allow drawings, or any other form of physical models, unless they are converted into binary strings. We do not make any distinction between a scientific representations and any other kind of representations (demarcation problem). It is up to the oracle to decide if a string is a representation or not of a given entity. It is also important to note that the function $f_\mathcal{E}$ is total, that is, all possible strings represent entities (no targetless models allowed).

According to the theory of nescience, scientific research would be not only about figuring out how to properly encode entities, but also about discovering the inner workings of decoding oracle. The capacity of the oracle of reconstruct the original entities is what justifies the fact that we can formulate hypotheses about entities given their representations (surrogative reasoning).

The entities of $\mathcal{E}$ might be encoded in multiple ways (problem of style). Our oracle $f_\mathcal{E}$ admits all the possible encodings of an entity as representations of that entity, including the wrong representations and the incomplete ones.

\begin{example}
When the topics under study are animals we could use as encodings a detailed description of the body of those animals. In this case, the oracle would be an hypothetical machine that given its description is able to reproduce the original animal. An alternative encoding we could consider is to use the DNAs of the animals. Both encodings are possible representations from the point of view of the oracle.
\end{example}

Some strings are better representations of an entity than others (standard of accuracy). The more information is missing from a representation, the harder is the work of the oracle. We are interested in those representations that make the work of the oracle as easy as possible.

We must be careful with those representations that do not capture all the details about the original entities, since the results of our analyses could present some bias. In Chapter \ref{chap:Miscoding} we will study in detail the problem of errors due to the miscoding of abstract entities, and we will see that not only representations are about entities, but also we require that entities should be about representations (requirement of directionality).

\begin{example}
The data used in time of Ptolomeo about the position of celestial bodies along the year was a misencoding of the entity "position of celestial bodies". A better encoding was the data provided by Tycho Brahe. Today, we have even betters encoding.
\end{example}

Ideally, from a theoretical point of view, the oracle should be just a universal machine, and representations should include all the instructions required to reconstruct the original entities. Unfortunately, in practice, that would be impossible for the majority of entities, and so, we require additional intelligence from the side of the oracle. Moreover, the oracle has also to deal with incomplete, and even wrong, representations.

\begin{definition}
\label{def:descriptions_topic}
Let $e \in \mathcal{E}$ and entity. We call the set of representations for $e$, denoted by $\mathcal{R}_e$, to $f_\mathcal{E}^{-1}(e)$.
\end{definition}

\begin{figure}[h]
\centering\includegraphics[scale=0.5]{entities_topics_1}
\caption{\label{fig:entities_topics_1}Encodings and Entities.}
\end{figure}

A consequence of working with finite strings as representations is that it might happen that there exist entities that are not encoded by any representation (see the gray areas in Figure \ref{fig:entities_topics_1}, in particular, the entity $e_2$ is not encoded by any representation). Intuitively, we could say that for some domains of knowledge the number of problems is much higher than the number of solutions.

\begin{example}
If the collection of entities under study are real numbers, it turns out that there exist numbers that can not be encoded using finite binary strings, since the set $\mathbb{R}$ has the cardinality of the continuum, and the set $\mathcal{B}^\ast$ is numerable.
\end{example}

We distinguish between \emph{knowable} and \emph{unknowable} entities.

\begin{definition}
We say that an entity $e \in \mathcal{E}$ is \emph{knowable} if there exists at least one $r \in \mathcal{B}^\ast$ such that $f_\mathcal{E}(r) = e$. An entity $e \in \mathcal{E}$ is \emph{unknowable} if it is not knowable.
\end{definition}

We assume that, a priori, it is not possible to determine if a set of entities $\mathcal{E}$ is knowable, unknowable, or partially unknowable, and that selecting the right knowable entities to study is a matter of trial and error. In this book  we have nothing to say about unknowable entities, beyond that the unknowability of an entity cannot be proved nor discovered.

\begin{definition}
Let $e \in \mathcal{E}$ a knowable entity. We call the set of possible representations for $e$, denoted by $\mathcal{R}_e$, to $f_\mathcal{E}^{-1} (e)$.
\end{definition}

Since, in general, our knowledge about the entities and the inner workings of the oracle are incomplete, in practice we will working with another set $\hat{\mathcal{R}}_e \subseteq \mathcal{B}^\ast$ of strings that we believe are close to the representations $\mathcal{R}_e$ that encode the entity $e$. The elements that belong to $\hat{\mathcal{R}}_e$ usually change over time, as we better understand the entities of $\mathcal{E}$, and how the oracle encode these entities as strings. The more abstract is our set of entities $\mathcal{E}$, the more difficult will be to approximate them as strings in $\hat{\mathcal{R}}_e$.

\begin{example}
\label{ex:luminiferous_ether}
The entity "luminiferous ether" was a theoretical postulate about a hypothetical medium in which the light would propagate. The ether was used as an explanation of how a wave-based light could propagate through the empty space. In 1887, the results of the Michelson-Morley experiment suggested that the ether did not exist, and after Einstein formulated his special theory of relativity, that successfully explained how light propagates through empty space, the idea of ether was completely dropped.
\end{example}

A consequence of working with approximations (the set $\hat{\mathcal{R}}_e$) instead of valid representations (the set $\mathcal{R}_e$) is that some of the candidate strings currently in use might a different entity from what we were expecting.

\begin{example}
In 1961, the Soviet physicist Nikolai Fedyakin, performed a series of experiments resulting in what was seemingly a new form of water. The new water, called polywater, showed a higher boiling point, a lower freezing point, and much higher viscosity than ordinary water. Later experiment showed that polywater was nothing more than contaminated water with small amounts of impurities.
\end{example}

Finally, we would like to clarify that our aim encoding entities is different from that of Shannon's information theory, as Example \ref{ex:shannon_encoding} shows.

\begin{example}
\label{ex:shannon_encoding}
Consider a set composed by two books, "The Ingenious Nobleman Sir Quixote of La Mancha" and "The Tragedy of Romeo and Juliet". We could encode the fist book with the string "0" and the second one with the string "1". Although those strings allow us to uniquely identify each book in the set, they are not proper encodings in the sense of the theory of nescience. Information theory is about how to uniquely identify an object given a reference and it requires that both, sender and received agree about a mapping between references and objects. Meanwhile, in the theory of nescience we are interested in how to provide a representation that captures all the details and nuances of the original objects. For example, given the strings "0" and "1" it is not possible to make any hypothesis about the influence of Cervantes in the work of Shakespeare. Technically speaking, the size of the oracle, the mappings between references and objects, required by Shannon information theory is not minimal.
\end{example}

An alternative way to deal with the problem described in Example \ref{ex:shannon_encoding}, where we used artificially short descriptions, would be to require the set $\mathcal{E}$ to be infinite, as Kolmogorov complexity does (we can not cheat an infinite number of times). However, even requiring the set of entities to be infinite, we can not guarantee that the highly desirable property of surrogative reasoning is satisfied.

One of the problems of science, and in general of any human intellectual activity, is that people tend to confuse symbols with what they represent. The theory of nescience has been carefully designed to avoid this problem, by means of clearly stating the difference between research entities and the representation of entities. However, keeping this distinction always explicit in the explanations would make the book very difficult to read. We have tried to find a compromise between clarity in the exposition and rigor in the definitions. Sometimes, during the introduction of new ideas we talk in general about a \emph{topics}, meaning an entity, a representation, or both, an entity and its representation. But, in the mathematical definitions and propositions we always make this difference unequivocal. In case of doubt about what we mean, please take the mathematical definitions as the authoritative reference. 

{\color{red} Perhaps we could mention that one entity can have more than one style of representation.}

%
% Section: Joint Representations
%

\section{Joint Representations}
\label{sec:descriptions_joint_topic}

We have seen in the previous section that usually there exists more than one possible representation of an entity $e \in \mathcal{E}$, what we have called the set $\mathcal{R}_e$. Some of these representations have a high quality, in the sense that they contain all the information required by the oracle to reconstruct (in whatever way the oracle manages to do that) the original entity. But also, in the set $\mathcal{R}_e$ there exists low quality representations, that is, representations that lack many of the details needed to fully reconstruct the entity\footnote{As we have said, the oracle cannot query an external service to retrieve the information missing from a representation. Instead, what the oracle does is to find the closest representation to the string provided that is complete, and assume that both describe the same entity (see Chapter \ref{chap:Miscoding}).}. If we want to increase our knowledge about an entity, that is, decrease our nescience, we have to find the best possible representation. One way to do that is to simply try different strings until we come up with a good enough representation. A more efficient method would be to complement our current bad representation with more symbols, or by combining those known representations that contain partial information. These two latter approaches require the introduction of the concept of joint representation.

\begin{definition}
Let $s, t \in \mathcal{B}^\ast$ be two different representations. We call the \emph{joint representation} of $s$ and $t$ to the concatenation string $st$.
\end{definition}

The concatenation of any two arbitrary representations is also a representation.

\begin{proposition}
Let $s, t \in \mathcal{B}^\ast$ be two different representations and $st$ its joint representation, then $st$ is also a representation.
\end{proposition}
\begin{proof}
As we have seen in Section \ref{sec:strings} the set $\mathcal{B}^\ast$ is closed under the operation of concatenation of strings.
\end{proof}

Note that we do not require the set of representations $\mathcal{R}_e$ for a given entity $e \in \mathcal{E}$ to be closed under the operation of concatenation. That is, it might happen that $st \notin \mathcal{R}_e$, even if it is the case that $s \in \mathcal{R}_e$ and $t \in \mathcal{R}_e$.

\begin{example}
\label{ex:lung_cancer}
Suppose that the research entity $e$ in which we are interested is the causes of lung cancer. In order to understand this entity, we have measured a collection of risk factors in a random sample of the population (smooking, exercise, diet, age, etc.). However, due to a problem with the sampling procedure, all the samples correspond to a subset of the population, for example, males. This dataset would be a representation $s$ for our entity $e$, but a very bad one, since it is biased. If we have a second representation, corresponding the sample data of females $t$, the joint representation $st$ will be a better one that any of them, $s$ or $t$, isolated.
\end{example}

The operation of concatenation is associative, that is, $(rs)t = r(st)$, for all $r, s, t \in \mathcal{B}^\ast$.

\begin{proposition}
The set $\mathcal{B}^\ast$ of representations together with the operation of concatenation has the structure of free monoid.
\end{proposition}
\begin{proof}
As we have seen in Section \ref{sec:strings} the operation of concatenation is associative, and the empty string $\lambda$ plays the role of neutral element.
\end{proof}

It might happen that there exists some research entities whose existence we do not know yet. That is, there are knowable entities in $\mathcal{E}$ (they can encoded by strings in $\mathcal{B}^\ast$) that we are not aware of their existence, since nobody has studied them. The collection of those entities is what we call the \emph{unknown unknown} (see Section \ref{sec:New_Research_Topics}). We are interested in to investigate if there exists a procedure to discover those unknown research entities. A possible approach could be by means of combining the representations of already known entities, since all possible strings represent something (see Section \ref{sec:New_Research_Topics}).

The concept of joint representation can be extended to any arbitrary, but finite, collection of representation. In this way, we could add multiple entities to our research, or to the process of discovering new entities.

\begin{definition}
Let $t_1, t_2, \ldots, t_n \in \mathcal{B}^\ast$ be a finite collection of representations. We call the \emph{joint representation} of $t_1, t_2, \ldots, t_n$ to the string $t_1 t_2 \ldots t_n$.
\end{definition}

It is easy to show that $t_1 t_2 \ldots t_n \in \mathcal{B}^\ast$ for all $t_1, t_2, \ldots, t_n \in \mathcal{B}^\ast$, that is, $\mathcal{B}^\ast$ is closed under the operation of concatenation of multiple, finite, topics.

{\color{red} Clarify why concatenation has to be conmutative.}


%
% Section: Descriptions
%

\section{Descriptions}
\label{sec:descriptions_models}

So far, our aim with the strings of $\mathcal{B}^\ast$ has been to provide an enconding, or representation, as complete and detailed as possible of the entities of $\mathcal{E}$, no matter its length. However, as we have said in the preface of this book, human understanding requires the derivation of concise models for those entities, since human reasoning cannot be based on long representations.

\begin{example}
In Example \ref{ex:lung_cancer} we have shown that a good representation for the entity "lung cancer" could be a sample dataset in which we measure different risk factors. If someone decides to quit smooking is not because they know and understand this large sample dataset, but because they know and understand the much simpler derived model "smooking increases the risk of lung cancer".
\end{example}

A description or model\footnote{In the theory of nescience use the words "description" and "model" interchangeably.} is a finite binary string mapped to a representation of an entity (recall Figure \ref{fig:entities_topics_models} from Chapter \ref{chap:Introduction}). Descriptions do not model entities (target systems) directly, they do so through string based representations.

In the theory of nescience we require that descriptions must be computable, so we can fully and effectively reconstruct the original representations given their descriptions. The requirement of computability allows us to clearly state the limits of the concept of "description". For example, the problem of self-referential descriptions, like the Berry paradox\index{Berry paradox}, can be addressed in the scope of the limits of computation.

\begin{definition} [Model]
\label{def:descriptions_model}
Let $d \in \mathcal{B}^\ast$ be a binary string in the form $d = \langle TM,a \rangle$, where $TM$ is the encoding of a prefix free Turing machine and $a$ is the input string to that machine. If $TM(a)$ is defined, we say that $d$ is a \emph{description}\index{Description}. 
\end{definition}

The output of the description $TM(a)$ is a string, that is, the representation of an entity. Intuitively, a description is composed by two parts, a Turing machine that compresses all the regularities found in this representation, and a string containing what is left, that is, the non-compressible part. From an otological point of view, descriptions are just string based representations that satisfy some additional requirements, like being computable. In this sense, descriptions are also representations themselves, i.e., the set of descriptions is a subset of the set of representations.

\begin{definition}
\label{def:descriptions_model}
We define the \emph{set of descriptions}\index{Set of descriptions}, denoted by $\mathcal{D}$, as:
\[
\mathcal{D} = \{ d \in \mathcal{B}^\ast : d = \langle TM,a \rangle \wedge TM(a) \downarrow \}.
\]
Let $r \in \mathcal{B}^\ast$ be a representation. We define the set of \emph{descriptions for $r$}, denoted by $\mathcal{D}_r$, as:
\[
\mathcal{D}_r = \{ d \in \mathcal{D} : TM(a) = r \}.
\]
Finally, given an entity $e \in \mathcal{E}$, we define the set of \emph{descriptions for $e$}, denoted by $\mathcal{D}_e$, as:
\[
\mathcal{D}_e = \{ d \in \mathcal{D} : \exists r \in \mathcal{R}_e,\, TM(a) = r \}.
\]
\end{definition}

Given that descriptions are representations themselves, there exists descriptions that describe descriptions. From a practical point of view it is not a good idea to use descriptions as the representation of entities, since what we are looking for in a good representation is that they contain as much inforformation as possible about the original entities, not an encoding as concise as possible. Working with descriptions in the role of representations would make the job of scientific discovery very difficult.

\begin{notation}
In case of dealing with multiple represenations and descriptions, and in order to avoid ambiguity, we will denote by $d_r$ the description $d$ for the representation $r$.
\end{notation}

Since each description describes one, and only one, representation, we can define a function that maps descriptions into representations. Given that descriptions are Turing machines, it is natural to use as description function a universal Turing machine. Consequently, not only the individual descriptions of representations are computable, but also the function that maps descriptions into representations is computable.

\begin{definition}
We call \emph{description function}\index{Description function}, denoted by $\delta$, to any universal Turing machine $\delta : \mathcal{D} \rightarrow \mathcal{B}^\ast$ that maps descriptions to their corresponding representations.
\end{definition}

If $d$ is a description of the representation $r$, then we have that $\delta \left( d \right) = \delta \left( \langle TM, a \rangle \right) = TM(a) = r$.

Inspired by the Occam's razor principle\index{Occam's razor principle}\footnote{The Occam's razor principle refers to the number of assumptions of an explanation, not to the length of the explanation itself.}, if two explanations are indifferent, we should prefer the shortest one. Therefore, the limit of what can be known (understand) about a representation, that is, its perfect model, is given by the shortest description that allows us to reconstruct this representation. Of course, what we know about the original entity is conditional to the quality of the representation.

\begin{definition}
\label{def:descriptions_perfect_model}
Given the set of descriptions $\mathcal{D}_r$ of a representaton $r \in \mathcal{B}^\ast$, let $d_r^{\star} \in \mathcal{D}_r$ be the shortest possible description of $r$ using the standard shortlex ordering. We call $d_r^{\star}$ the \emph{perfect description}\index{Perfect description} of the representation $r$.
\end{definition}

Unfortunately, the perfect description of a representation is in general not known and, as Proposition \ref{prop:nescience-kolmogorov} shows, there exist no algorithm to compute it. In practice what we have to do is to use an approximation to estimate how far our current best description is from the perfect one, that is, how much we do not know about a particular representation for an entity (see Chapter \ref{chap:Redundancy}).

\begin{proposition}
\label{prop:nescience-kolmogorov}
Given a representation $r \in \mathcal{B}^\ast$, we have that $l \left( d_r^{\star} \right) = K\left( r \right)$.
\end{proposition}
\begin{proof}
Apply Definition \ref{def:Kolmogorov-Complexity} and the fact that we require that the Turing machines $TM$ used in definitions $\langle TM,a\rangle$ must be prefix-free.
\end{proof}

The actual length of a description $l \left( d \right)$ for a representation $r$ is something that depends on the particular enconding of Turing machines used. The encoding method is given by the description function $\delta$ used. Fortunately, if we replace our description function by a different one, the length of perfect models do not change (up to an additive constant that does not depend on the representations themselves).

\begin{corollary}
Let $r \in \mathcal{B}^\ast$ be a representation, $\delta$ and $\dot{\delta}$ two different description functions, and $d_r^{\star}$ the perfect description of the representation $r$ using $\delta$ and $\dot{d}_r^{\star}$ the perfect description using $\dot{\delta}$, then we have that $l \left( d_r^{\star} \right) \leq l \left( \dot{d}_r^{\star} \right) + c$ where $c$ is a constant that does not depend on $r$.
\end{corollary}
\begin{proof}
Apply Theorem \ref{prop:nescience-kolmogorov} and Theorem \ref{def:Invariance-theorem}.
\end{proof}

In general, in the theory of nescience we are not interested in computing the actual value of the nescience about an entity given a description and a representation. Instead what we are interested is in the ordering of the different possible pairs of descriptions and representations given their nescience. In this sense, the details of the particular universal Turing machine used in practice are not relevant\footnote{Do not confuse the inner workings of the universal Turing machine that maps descriptions to representations, in which we are not interested, with the inner workings of the universal oracle Turing machine that maps representations to entities, in which we are interested, since this knowledge is critical to understand how things work.}. For the rest of this book we will assume that $\delta$ is fixed to a reference universal Turing machine. For example, in Section \ref{sub:ax_type_theory} we use as univeral Turing machine the lambda calculus. Alternatively, the reader could consider that all the theorems provided in this book that deal with the length of shortest models are valid up to an additive constant that does not depend on the topics themselves.

A remarkable consequence of Proposition \ref{prop:nescience-kolmogorov} is that perfect descriptions must be incompressible, that is, \emph{perfect knowledge implies randomness} (see Section \ref{sec:incompressibility_randomness}).

\begin{corollary}
Let $d_r^{\star}$ be the perfect description of a representation $r$, then we have that $K \left( d_r^{\star} \right) = l \left( d_r^{\star} \right)$.
\end{corollary}
\begin{proof}
Having $K \left( d_r^{\star} \right) < l \left( d_r^{\star} \right)$ would be a contradiction with the fact that $d_r^{\star}$ is the shortest possible description of $r$.
\end{proof}

The converse, in general, does not hold, since we could have a random description that it is not the shortest possible one, that is, a description $d$ for a reprsentation $r$ such that $l(d) = K(d)$ but $l(d_r^{\star}) < l(d)$.

\begin{example}
\label{ex:description_neural}
We could define a deep neural network\index{Neural network} with an input layer of one thousands nodes, ten hidden layers of fifty thousands nodes each, and an output layer of one thousand nodes, and then train the network to output a fixed string of one thousand 1's for any given input string. The Kolmogorov complexity of this neural network is much higher that the complexity of a string of one thousand 1's.
\end{example}

There is little value on a descriptions that is longer than the representation it describes.

\begin{definition}
\label{def:trivial_model}
Let $r \in \mathcal{B}^\ast$ be a representation, and $d \in \mathcal{D}_r$ one of its descriptions. If $l(d) \geq l(r)$, we say that $d$ is a \emph{pleonastic description}\index{Pleonastic description} of the representation $r$.
\end{definition}

\begin{example}
\label{ex:topics_models_graph}
Consider the set of all possible finite graphs\index{Graph}. Since graphs are abstract mathematical objects, we need a way to represent them as strings, for example, by using a binary encoding of their adjacency matrices (see Section \ref{sec:Graphs} for an introduction to graphs). The description $d = \langle TM, r \rangle$, where $r$ is the representation of a graph and $TM$ is a Turing machine that just halts, will be part of $\mathcal{D}_r$ since $TM(r) = r$. We are concerned about the fact that this description may not be shortest possible description of $r$.
\end{example}

It might happen that there is no shortest possible description of a representation than the representation itself. This is the case when representations are random, or incompressible, strings. And, as we have seen in Section \ref{sec:incompressibility_randomness} the overhelming majority of strings are incompressible. It does not make any sense to do research about random representation, since it is not possible to find shorter models for that representation.

The concept of perfect description can be generalized from individual representations to entities. This generalization allow us to study the nature and properties of these entities.

\begin{definition}
\label{def:entities_perfect_model}
Given the set of descriptions $\mathcal{D}_e$ of an entity $e \in \mathcal{E}$, let $d_e^{\star} \in \mathcal{D}_e$ be the shortest possible description of $\mathcal{D}_e$ using the standard shortlex ordering. We call $d_e^{\star}$ the \emph{perfect model}\index{Perfect model} of entity $e$.
\end{definition}

For each possible representation $r$ of $e$ we can compute its perfect description $d_r^{\star}$. Then, the perfect description $d_e^{\star}$ for $e$ would be the shortest of this collection of perfect descriptions of the representations.

An interesting case is when all the descriptions that compose $\mathcal{D}_e$ are pleonastics, that is, there is no model that it is shorter than the representation for all the possible representations of the entity. That would the case if all the representations of the entity $e$ are random strings. In this particular case, scientific research would be doomed, since it is not possible to find a suitable model for $e$. The fact that we can understand and make predictions about $e$ will be limited by the length of the non-compressible representations of $e$.

{\color{red} Elaborate a little bit more over the concept of perfect description of an entity.}

%
% Section: Models for Joint Representations
%

\section{Descriptions for Joint Representations}
\label{sec:description_joint_represenation}

In Section \ref{sec:descriptions_joint_topic} we introduced the concept of joint representation $ts$ of two individual representations $t$ and $s$. In this section we are interested in to study how the length of the optimal description of a joint representation relates to the length of the optimal descriptions of the individual representations.

The length of the perfect description of a joint representation is greater or equal than the length of the perfect description of any of the individual representations. That is, the more information we include in a representation, the longer will take to describe it. 

\begin{proposition}
\label{prop:joint_length}
Given any two representations $t,s \in \mathcal{B}^\ast$, we have that $l \left( m_{ts}^{\star} \right) \geq l \left( m_{t}^{\star} \right)$ and $l \left( m_{ts}^{\star} \right) \geq l \left( m_{s}^{\star} \right)$, where $m_{t}^{\star}$, $m_{s}^{\star}$ and $m_{ts}^{\star}$ are the perfect descriptions of the representatons $t$, $s$ and the joint representation $ts$ respectively.
\end{proposition}
\begin{proof}
The statement $l \left( m_{ts}^{\star} \right) \geq l \left( m_{t}^{\star} \right)$ is equivalent to $K(t,s) \geq K(t)$. Then apply Proposition \ref{prop:excess_kolmogorov}.
\end{proof}

Intuitively, adding more information to a representation is a good thing if this information is relevant to describe the entity in which we are interested. But adding non-relevant information will make our model unnecessarily long.

If the selected two representations partially overlap, we could take advantage of this fact to come up with a shorter description. In the worst case, the perfect description of a joint represenation would be just the concatenation of the perfect descriptions of the involved representations.

\begin{proposition}
\label{prop:joint_sum}
Given any two topics $t,s \in \mathcal{T}$, we have that $l \left( m_{ts}^{\star} \right) \leq l \left( m_{t}^{\star} \right) + l \left( m_{s}^{\star} \right)$.
\end{proposition}
\begin{proof}
The statement $l \left( m_{ts}^{\star} \right) \leq l \left( m_{t}^{\star} \right) + l \left( m_{s}^{\star} \right)$ is equivalent to $K(t,s) \leq K(t) + K(s)$, then apply Proposition \ref{prop:additive_kolmogorov}.
\end{proof}

Another interpretation of the above proposition would be to consider a representation $r$ that is the concatenation of two, partially overlapping, representations $s$ and $t$, that is, $r = st$. In this sense, Proposition \ref{prop:joint_sum} says that including redundant information in a description is not a problem from the proint of view of finding the shortes possible description. Compare with Proposition \ref{prop:joint_length} that states that adding non-relevant information to a representation is a bad thing.

Finally, next proposition proves that the order of the representations in the perfect description of a joint representation does not change its length.

\begin{proposition}
\label{prop:joint_order}
Given any two representations $t, s \in \mathcal{B}^\ast$, we have that $l \left( m_{ts}^{\star} \right) = l \left( m_{st}^{\star} \right) + c$.
\end{proposition}
\begin{proof}
The statement $l \left( m_{ts}^{\star} \right) = l \left( m_{st}^{\star} \right) + c$ is equivalent to $K(ts) = K(st) + c$, then apply Proposition \ref{prop:kolmogorov_order}.
\end{proof}

Of course, given the concatenated $ts$ we cannot infer the original $t$ and $s$, since they are not self-delimited repsentations. But this fact is not relevant to the problem of finding the shortest possible description of the concatenated string.

Propositions \ref{prop:joint_length}, \ref{prop:joint_sum} and \ref{prop:joint_order} can be generalized to any arbitrary, but finite, collection of representations $t_1, t_2, \ldots, t_n$.

\begin{proposition}
\label{prop:joint_multiple_topics}
Let $t_1, t_2, \ldots, t_n \in \mathcal{T}$ be a finite collection of representations. Then, we have that:

\renewcommand{\theenumi}{\roman{enumi}}
\begin{enumerate}
\item $l(m_{t_1 t_2 \ldots t_n}^\star) \geq l(m_ {t_i}^\star) \; \forall \, 0 \leq i \leq n$,
\item $l(m_{t_1 t_2 \ldots t_n}^\star) \leq l(m_ {t_1}^\star) + l(m_ {t_2}^\star) + \ldots + l(m_ {t_n}^\star)$,
\item $l(m_{t_1 \ldots t_i \ldots t_j \ldots t_n}^\star) = l(m_{t_1 \ldots t_j \ldots t_i \ldots t_n}^\star) + c \; \forall \, 0 \leq i \leq j \leq n$,
\item $l(m_{t_1 \ldots t_{n-1}}^\star) \leq l(m_{t_1 \ldots t_{n-1} t_n}^\star)$.
\end{enumerate}
\end{proposition}
\begin{proof}
Apply Propositions \ref{prop:joint_length}, \ref{prop:joint_sum} and \ref{prop:joint_order} to individual pairs of topics $i$ and $j$.
\end{proof}

%
% Section: Conditional Models
%

\section{Conditional Models}

Sometimes it is cumbersome to include all the information needed to reconstruct a topic in its model, since that would require very large strings. In those cases, it is usually more convenient to assume some already existing background knowledge, and compute how much we do not know about a topic given that background, what we call conditional models.

{\color{red} TODO: clafify that conditional models are out proposed method to do reasearch.}

\begin{definition}
Let $t,s \in \mathcal{T}$ be two different topics. We say that the string $\langle TM,a \rangle$ is a \emph{valid conditional model}\index{Conditional model} of the topic $t$ given a perfect model of $s$, or a conditional model of $t$ given $s$ for short, denoted by $m_{t \mid s^\star}$, if $TM \left(\langle m_s^\star, a \rangle \right) = t$.
\end{definition}

From a practical point of view, perhaps it would have been a better idea to define the conditional model of a topic $t$ given any model of topic $s$ instead of its perfect model, that is, by using $TM \left( \langle m_s, a \rangle \right)$ instead of $TM \left( \langle m_s^\star, a \rangle \right)$. However, it is very difficult to say anything useful about how much we do not know about topic $t$ assuming an incomplete background knowledge of $s$.

\begin{example}
For each topic $t \in \mathcal{T}$ there always exists a conditional model $m_{t \mid s^\star}$ that describes $t$ independently of the topic $s \in \mathcal{T}$. For example, we can use a Turing machine that given the input $\langle m_s^\star, a \rangle$ safely ignores the $m_s^\star$ substring.
\end{example}

Note that $m_{t \mid s^\star}$ does not belong to $\mathcal{M}_t$, since $m_s^\star$ is required to compute the topic $t$, but it is not part of the conditional model itself. A new definition is required to capture this concept.

\begin{definition}
Given topics $t,s \in \mathcal{T}$, we define the \emph{set of conditional models}\index{Set of conditional models} of $t$ given the perfect model of $s$, denoted by $\mathcal{M}_{t \mid s^\star}$, as:
\[
\mathcal{M}_{t \mid s^\star} = \{ m \in \mathcal{M}_{\mathcal{T}} : TM \left(\langle m_s^\star, a \rangle \right) = t \}.
\]
\end{definition}

Again, we are interested in the concept of perfect conditional model. The perfect conditional model of a topic given the perfect model of a second topic is the shortest possible string that allow us to fully reconstruct the topic given the perfect model of the second one.

\begin{definition}
Let $t,s \in \mathcal{T}$ be two topics, and let $m_{t \mid s^\star}^{\star} \in \mathcal{M}_{t \mid s^\star}$ be the shortest possible conditional model of $t$ given a perfect model of $s$ using the standard shortlex ordering. We call $m_{t \mid s^\star}^{\star}$ the \emph{perfect conditional model}\index{Perfect conditional model} of topic $t$ given a perfect model of topic $s$, or perfect conditional model of $t$ given $s$ for short.
\end{definition}

The length of perfect conditional model is equal or shorter that their unconditional counterparts. That is, assuming some already existing background knowledge could reduce the effort required to describe a topic.

\begin{proposition}
\label{prop:description_conditional_inequality}
For any topic $t \in \mathcal{T}$ we have that $l \left( m_{t \mid s^\star}^{\star} \right) \leq l \left(m^\star_t\right)$ for all $s \in \mathcal{T}$.
\end{proposition}
\begin{proof}
The statement $l \left( m_{t \mid s^\star}^{\star} \right) \leq l \left(m^\star_t\right)$ is equivalent to $K(t \mid s^\star) \leq K(t)$, then apply Proposition \ref{prop:kolmogorov_conditional}.
\end{proof}

Next proposition shows the relation between the lengths of models, join models and conditional models.

\begin{proposition}
\label{prop:description_conditional_joint}
Let $t, s \in \mathcal{T}$ two different topics. We have that:
\[
l \left( m_{t \mid s^\star}^{\star} \right) \leq l \left( m_t^\star \right) \leq l \left( m_{ts}^\star \right)
\]
\end{proposition}
\begin{proof}
The statement $l \left( m_{t \mid s^\star}^{\star} \right) \leq l \left( m_t^\star \right) \leq l \left( m_{ts}^\star \right)$ is equivalent to $K(t | s^\star ) \leq K(t)$ and $K(t) \leq K(t, s)$, then apply Proposition \ref{prop:kolmogorov_relations}.
\end{proof}

As it was the case of joint models, the concept of conditional model can be extended to finite collections of topics.

\begin{definition}
Let $t, s_1, s_2, \ldots, s_n \in T$ be a finite collection of topics. We say that the string $\langle TM,a \rangle$ is a \emph{valid conditional model} of topic $t$ given the perfect models of topics $s_1, s_2, \ldots, s_n$, or conditional model of $t$ given $s_1, s_2, \ldots, s_n$ for short, and denoted by $m_{t \mid s_1^\star, s_2^\star, \ldots, s_n^\star}$, if
\[
TM \left(\langle m_{s_1}^\star, m_{s_2}^\star, \ldots, m_{s_n}^\star, a \rangle \right) = t
\]
\end{definition}

In the next definition we provide the generalization of the concept of perfect conditional models.

\begin{definition}
Let $\mathcal{M}_{t \mid s_1^\star, s_2^\star, \ldots, s_n^\star}$ be the set of conditional models of topic $t$ given the perfect models of topics $s_1, s_2, \ldots, s_n$, and let $m_{t \mid s_1^\star, s_2^\star, \ldots, s_n^\star}^\star \in \mathcal{M}_{t \mid s_1^\star, s_2^\star, \ldots, s_n^\star}$ be its shortest possible conditional model using the standard shortlex ordering. We call $m_{t \mid s_1^\star, s_2^\star, \ldots, s_n^\star}^\star$ the \emph{perfect conditional model} of topic $t$ given the perfect models of topics $s_1, s_2, \ldots, s_n$, or the perfect model of $t$ given $s_1, s_2, \ldots, s_n$ for short.
\end{definition}

Next proposition generalizes Propositions \ref{prop:description_conditional_inequality} and \ref{prop:description_conditional_joint} to any arbitrary, but finite, collection of topics $s_1, s_2, \ldots, s_n$. Moreover, the proposition shows that the more background knowledge we assume for a research topic, the shorter is the perfect model for that topic.

\begin{proposition}
\label{prop:joint_multiple_topics}
Let $t, s_1, s_2, \ldots, s_n, s_{n+1} \in T$ be a finite collection of topics. Then, we have that:

\renewcommand{\theenumi}{\roman{enumi}}
\begin{enumerate}
\item $l \left( m_{t \mid s_1^\star, s_2^\star, \ldots, s_n^\star}^\star \right) \leq l \left( m^\star_t \right)$,
\item $l \left( m_{t \mid s_1^\star, s_2^\star, \ldots, s_n^\star}^\star \right) \leq m_t^\star \leq l(m_{t_1 s_1 \ldots s_n}^\star)$,
\item $l \left( m_{t \mid s_1^\star, s_2^\star, \ldots, s_n^\star}^\star \right) \geq l \left( m_{t \mid s_1^\star, s_2^\star, \ldots, s_n^\star, s_{n+1}^\star}^\star \right)$.
\end{enumerate}
\end{proposition}
\begin{proof}
Apply Propositions \ref{prop:description_conditional_inequality} and \ref{prop:description_conditional_joint} to individual pairs of topics $i$ and $j$.
\end{proof}


%
% Section: Areas
%

\section{Research Areas}
\label{sec:areas}

{\color{red} TODO: not sure about the concept of research area.}

Topics can be grouped into research areas. The concept of area is useful as long as all the topics included in the area share a common property. The particular details of grouping properties depend on the practical applications in which the theory of nescience is being used.

\begin{definition}
Given a set of topics $\mathcal{T}$, we define a \emph{research area}\index{Research area} $A$ as a subset of topics $A \subset \mathcal{T}$.
\end{definition}

If we want to know how much we do not know about a research area, first we have to provide a model for that area. Unfortunately, in general, areas are infinite, but our knowledge is finite, and so, we can only partially describe them.

\begin{definition}
Let $A \subset \mathcal{T}$ be an area. We define the \emph{known subset of the area}\index{Known subset of an area} $A$, denoted by $\hat{A}$, as the set composed by those topics $t_1, t_2, \ldots, t_n \in A$ for which at least one non-trivial model is known.
\end{definition}

As our understanding of a research area changes, the number of topics included in its known subset changes as well. The properties of areas studied in this book will be always relative to our current knowledge.

\begin{definition}
Let $A \subset \mathcal{T}$ be an area with known subset $\hat{A}$. We call a \emph{model of the area $A$} given the known subset $\hat{A}$, abbreviated as \emph{model of $A$}, and denoted by $m_{\hat{A}}$, to any string in the form $\langle TM, a\rangle$ such that $TM(a) = \langle t_1, t_2, \ldots, t_n\rangle$, where $\{t_1, t_2, \ldots, t_n\} = \hat{A}$.
\end{definition}

Since our definition of model requires that the Turing machine halts at some point, we cannot provide models for infinite areas. In case of an infinite area we will always refer to its finite known subset.

\begin{definition}
Given the set of topics $\mathcal{T}$ and an area $A \subset \mathcal{T}$, we define the set of \emph{valid models of the area $A$} given the known subset $\hat{A}$\index{Set of models of an area}, denoted by $\mathcal{M}_{\hat{A}}$, as:
\[
\mathcal{M}_{\hat{A}} = \{ m \in \mathcal{M} : TM \left(\langle m, a \rangle \right) = \hat{A} \}.
\]
\end{definition}

Finally, we are interested in the perfect model for a research area, that is, the shortest possible model that fully describes its known subset. According to Definition \ref{def:trivial_model}, if we are aware of the existence of a topic $t \in A$, that topic should be part of $\hat{A}$, even in the case we have not started yet to do research about that particular topic.

\begin{definition}
Let $A \subset \mathcal{T}$ be an area with known subset $\hat{A}$, and let $m_{\hat{A}}^{\star} \in \mathcal{M}_{\hat{A}}$ be the shortest possible model of $A$ using the standard shortlex ordering. We call  $m_{\hat{A}}^{\star}$ the \emph{perfect model of the area $A$} given the known subset $\hat{A}$\index{Perfect model of an area}, abbreviated as \emph{perfect model of $A$}.
\end{definition}

Next proposition shows the relation between the model of an area, and the models of the topics that compose the known subset of that area. In general, the models for an area are different from the collection of models of the individual topics.

\begin{proposition}
Let $A \subset \mathcal{T}$ be an area with known subset $\hat{A} = \{t_1, t_2, \ldots, t_n\}$, then we have that $l \left( m_{\hat{A}}^{\star} \right) \leq l(m_ {t_1}^\star) + l(m_ {t_2}^\star) + \ldots + l(m_ {t_n}^\star)$.
\end{proposition}
\begin{proof}
Apply Proposition \ref{prop:joint_multiple_topics}-ii. 
\end{proof}

Also, as it was proved in Proposition \ref{prop:joint_multiple_topics}, the order in which the topics are listed in the description of an area is not relevant when dealing with the perfect model for that area.

Areas can overlap, that is, given two areas $A$ and $B$ it might happen that $A \cap B \neq \varnothing$. Moreover, areas can be subsets of other areas, creating an hierarchy of areas. We are interested in the length of perfect models of areas in relation to the length of perfect models of other areas.

\begin{proposition}
Let $A, B \subset \mathcal{T}$ be two areas such that $A \subset B$, and let $\hat{A}$ and $\hat{B}$ be their know subsets respectively, then we have that $l \left( m_{\hat{A}}^{\star} \right) \leq l \left( m_{\hat{B}}^{\star} \right)$.
\end{proposition}
\begin{proof}
If $t \in \hat{A}$ there exists a non-trivial model $m_t$ for $t$, and given that $A \subset B$, $m_t$ is also a non-trival model for $B$, and so $t \in \hat{B}$. Finally, we apply \ref{prop:joint_multiple_topics}-iv.
\end{proof}

Next proposition \label{prop:areas_union} shows how the length of the shortest possible model of areas relate to the union and intersection of such areas.

\begin{proposition}
\label{prop:areas_union}
Let $A, B \subset \mathcal{T}$ be two areas with know subsets $\hat{A}$ and $\hat{B}$ respectively, then we have that $l \left( m_{\hat{A} \cup \hat{B}}^{\star} \right) = l \left( m_{\hat{A}}^{\star} \right) + l \left( m_{\hat{B}}^{\star} \right) - l \left( m_{\hat{A} \cap \hat{B}}^{\star} \right)$.
\end{proposition}
\begin{proof}
{\color{red} TODO}
\end{proof}

A consequence of Proposition \label{prop:areas_union} is that $l \left( m_{\hat{A} \cup \hat{B}}^{\star} \right) \leq l \left( m_{\hat{A}}^{\star} \right) + l \left( m_{\hat{B}}^{\star} \right)$, that is, when we combines two different research areas, how much we do not know about these areas decreases.

In the same way we introduced a chain rule for entropy in Proposition \ref{prop:chain_rule_entropy}, we can provide a chain rule for the shortest length for a model of a research area.

\begin{proposition}
Let $A, B \subset \mathcal{T}$ be two areas with know subsets $\hat{A}$ and $\hat{B}$, then we have that $l \left( m_{\hat{A} \cup \hat{B}}^{\star} \right) = l \left( m_{\hat{A}}^{\star} \right) + l \left( m_{\hat{B} \backslash \hat{A}}^{\star} \right)$.
\end{proposition}
\begin{proof}
{\color{red} TODO}
\end{proof}

%
% Section: References
%

\section*{References}

For more information about Russell's paradox, Cantor theorem and universal sets refer, for example, to \cite{jech2013set}. The idea of using a function to assigns to each symbol and well-formed formula of some formal language a unique natural number (GÃ¶del number) was introduced by Kurt GÃ¶del for the proof of his incompleteness theorems \cite{godel1931formal}. A detailed description of the Berry paradox from the point of view of computability can be found at \cite{chaitin1995berry}. For a detailed account of the implications of Kolmogorov complexity being true up to a constant, please refer to \cite{li2013introduction}. That oracle machines are not mechanical was stated by Turing when he introduced the concept of oracle machine in \cite{turing1939systems}.



