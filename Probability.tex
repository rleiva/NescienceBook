%
% CHAPTER: Probability Theory
%

\chapterimage{Koenigsberg_Map_by_Bering_1613.pdf} % Chapter heading image

\chapter{Discrete Probability}
\label{chap:Probability Theory}

\begin{quote}
\begin{flushright}
\emph{Mathematics may be defined as the subject in which\\
we never know what we are talking about,\\
nor whether what we are saying is true.}\\
Bertrand Russell
\end{flushright}
\end{quote}
\bigskip

{\color{red} Find a header image. Find a quote. Introduce the chapter. Mention that we will cover only discrete probability, and explain why.}

%
% Section: Foundations
%

\section{Foundations}
\label{sec:probability_foundations}

\emph{Probability} is a very difficult concept to grasp. Imagine we throw a dice and we want to calculate the probability that the number that appears is even. The dice has six possible outcomes, and since there are three even numbers, we say that the probability of having an even number is $3/6$ or $1/2$. This what the \emph{classical interpretation}\index{Classical interpretation of probability} of probability proposes: if we have an experiment in which all the possible {(\color{red} TODO: finite)} outcomes are equally likely to happen, the probability of an event is the number of favourables cases divided by the total number of cases. The problem with this interpretations is that "equally likely" is essentially the same thing than "having the same probability", and so, it is a circular definition. An alternative approach to assign probabilities would be to apply the \emph{principle of indiference}\index{Principle of indiference}: in absense of any relevant evidence, all possible outcomes should have the same probability. The difficulty with this principle araises when there is evidence that not all cases are equal. For example, what happens if we know that the dice is loaded? How do we assign probabilities when not all the sides of the dice are equally likely?

The \emph{frequentist interpretation}\index{Frequentist interpretation of probability} of probability proposes throwing the dice multiple times and compare the frequency of even numbers with the total number of throws. The general idea is to repeat the experiments a sufficiently large number of times under similar conditions and assing the releative frequency of each outcome as its probability. This interpretation has two main limitations. First of all, it is not clear how we should repeat an experiment under "similar conditions", since if we use exactly the same conditions the results of all the trials would be the same. The second is that it is not defined what a "large number of times" means (technically speaking, we should repeat the experiment an infinite number of times). From a practical point of view it is also very to apply the frequentist interpretation: some experiments cannot be repeated a large number of times, for example, what it is the probability that a candidate wins an election?; probability is defined in terms of a succession of experiments, so we cannot compute the probability of an individual outcome; and we require that the relative frequency limit exists, that is not always the case, for example, in finacial time series.

A third intepretation of the concept of probability, called \emph{subjective interpretation}\index{Subjective interpretation of probability}, proposes to assign to each event a probability based on our degree of belief: the more we belive that an event is true, the higher its probability. Of course, not all possible combinations of probabilities are valid, it is required that some rules of coherence must be satisfied. For example, if we are betting on the result of the dice thrown, an assigment of probabilities that guarantees that we will loose all of our money (what it is called a \emph{dutch book}\index{Dutch book}) does not satify the conditions of the subjective interpretation. It turns out that the conditions neccesary and sufficient to guarantee a fair bet are the same conditions required by the axioms of probability introduced below. In this sense, we can assign to events whatever probabilities we want, as long as they are consistant with the axioms of probability. The problem with the subjective interpretation is that people have different degrees of belief. A solution to this problem is proposed by the \emph{Bayesian interpretation}\index{Bayesian interpretation of probability} of probability: we start with a tentative assigment of probabilities, and as we get futher evidence, we modify our degree of belief, or probability, accordingly; as we get more evidence, estimated probabilities will converge to the true probabilities. In any case, assigning probabilities to an infinite number of events is not, in general, human attainable. 

Currently, the concept of probabilty is defined axiomatically (\emph{axiomatic interpretation}\index{Axiomatic interpretation of probability}), which means we give up in trying to define what a probability is, and instead we assume as true some of its properties. Intuitively, a probability should be a number between $0$ and $1$, where an event that cannot happen has a probability of zero, and an event that for sure will happen has a probability of one. That fact that probability must be a number between $0$ and $1$ is more a social convention that a real mathematical requirement, since other ranges of numbers are equally good. We require some additional properties of probabilities. For example, if two events $A$ and $B$, with probabilities $P \left( A \right)$ and $P \left( B \right)$ respectively, cannot happen at the same time, the probabilty that either $A$ or $B$ occurs should be $P \left( A \right) + P \left( B \right)$. If $A$ and $B$ can happen at the same time, but they are not related in any way, that is, they are independent events (whatever that means), we expect that the probability of the two events happening at the same time shold be $P \left( A \right) P \left( B \right)$. Finally, we are also interested in the pobability of $A$ happeing assuming that $B$ has already happend, it should be the the fraction of probability $A$ that intersects with $B$. Probability, then, would be anything that satisfies these properties. The problem with the axiomatic interpretation is that too many things satisfy those requirements, like for example phisical quantities like normalized mass or normalized volume.

Probability theory is about assigning a number to some events from a sample space. The word "event" is a little bit misleading in this context because it suggests that something happened, which is not always the case {\color{red} TODO: provide an example}. However, to avoid confusion, we will keep calling events to what are essentially subsets.

\begin{definition}
Let $\left( \Omega, \mathcal{A} \right)$ be a field over a non-empty discrete set. $\Omega$ is called the \emph{sample space}\index{Sample space}, its elements \emph{outcomes}\index{Outcome}, and the elements of $\mathcal{A}$ \emph{events}. In particular, $\Omega$ is called the \emph{certain event}\index{Certain event} and the empty set $\varnothing$ the \emph{impossible event}\index{Impossible event}.
\end{definition}

As we saw in Section \ref{sec:sets}, given that $\left( \Omega, \mathcal{A} \right)$ is a field, we have that $\Omega \in \mathcal{A}$ and that $\varnothing \in \mathcal{A}$. Moreover, the union of a finite collection of events is an event $A_1 \cup A_2 \cup \ldots \cup A_n \in \mathcal{A}$, and that the intersection of a fine collection of events is also an event $A_1 \cap A_2 \cap \ldots \cap A_n \in \mathcal{A}$.

As we have said in the introduction of this chapter, our main interest is in discrete mathematics, and so, we will mostly working with probabilities over discrete (finite or countably infinite) sets. An extension of the concept of probability to continuous sets requires the use of $\sigma$-algebras\index{$\sigma$-algebra} of sets instead of fields and the use measure theory {\color{red} TODO: explain in a footnote why this is the case}.

The standard axiomaitization used in probability theory is called \emph{Kolmogorov axioms}.

\begin{definition} (\emph{Kolmogorov's Axioms})\index{Kolmogorov's axioms}\label{Kolmogorov_axioms}
A \emph{probability}\index{Probability} is a real number $P(A) \in \mathbb{R}$ assigned to each event $A \in \mathcal{A}$ of the field $\left( \Omega, \mathcal{A} \right)$ that satisfy the following axioms:

\medskip

\begin{description}
\item [Axiom 1] $P(A) \geq 0$.
\item [Axiom 2] $P(\Omega) = 1$.
\item [Axiom 3] For every finite sequence of disjoint events $A_1, A_2, \ldots, A_n$ we have that $P(\cup_{i=1}^n) = \sum_{i=1}^n P(A_i)$.
\end{description}

The triple $\left( \Omega, \mathcal{A}, P \right)$ is called a \emph{probability space}. 
\end{definition}

A problem with these axioms is that they cannot be reduced to first-order logic (See Appendix \ref{apx:foundations_mathematics}), since real numbers cannot be described in terms of this logic. Also, no information is contained in the axioms about how probabilites can be assigned to events. {\color{red} TODO: split the two ideas in two separate paragraphs and extend a little bit. Perhaps mention model theory.}

\begin{example}
\label{ex:discrete_sample_space}
Let $\Omega$ a sample space containing $n$ elements equally probable. If $A \subset \Omega$ is an event with $d(A) = m$, then we have that $P(A) = m/n$.
\end{example}

Let's prove some basic results about probabilities, starting by calculating the probability of the complement of an event, that is, the probability that the event does not happen.

\begin{proposition}
For every event $A$, $P \left( A^{c} \right) = 1 - P \left( A \right)$
\end{proposition}
\begin{proof}
The sets $A$ and $A^c$ are disjoint and $A \cup A^c = \Omega$. Given Axiom 3 we have that $P \left( A \cup A^c \right) = P \left( A \right) + P \left( A^c \right)$, and given Axiom 2 we have that $P \left( A \cup A^c \right) = P(\Omega) = 1$, and so, $P \left( A \right) + P \left( A^c \right) = 1$.
\end{proof}

As a direct consequence of previous proposition, we can derive the probability of the impossible event.

\begin{proposition}
The probability of the impossible event is zero, that is, $P \left( \varnothing \right) = 0$
\end{proposition}
\begin{proof}
Given that $P \left( \varnothing \right) = 1 - P \left( \Omega \right) = 0$
\end{proof}

As it was expected, sub-events (subsets) have smaller probabilities than events.

\begin{proposition}
If $A\subset B$ then $P \left( A \right) \leq P \left( B \right)$
\end{proposition}
\begin{proof}
The event $B$ can be decomposed as the union of two disjoint events $A$ and $A^c \cap B$, so we have that $P \left( B \right) = P \left( A \right) + P \left( A^c \cap B \right)$, that combined with the fact that  $P \left( A^c \cap B \right) \geq 0$ proves the proposition.
\end{proof}

We have in place all the elements we need to prove that probabilities are numbers between zero and one.

\begin{proposition}
For every event $A$ we have that $0 \leq P \left( A \right) \leq 1$.
\end{proposition}
\begin{proof}
Based on Axiom 1 and given that $A \subset \Omega$ and so $P \left( A \right) \leq P \left( \Omega \right) = 1$
\end{proof}

Axiom 3 allows us to compute the probability of the union of disjoint events, but it says nothing about the case of non-disjoint events. Next proposition shows how to compute the probability of the union of events that are not disjoint.

\begin{proposition}
For every two events $A$ and $B$, $Pr\left(A\cup B\right)=Pr\left(A\right)+Pr\left(B\right)-Pr\left(A\cap B\right)$.
\end{proposition}
\begin{proof}
The union of sets $A$ and $B$ can be decomposed as the union of the two disjoint sets $A \cup B = B \cup \left( A \cap B^c \right)$, so given Axiom 3 we have that
\[
P \left( A \cup B \right) = P \left( B \right) + P \left( A \cap B^c \right)
\]
In the same way, the set $A$ can be decomposed as the unition of the disjoint sets $A = \left( A \cap B \right) \cup \left( A \cap B^c \right)$, so that
\[
P \left( A \cup B^c \right) = P \left( A \right) - P \left( A \cap B \right)
\]
Combining boths expression we get the desired result.
\end{proof}

The following formula generalizes to the case of $n$ events $A_1, \ldots, A_n$ {\color{red} TODO: there is no need to use a named formula, since it is not reference bellow}:
\begin{multline}
P \left( \bigcup_{i=1}^n A_i \right) = \sum_{i=1}^n P \left( A_i \right) - \sum_{i<j} P \left( A_i \cap A_j \right) + \sum_{i<j<k} P \left( A_i \cap A_j \cap A_k \right) - \\
 - \sum_{i<j<k<l} P \left( A_i \cap A_j \cap A_k \cap A_l \right) + \ldots +  (-1)^{n+1} P \left( A_1 \cap A_2 \cap \ldots \cap A_n \right) 
\end{multline}

A probability mass function is a function that assigns to each possible event of a sample space its probability.

\begin{definition}
\label{def:probability_function}
Let $\left( \Omega, \mathcal{A} , P \right)$ be a discrete probability space. A \emph{probability mass function}\index{Probability mass function} is a real value funcion $f : \mathcal{A} \rightarrow [0, 1]$ such that for each $A \in \mathcal{A}$ we have that $f \left( A \right) = P \left( A \right)$.
\end{definition}

In the particular case of discrete probability spaces, we can also define the probability mass functions by assigning to each outcome of the sample space its probability, that is $f \left( \omega \right) = P \left( \omega \right)$ for all $\omega \in \Omega$.

Probability mass functions are the primary means we have to specify the distribution of probabilities in a probability space.

\begin{example}
In Example \ref{ex:discrete_sample_space} we introduced a probability space $\left( \Omega, \mathcal{A}, P \right)$ containing $n$ elements equally probable. The probability mass function associated with this experiment will be the function $f : \mathcal{A} \rightarrow [0, 1]$ defined as $f \left( A \right) = d\left( A \right)/n$, for all $A \in \mathcal{A}$.
\end{example}


%
% Section: Conditional Probability
%

\section{Conditional Probability}
\label{sec:probability_conditional}

The concept of conditional probability plays a fundamental role in the area of statistical learning. Traditionally the conditional probability of event $A$ given event $B$ has been seen as the updated probability of $A$ after we have learnt that $B$ has occurred. However, this interpretation suggests that there is a temporal, even causal, relationshipt between events $B$ and $A$, which is not necessarly true {\color{red} TODO: provide an example}.

Under the axomatization of Kolmogorov, conditional probability is introduced as a definition. Some authors claim that conditional probabilty, being a central element in probability theory, should be a property that must be derived from the axioms. Of course, that would require to extend Definition \ref{Kolmogorov_axioms} with additional properties. Unfortunately, there is no consensus among mathematicians and philosophers on how this extention should be carried out.

\begin{definition}
Let $A$ and $B$ two events such that $P \left( B \right) \neq 0$. The \emph{conditional probability} of $A$ given $B$, denoted by $P \left( A \mid B \right)$, is defined as
\[
P\left(A\mid B\right) = \frac{P\left(A\cap B\right)}{P\left(B\right)}
\]
\end{definition}

Conditional probability is itself a probability, since it satisfy the axioms. Conditional probability $P\left(A\mid B\right)$ is not defined if $P\left(B\right)=0$.

The probability that two events will happen toghether (although not necessarily at the same time as we have seen {\color{red} TODO: be sure we have seen it}), given their conditional probabilities, is $P \left( A \cap B \right) = P \left( A \mid B \right) P \left( B \right)$ or $P \left( A \cap B \right) = P \left( A \mid B \right) P \left( B \right)$. The formula $P \left( A \cap B \right) = P \left( A \mid B \right) P \left( B \right)$ provides, perhaps, a more intuitive interpretation of the concept of conditional probability. There are even some authors that propose that conditional probability should be defined using this interpretation instead of a quotient.

The generalization of this formula for the case of $n$ events, called \emph{multiplication rule}, is
\[
P \left( A_{1} \cap A_{2} \cap \ldots \cap A_{n} \right) = P \left( A_{1} \right) P \left( A_{2} \mid A_{1}\right) \ldots  P \left( A_{n} \mid A_{1}\cap A_{2} \cap \ldots \cap A_{n-1} \right)
\]

The concept of events independence plays also a very important role in probability theory and statistical learning. 

\begin{definition}\label{independent_events}
Two events $A$ and $B$ are said to be \emph{independent} if $P \left( A \cap B \right) = P \left( A \right) P \left(B \right)$
\end{definition}

From an intuitive point of view, the events $A$ and $B$ are independent if observing that B has occurred does not alter the probability of A. This property can be derived from the definition of independence.

\begin{proposition}
Let $A$ and $B$ two events shuch that $P \left( A \right) > 0$ and $P \left( B \right)>0$, then $A$ and $B$ are independent if and only if $P \left( A \mid B\right) = P \left( A \right)$ and $P \left( B \mid  A \right) = P \left( B \right)$.
\end{proposition}
\begin{proof}
Assume that $A$ and $B$ are independent, that is $P \left( A \cap B \right) = P \left( A \right) P \left(B \right)$, then
\[
P \left( A \mid B \right) = \frac{P\left(A\cap B\right)}{P\left(B\right)} = \frac{P \left( A \right) P \left(B \right)}{P\left(B\right)} = P \left( A \right)
\]
Now let's assume that $P \left( A \mid B \right) = P \left( A \right)$. Given the multiplication rule we have that,
\[
P \left( A \cap B \right) =  P \left( A \mid B \right) P \left( B \right) = P \left( A \right) P \left( B \right)
\] 
The same applies if we interchange $A$ and $B$. 
\end{proof}

As it was the case of conditional probability, some authors claim that independence, being a fundamental concept in probability theory, should be derived from the axioms, not introduced as a definition.

The concept of independence can be generalized to the case of multiple events: the events $A_{1}, \ldots, A_{n}$ are independents (or mutually independent) if for every subset $A_{i_1}, \ldots, A_{i_j}$ composed by $j$ events $\left( j = 2, 3, \ldots, n \right)$, we have that $P \left( A_{i_1} \cap \ldots \cap A_{i_j} \right) = P \left( A_{i_1} \right) \ldots P \left( A_{i_j}\right)$.

\begin{example}
There is some confussion about the difference between mutually exclusive, or disjoint, events and independent events. If $A$ and $B$ are two mutually exclusive events, it does not make too much sense to compute the probability that $A$ will happen given $B$, since if $B$ happens, $A$ cannot happen; in the same way that it does not make too much sense to talk about the conditional probability that $A$ will happen given $B$ if the probability of $B$ is zero. However, since Definition \ref{independent_events} does not explicitly exclude the case of $A$ and $B$ being mutually exclusive, we have to conclude that two mutually exclusive events are independent if, and only if, one of them (or both) have a probability of zero.
\end{example}

A particular interesting case is when events $A$ and $B$ are not independent but they become independent if we know that some other even $C$ has happened. {\color{red} Explain why, and provide a reference to the section in which conterfactuals are introduced.}

\begin{definition}
Let's $A$, $B$ and $C$ events such that $P\left( B \cap C \right)>0$. $A$ and $B$ are \emph{conditionally independent} given $C$ if $P\left(A \mid B \cap C \right) = P\left( A \mid C \right)$.
\end{definition}

Next theorem introduces the Bayes' rule, which constitutes the foundations of a very important technique in statistical learning called Bayesian inference (see Section {\color{red} XX}). 

\begin{theorem} (Bayes' Theorem) Let's $A$ and $B$ two events such that $P\left( B \right) \neq 0$. Then we have that
\[
P \left( A \mid B \right) = \frac{P \left( B \mid A \right) P \left( A \right)}{P \left( B \right)}
\]
The probability $P\left( A \right)$ is called \emph{prior probability}, and $P\left( A \mid B \right)$ is called \emph{posterior probability}.
\end{theorem}
\begin{proof}
From the definition of conditional probability we have that $P \left( A \mid B \right) = P \left( A \cap B \right) / P \left( B \right)$ (given that $P \left( B \right) \neq 0$) and that $P \left( B \mid A \right) = P \left( A \cap B \right) / P \left( A \right)$ (given that $P \left( A \right) \neq 0$). Solving for $P(A\cap B)$ and substituing into the above expressions for $P(A\mid B)$ gives us the theorem.
\end{proof}

As we have seen in the proof, Bayes' theorem is a direct consequence of the definition of conditional probability (and we are not happy that conditional probability is a definiton). Its intepretation according to Bayesian inference is that it allows us to compute how our degree of beleif about an event $A$ (the prior probability $P\left( A \right)$) changes when we gather additional evidence in the form of the occurence of event $B$ (and becoming the posterior probability $P\left( A \mid B \right)$).

\begin{example}
Let $E$ a disease that affects to one of every one million persons, $P(E) = 1 \times 10^{-6}$, and let $+$ a test designed to detect the disease that fails once every one thousand applications, $P(+ \mid E) = 999/1000$. We are interested in knowing the probability of having the disease if the test is positive $P(E \mid +)$. Applying Bayes' theorem we have that:
\[
P(E \mid +) = \frac{P(+ \mid E) P(E)}{P(+)} = \frac{P(+ \mid E) P(E)}{P(+ \mid E) P(E) + P(+ \mid E^c) P(E^c)} = 0.001
\]
That is, although the test only fails once per thousand applications, it is still very unlikely we have the disease in the case of a positive result. This counterintuitive result is explained because the probability of failure of the test $10^{-3}$ is much higher than the probability of having the disease $10^{-6}$. In practice we solve this problem by applying a second test to those who got a positive result, since the probability of having the disease after two positives is $0.5$ (assuming that the successive repetitions of the test are independent).
\end{example}

{\color{red} Somewhere we should provide a second example in which the Bayes' theorem is meaningless.}

Bayes theorem can be generalized to the case of multiple events: let's the events $A_{1}, \ldots, A_{k}$ ($P\left( A_{j} \right)>0$ for $j=1, \ldots, k$) form a partition of the sample space $\Omega$, and let $B$ be an event such that $P\left(B\right)>0$, then for $i=1, \ldots, k$ we have that
\[
P\left(A_{i}\mid B\right)=\frac{P\left(B\mid A_{i}\right) P\left(A_{i}\right)}{\sum_{j=1}^{k} P\left(B \mid A_{j}\right) P\left(A_{j}\right)}
\]

%
% Section: Random Variables
%

\section{Random Variables}
\label{sec:probability_random_variables}

A random variable is a function that asigns a real number to the outcomes of an experiment. In this sense, a random variable acts as a numerical description of the experiment. Random variables are useful because they allow us to quantify the results of the experiments and study their properties from a mathematical point of view. In fact, they are so useful that most of statisticians only think in terms of random variables.

\begin{definition}
Let $\left( \Omega, \mathcal{A} , P \right)$ be a probability space. A \emph{random variable} is a real-valued function $X : \Omega \rightarrow \mathbb{R}$.
\end{definition}

The name random variable could be a little bit misleading. First, because random variables are not variables in the ususal sense in algebra, they are functions. And second, because they are not random, what it is random is the experiment they describe. We will stay true to standard terminology.

Random variables are more useful when they describe properties of the experiments: if our sample space is composed by the students of a school, a random variable could assign to each student its height. Random variables allow us also to reasign the elements of the sample space into new events: if we throw two dices, a random variable could be the sum of the values of the dices. Be aware that we are free to assing a random variable to any sample space, even if that assignment does not make too much sense. For example, we could assign a number to each possible colour of a deck of cards, then pick up two cards at random, and sum their numbers; although the results do not have a meaningful interpretion, we could compute all sort of probabilities over them.

\begin{definition}
Let $X : \Omega \rightarrow \mathbb{R}$ be a random variable, and let $C \subset \mathbb{R}$ be a subset such that $\{ \omega \in \Omega \,:\, X \left( \omega \right) \in C\}$ is an event. The probability that $X$ belongs to $C$, denoted $P\left(X \in C \right)$, is given by $P\left( X \in C \right)=P \left( \left\{ \omega \in \Omega \,:\, X \left( \omega \right) \in C\right\} \right)$.
\end{definition}

In case of discrite probability spaces we could also compute the probability that $X$ is equal to a single value $x \in \mathbb{R}$ as $P(X = x) = P(\{ \omega \in \Omega : X(\omega) = x\})$.

The probability distribution of a random variable describes how the probabilities are assigned to the values of the random variable. 

\begin{definition}
Let $X : \Omega \rightarrow \mathbb{R}$ be a random variable. The \emph{probability distribution} of the random variable $X$ is the collection of probabilities $P\left( X \in C \right)$ for all subsets $C \subset \mathbb{R}$ such that $\{ \omega \in \Omega \,:\, X \left( \omega \right) \in C\}$ is an event.
\end{definition}

The probability distribution of a random variable $X$ defines a probability space over the real numbers (over the range of $X$). 

\begin{example}
{\color{red} TODO }
\end{example}

If $X$ is a random variable whose range is discrete, it is said that $X$ is a discrete random variable or that $X$ has a discrete distribution. In this book we will consider only discrete random variables, and we will use the name probability distribution to mean either the set of probabilities of a discrete probability space, or the distribution of a discrete random variable.

Definition \ref{def:probability_function} introduced the concept of probability mass function for discrete probability spaces. We could extend the same concept to random variables.

\begin{definition}
Let $X$ be a random variable over a discrete probability space, and let $\{ x_1, x_2, \ldots, x_i, \ldots \}$ be the range of $X$. The \emph{probability mass function}\index{Probability mass function} of the random variable $X$ is defined as the function $f : range \left( X \right) \rightarrow [0, 1]$ such that $f \left( x_i \right) = P \left( X = x_i \right)$.
\end{definition}

The set of points for which the probability mass function is greater than zero, that is $\left\{ x \, : \, f \left( x \right) > 0 \right\}$, is called the \emph{support} of the distribution of $X$.

It is possible for two random variables to have identical probability mass functions but to differ in significant ways; for instance, they may be independent, as next example shows.

\begin{example}
{\color{red} Random variables can have the same distribution without being the same random variable. (this is also mentioned in the masures of centrality section}.
\end{example}


{\color{red} Introduce the following proposition}

\begin{proposition}
Let $X$ be a discrete random variable with p.f. $f$. If $x$ is not one of the possible values of $X$, then $f\left(x\right)=0$. Also, if the sequence $x_{1},x_{2},\ldots$ includes all the possible values of $X$, then $\sum_{i=1}^{\infty}f\left(x_{1}\right)=1$.
\end{proposition}
\begin{proof}
\end{proof}

{\color{red} Any function that satisfied the two displayed formulas is the joint p.d.f. for some probability distribution.}

{\color{red} The p.f. of a discrete random variable characterizes its distribution.}

\begin{proposition}
If $X$ has a discrete distribution, the probability of each subset $C$ of the real line can be determined from the relation $P\left(X\in C\right)=\sum_{x_{i}\in C}f\left(x_{i}\right)$
\end{proposition}
\begin{proof}
\end{proof}

{\color{red} Introduce the concept of Cumulative Distribution Function}

\begin{definition}
The distribution function or cumulative distribution function, abbreviated c.d.f.) $F$ of a random variable $X$ is the function $F(x)=Pr(X\leq x)\qquad-\infty<x<\infty$
\end{definition}

{\color{red}
The function $F\left(x\right)$ is nondecreasing as $x$ increases, that is, if $x_{1}<x_{2}$ then $F\left(x_{1}\right)\leq F\left(x_{2}\right)$. Also, $\lim_{x\rightarrow-\infty}F\left(x\right)=0$ and $\lim_{x\rightarrow\infty}F\left(x\right)=1$.
}

{\color{red}
Suppose that X has a discrete distribution with the p.f. $f\left(x\right)$. $F\left(x\right)$ must have the following form: $F\left(x\right)$ will have a jump of magnitude $f\left(x_{i}\right)$ at each possible value $x_{i}$ of $X$, and $F\left(x\right)$ will be constant between every pair of successive jumps.
}

\begin{proposition}
For every value $x$ $Pr\left(X>x\right)=1-F\left(x\right)$
\end{proposition}
\begin{proof}
\end{proof}

\begin{proposition}
For all values $x_{1}$ and $x_{2}$ such that $x_{\text{1}}<x_{2}$ $Pr\left(x_{1}<X\leq x_{2}\right)=F\left(x_{2}\right)-F\left(x_{1}\right)$
\end{proposition}
 \begin{proof}
\end{proof}

{\color{red} Introduce the concept of random sample}

The concept of random sample plays a very important role in the area of statistical learning. Assuming that a collection of random variables form a random sample simplifies the mathematics behind inference methods. However, the requirements behind random samples are not always satisfied in practice.

\begin{definition}
Let $f$ be a probability distribution, and let $X_1, X_2, \ldots, X_n$ be $n$ random variables. It is said that the variables $X_1, X_2, \ldots, X_n$ form a \emph{random sample} for the distribution $f$ if they are independent, and the marginal distribution of each of them is $f$. 
\end{definition}

If the random variables $X_1, X_2, \ldots, X_n$ form a random sample for the distribution $f$, it is said that they are \emph{independent and identically distributed}, abbreviated \emph{i.i.d}. The number $n$ is called the \emph{sample size}. The joint distribution $g$ of the random sample is given by:
\[
g \left( x_1, x_2, \ldots, x_n \right) = f \left( x_1 \right) f \left( x_2 \right) \ldots f \left( x_n \right)
\]
for all the points $\left( x_1, x_2, \ldots, x_n \right) \in \mathcal{R}$.

% Multivariate Distributions

\subsection{Multivariate Distributions}

{\color{red} Multivariate distributions show comparisons between two or more measurements and the relationships among them [...]  For discrete random variables, multivariate distribution and described by joint probabilities.}

We will start by introducing bivariate distributions and studying their properties, and then we will generalize to the case of multivariate distributions. A bivariate distribution is the simplest form of multivariate distribution, it is comprised by a pair of random variables.

\begin{definition}
Let $X_1 : \Omega_1 \rightarrow \mathbb{R}$ and $X_2 : \Omega_2 \rightarrow \mathbb{R}$ be two random variables. The \emph{joint probability distribution} or \emph{bivariate probability distribution} of $X_1$ and $X_2$ is defined as the collection of all probabilities of the form $Pr\left( \left( X_1, X_2 \right) \in C \right)$ for all sets $C \subset \mathbb{R} \times \mathbb{R}$ of pairs of real numbers such that $\left( \left( \omega_1, \omega_2 \right) \in \Omega_1 \times \Omega_2: \left( X_1 \left( \omega_1 \right), X_2 \left( \omega_2 \right) \right) \in C \right)$ is an event.
\end{definition}

The joint probability distribution of two random variables $X_1$ and $X_2$ defines a probability space in $\mathbb{R}^2$. If the random variables $X_1$ and $X_2$ each have a discrete distribution, then the joint distribution is also a discrete distribution.

\begin{definition}
Let $X_1$ and $X_2$ be two random variables over discrete probability spaces. The \emph{joint probability mass function} of the random variables $X_1$ and $X_2$ is defined as the function $f : range \left( X_1 \right) \times range \left( X_2 \right) \rightarrow [0, 1]$ such that $f \left( x_1, x_2 \right) = P \left( X_1 = x_1, X_2 = x_2 \right)$.
\end{definition}

\begin{proposition}
Let $X$ and $Y$ have a discrete joint distribution. If $\left(x,y\right)$ is not one of the possible values of the pair $\left(X,Y\right)$, then $f\left(x,y\right)=0$. Also, $\sum_{\left(x,y\right)}f\left(x,y\right)=1$
\end{proposition}
\begin{proof}
\end{proof}

Finally, for each $C$ of ordered pairs $Pr\left[\left(X,Y\right)\in C\right]=\sum_{\left(x,y\right)\in C}f\left(x,y\right)$

{\color{red} Any function that satisfied the two displayed formulas is the joint p.d.f. for some probability distribution.}

A particular interesting case of bivariate distribution is given by the sum of two random variables. This is a highly confusing scenario, since we are not adding two probability distributions, as the notation $X + Y$ might suggest. Instead, we are defining a new random variable over the cartesian product of the original sample spaces.

\begin{definition}
Let $X : \Omega_X \rightarrow \mathbb{R}$ and $Y : \Omega_Y \rightarrow \mathbb{R}$ be two random variables. The sum distribution of $X$ and $Y$, denoted by $X + Y$, is defined as the random variable $X + Y : \Omega_X \times \Omega_Y \rightarrow \mathbb{R}$ that assigns to each pair $\left( a, b \right) \in \Omega_X \times \Omega_Y$ the number $X(a) + Y(b)$.
\end{definition}

{\color{red} TODO: Generalization to $n$ random variables.}

% Marginal Distribution

\subsection{Marginal Distribution}

{\color{red} Often, we start with a joint distribution of two random variables and we then want to find the distribution of just one of them, called the marginal distribution.}

\begin{definition}
Suppose that $X$ and $Y$ have a joint distribution. The c.d.f. of $X$ derived with the above theorem is called the marginal c.d.f. of $X$. Similarly, the p.f. or p.d.f. of $X$ associated with the marginal c.d.f. of $X$ is caled the marginal p.f. or marginal p.d.f. of $X$.
\end{definition}

\begin{proposition}
If $X$ and $Y$ have a discrete joint distribution for which the joint p.f. is f, then the marginal p.f. of $X$ is $f_{1}\left(x\right)=\sum_{y}f\left(x,y\right)$
\end{proposition}
\begin{proof}
\end{proof}

{\color{red} Similarly, the marginal p.f. $f_{2}$ of $Y$ is $f_{2} \left(y\right)=\sum_{x}f\left(x,y\right)$.}

{\color{red} Although the marginal distributions of X and Y can be derived from their joint distribution, it is not possible to reconstruct the joint distribution of X and Y from their marginal distributions without additional information.}

{\color{red} Independent Random Variables}

\begin{definition}
It is said that two random variables $X$ and $Y$ are independent if, for every two sets $A$ and $B$ of real numbers such that $\left\{ X\in A\right\}$ and $\left\{ Y\in B\right\}$ are events $Pr\left(X\in A\enskip and\enskip Y\in B\right)=Pr\left(X\in A\right)Pr\left(Y\in B\right)$
\end{definition}

\begin{proposition}
Suppose that $X$ and $Y$ are random variables that have a joint p.f., p.d.f., or p.f./p.d.f. $f$. Then $X$ and $Y$ will be independent if and only if f can be represented in the following form for $-\infty<x<\infty$ and $-\infty<y<\infty$ $f\left(x,y\right)=h_{1}\left(x\right)h_{2}\left(y\right)$ where $h_{1}$ is a nonnegative function of $x$ alone and $h_{2}$ is a nonnegative function of $y$ alone.
\end{proposition}
\begin{proof}
\end{proof}

\begin{corollary}
Two random variables $X$ and $Y$ are independent if and only if the following factorization is satisfied for all real numbers $x$ and $y$ $f\left(x,y\right)=f_{1}\left(x\right)f_{2}\left(y\right)$
\end{corollary}
\begin{proof}
\end{proof}

{\color{red} Two discrete random variables $X$ and $Y$ are independent if, for each $y$, learning that $Y=y$ does not change any of the probabilities of the events $\left\{ X=x\right\}$ .}

{\color{red} If $X$ and $Y$ are independent, then $h\left(X\right)$ and $g\left(Y\right)$ are independent no matter what the functions $h$ and $g$ are.}

% Conditional Distributions

\subsection{Conditional Distributions}

{\color{red} The conditional distribution of one random variable $X$ given another $Y$ is the distribution that we would use for $X$ after we learn the value of $Y$.}

\begin{definition}
Let $X$ and $Y$ have a discrete joint distribution with joint p.f. f. Let $f_{2}$ denote the marginal p.f. of $Y$. For each y such that $f_{2}\left(y\right)>0$, define $g_{1}\left(x\mid y\right)=\frac{f\left(x,y\right)}{f_{2}\left(y\right)}$
\end{definition}

{\color{red} Then $g_{1}$ is called the conditional p.f. of $X$ given $Y$. The discrete distribution whose p.f. is $g_{1}\left(\cdot\mid y\right)$ is called the conditional distribution of $X$ given that $Y=y$.}

{\color{red} Construction of the Joint Distribution}

\begin{proposition}
Let $X$ and $Y$ be random variables such that $X$ has p.f. or p.d.f. $f_{1}\left(x\right)$ and $Y$ has p.f. or p.d.f. $f_{2}\left(y\right)$. Also, assume that the conditional p.f. or p.d.f. of $X$ given $Y=y$ is $g_{1}\left(x\mid y\right)$ while the conditional p.f. or p.d.f. of $Y$ given $X=x$ is $g_{2}\left(y\mid x\right)$. Then for each $y$ such that $f_{2}\left(y\right)>0$ and each $x$, $f\left(x,y\right)=g_{1}\left(x\mid y\right)f_{2}\left(y\right)$ where $f$ is the joint p.f., p.d.f. or p.f./p.d.f. of $X$ and $Y$. Similarly, for each $x$ such that $f_{1}\left(x\right)>0$ and each $y$, $f\left(x,y\right)=f_{1}\left(x\right)g_{2}\left(y\mid x\right)$
\end{proposition}
\begin{proof}
\end{proof}

{\color{red} Next theorem provides a generalization of the law of total probability to random variables.}

\begin{proposition}
If $f_{2}\left(y\right)$ is the marginal p.f. or p.d.f. of a random variable $Y$ and $g_{1}\left(x\mid y\right)$ is the conditional p.f. or p.d.f. of $X$ given $Y=y$, then the marginal p.f. or p.d.f. of $X$ is $f_{1}\left(x\right)=\sum_{y}g_{\text{1}}\left(x\mid y\right)f_{2}\left(y\right)$
if Y is discrete.
\end{proposition}
\begin{proof}
\end{proof}

{\color{red} The following theorem is a generalization of Bayes' theorem for random variables.}

\begin{theorem}
If $f_{2}\left(y\right)$ is the marginal p.f. or p.d.f. of a random variable $Y$ and $g_{1}\left(x\mid y\right)$ is the conditional p.f. or p.d.f. of $X$ given $Y=y$, then the conditional p.f. or p.d.f. of $Y$ given $X=x$ is $g_{2}\left(y\mid x\right)=\frac{g_{1}\left(x\mid y\right)f_{2}\left(y\right)}{f_{1}\left(x\right)}$
\end{theorem}
\begin{proof}
\end{proof}

\begin{proposition}
Suppose that $X$ and $Y$ are two random variables having a joint p.f., p.d.f., or p.f./p.d.f. $f$. Then $X$ and $Y$ are independent if and only if for every value of $y$ such that $f_{2}\left(y\right)>0$ and every value of $x$ $g_{1}\left(x\mid y\right)=f_{1}\left(x\right)$
\end{proposition}
\begin{proof}
\end{proof}

{\color{red} Conclude that all the above can be generalized to multivariate distributions, and provide a couple of examples of such generalization.}

%
% Section: Expectation
%

\section{Characterizing Distributions}
\label{sec:probability_expectation}

A \emph{measure of central tendency} is a number derived from a probability distribution, intended as a summary of that distribution. The most common measures of central tendency in use are the mean and the median. Each of these measures provides a different approach to characterize distributions. It is also common to use \emph{metrics of dispersion} to describe the variability of a distribution around the measures of centrality. We will review two metrics of dispersion, the variance and the standard deviation. The metrics of disperson can also be used in case of bivariate distributions, under the names of covariance and correlation, to measure the \emph{statistical relationship} between two random variables. All these measures allow us to summarize and compare distributions.

% Subsection: Measures of Central Tendency

\subsection{Measures of Central Tendency}

The most comonly used measure of central tendency is the \emph{mean}. The mean of a collection of outcomes is the weigthed average of these outcomes, where the weights are equal to the probabilities. The mean is also know as expected value or expectation.

\begin{definition}\label{probability:expectation}
Let $X$ be a discrete random variable whose probability function is $f$. The \emph{mean} of $X$, denoted by $E\left(X\right)$, is defined as:
\[
E\left(X\right)=\sum_{x}xf\left(x\right)
\]
\end{definition}

Of course, Definition \ref{probability:expectation} only makes sense if the summation converges. It is also possible that the mean is infinite, but in this book we are only interested in finite means. We have defined the concept of mean based on random variables. In this sense, the definition of mean only takes into account the distribution of the random variables, not the original outcomes. That is, two different random variables with the same distribution will have the same mean. When working with random variables it is common to use the name expeced value instead of mean. However, this name is a little bit misleading, since for the majority of the discrete distributions, the expected value is not one of the possible values of the distribution, i.e., the expected value is not expected at all. For example, if we throw a dice, the expected value would be 3.5. This undesired property of something known as expected value has generated a lot of confusion in scientific research. The expectation of a random variable has a physhical interpretation as the center of gravity of the distribution. Expectation, as the center of gravity, can be greatly affected by a small change in the probability assigned to a large value of $X$.

The expectation of the linear combination of $n$ random variables is the linear combination of their expectations.

\begin{proposition}
Let $X_{1}, \ldots, X_{n}$ be $n$ independent discrete random variables with expectations $E\left(X_{i}\right)$, and let $a_1, \ldots, a_n$ and $b$ constants, then
\[
E\left(a_{1}X_{1}+\ldots+a_{n}X_{n}+b\right)=a_{1}E\left(X_{1}\right)+\ldots+a_{n}E\left(X_{n}\right)+b
\]
\end{proposition}
\begin{proof}
We have that
\begin{multline}
E \left(a_1 X_1 + \ldots + a_n X_n +b \right) = 
\sum_{x_1} \ldots \sum_{x_n} \left(a_ 1 x_1 + \ldots + a_n x_n + b  \right) f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} \ldots \sum_{x_n} a_1 x_1 f\left(x_1, \ldots, x_n \right) + \ldots + \sum_{x_1} \ldots \sum_{x_n} a_n x_n f\left(x_1, \ldots, x_n \right) + \sum_{x_1} \ldots \sum_{x_n} b f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} a_1 x_1 f\left(x_1\right) + \ldots + \sum_{x_n} a_n x_n f\left( x_n \right) + b = 
a_1 \sum_{x_1} x_1 f\left(x_1\right) + \ldots + a_n \sum_{x_n} x_n f\left( x_n \right) + b = \\
a_1 E\left(X_1\right) + \ldots + a_n E\left(X_n\right) + b
\end{multline}
\end{proof}

The expectation of the product of $n$ independent random variables is the product of the individual expectations.

\begin{proposition}
Let $X_{1}, \ldots, X_{n}$ be $n$ independent discrete random variables with expectations $E\left(X_{i}\right)$, then:
\[
E\left(\prod_{i=1}^{n}X_{i}\right)=\prod_{i=1}^{n}E\left(X_{i}\right)
\]
\end{proposition}
\begin{proof}
We have that
\begin{multline}
E \left(X_1  \cdot \ldots \cdot X_n  \right) = 
\sum_{x_1} \ldots \sum_{x_n} \left(x_1 \cdot \ldots \cdot x_n  \right) f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} \ldots \sum_{x_n} x_1 f\left(x_1, \ldots, x_n \right) \cdot \ldots \cdot \sum_{x_1} \ldots \sum_{x_n} x_n f\left(x_1, \ldots, x_n \right) = \\
\sum_{x_1} x_1 f\left(x_1\right) \cdot \ldots \cdot \sum_{x_n} x_n f\left( x_n \right) = 
E \left( X_1 \right) \cdot \ldots \cdot E \left( X_n \right)
\end{multline}
\end{proof}

The expectation of the product of non-independent random variables is not necesarily equal to the product of their individual expectations.

In the area of statistical inference it is also highly convenient to compute the sample mean, as the average of $n$ random variables. In particular, we will compute the sample mean of random samples.

\begin{definition}
Let $X_1, X_2, \ldots, X_n$ be $n$ random variables. The \emph{sample mean}, denoted by $\bar{X}_n$, is defined as:
\[
\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i
\]
\end{definition}

Do not confuse $\frac{1}{n} \left( X_1 + X_2 + \ldots + X_n \right)$, which is a probability distribution, with $E \left( X_1 + X_2 + \ldots + X_n \right)$, which is a real number.

% The Median

\subsubsection*{The Median}


We have seen that the mean of a probability distribution is the center of gravity of that distribution. The actual center of the distribution is called  the \emph{median}.

\begin{definition}
Let $X$ be a discrete random variable. Every number $m$ that satisfy the following properties is called a median of the distribution of $X$:
\[
Pr\left(X\leq m\right)\geq1/2 \quad and \quad Pr\left(X\geq m\right) \geq 1/2
\]
\end{definition}

The median divides a probability distribution in two equal parts. A distribution could have more than one median. And, on the contrary of what happens in case of the expectation, every distribution must have at least one median. An advantage of the median over the mean is that we can move a value $x$ larger to the median to any arbitrary larger value, and the median will be remain the same. 

\subsection{Measures of Dispersion}

Definitions of the Variance and the Standard Deviation

\begin{definition}
Let X be a random variable with finite mean and $\mu=E\left(X\right)$. The variance of X, denoted by $Var\left(X\right)$, is defined as follows: $Var\left(X\right)=E\left[\left(X-\mu^{2}\right)\right]$
\end{definition}

{\color{red} If X has infinite mean or if the mean of X does not exist, we say that $Var\left(X\right)$ does not exist. The standard deviation of X is the nonnegative square root of $Var\left(X\right)$ if the variance exists. It is common to denote the standard deviation by the symbol $\sigma$, and the variance by $\sigma^{2}$. Variance depeds only on the distribution.}

\begin{proposition}
For every random variable X, $Var\left(X\right)=E\left(X^{2}\right)-\left[E\left(X\right)\right]^{2}$.
\end{proposition}
\begin{proof}
\end{proof}

{\color{red} The variance (as well as the standard deviation) of a distribution provides a measure of the spread or dispersion of the distribution around its mean $\mu$. The variance of a distribution, as well as its mean, can be made arbitrarily large by placing even a very small but positive amount of probability far enough from the origin on the real line.}

Properties of the Variance

\begin{proposition}
For constants a and b, let $Y=aX+b$, then $Var\left(Y\right)=a^{2}Var\left(X\right)$
and $\sigma_{Y}=\left|a\right|\sigma_{X}$.
\end{proposition}
\begin{proof}
\end{proof}

{\color{red} If $X_{\text{1}},\ldots,X_{n}$ are independent random variables with finite means, and if $a_{1},\ldots,a_{n}$ and b are arbitrary constants, then $Var\left(a_{1}X_{1}+\ldots+a_{n}X_{n}+b\right)=a_{1}^{2}Var\left(X_{1}\right)+\ldots+a_{n}^{2}Var\left(X_{n}\right)$}

\begin{example}
The Variance of a Binomial Distribution

The variance of a random variable X with a binomial distribution of n samples with probability p is $Var\left(X\right)=np\left(1-p\right)$
\end{example}


\subsection{Measures of Statistical Relationship}



Covariance and correlation are attempst to measure the linear dependence between to random variables.

Covarance

\begin{definition}
Definition 183. Let X and Y be random variables having finite means. Let $E\left(X\right)=\mu_{X}$ and $E\left(Y\right)=\mu_{Y}$. The covariance of X and Y, which is denoted by $Cov\left(X,Y\right)$ is defined as $Cov\left(X,Y\right)=E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right]$
\end{definition}

if the expectation exists.

The covariance between X and Y is intended to measre the degree to which X and Y tend to be large at the same time or the degree to which one tends to be large while the other is small.

\begin{proposition}
For all random variables X and Y such that $\sigma_{X}^{2}<\infty$ and $\sigma_{Y}^{2}<\infty Cov\left(X,Y\right)=E\left(XY\right)-E\left(X\right)E\left(Y\right)$
\end{proposition}
\begin{proof}
\end{proof}

Correlation

Correlation is a measure of association between two random variables that is not driven by arbitrary changes in the scales.

\begin{definition}
Let X and Y be random variables with finite variances $\sigma_{X}^{2}$ and $\sigma_{Y}^{2}$ respectively. Then the correlation of X and Y, which is denoted by $\rho\left(X,Y\right)$, is defined as follows $\rho\left(X,Y\right)=\frac{Cov\left(X,Y\right)}{\sigma_{X}\sigma_{Y}}$
\end{definition}

XX

\begin{definition}
It is said that X and Y are positively correlated if $\rho\left(X,Y\right)>0$, that X and Y are negatively correlated if $\rho\left(X,Y\right)<0$ and that X and Yare uncorrelated if $\rho\left(X,Y\right)=0$.
\end{definition}

Properties of Covariance and Correlation

\begin{proposition}
Moreover $-1 \leq \rho\left(X,Y\right) \leq 1$
\end{proposition}
\begin{proof}
\end{proof}

\begin{proposition}
If X and Y are independent random variables with $0<\text{\ensuremath{\sigma_{X}^{2}}}<\infty$ and $0<\text{\ensuremath{\sigma_{Y}^{2}}}<\infty$ then $Cov\left(X,Y\right)=\rho\left(X,Y\right)=0$
\end{proposition}
\begin{proof}
\end{proof}

The converse is not true as a general rule. Two dependent random variables can be uncorrelated.

\begin{proposition}
Suppose that X is a random variable such that $0<\sigma_{X}^{2}\infty$ and $Y=aX+b$ for some constants a and b, where $a\neq0$. If $a>0$ then $\rho\left(X,Y\right)=1$. If $a<0$, then $\rho\left(X,Y\right)=-1$.
\end{proposition}
\begin{proof}
\end{proof}

The converse is also true, that is, if $\left|\rho\left(X,Y\right)\right|=1$ implies that X and Y are linearly related.

\begin{proposition}
If X and Y are random variables such that $Var\left(X\right)<\infty$ and $Var\left(Y\right)<\infty$, then $Var\left(X+Y\right)=Var\left(X\right)+Var\left(Y\right)+2Cov\left(X,Y\right)$
\end{proposition}
\begin{proof}
\end{proof}

For all constants a and b, it can be shown that $Cov\left(aX,bY\right)=abCov\left(X,Y\right)$.

\begin{proposition}
Let X and Y are random variables such that $Var\left(X\right)<\infty$ and $Var\left(Y\right)<\infty$, and let a, b and c be constants, then $Var\left(aX+bY+c\right)=a^{2}Var\left(X\right)+b^{2}Var\left(Y\right)+2abCov\left(X,Y\right)$
\end{proposition}
\begin{proof}
\end{proof}

A special case is $Var\left(X-Y\right)=Var\left(X\right)+Var\left(Y\right)-2Cov\left(X,Y\right)$

\begin{proposition}
If $X_{1},\ldots,X_{n}$ are random variables such that $Var\left(X_{i}\right)<\infty for i=1,\ldots,n$, then $Var\left(\sum_{i=1}^{n}X_{i}\right)=\sum_{i=1}^{n}Var\left(X_{i}\right)+2\sum\sum Cov\left(X_{i},X_{j}\right)$
\end{proposition}
\begin{proof}
\end{proof}

\begin{proposition}
If $X_{1},\ldots,X_{n}$ are uncorrelated random variables, then $Var\left(\sum_{i=1}^{n}X_{i}\right)=\sum_{i=1}^{n}Var\left(X_{i}\right)$
\end{proposition}
\begin{proof}
\end{proof}


%
% Section: Distribution
%

\section{Common Distributions}
\label{sec:probability_distributions}


\subsection{Uniform Distribution}

\begin{definition}
Let $a \leq b$ be integers. Suppose that the value of a random variable $X$ is equally likely to be each of the integers $a, \ldots, b$. Then we say that $X$ has the uniform distribution on the integers $a, \ldots, b$.
\end{definition}

{\color{red} Introduce the following propostion}

\begin{proposition}
If $X$ has the uniform distribution on the integers $a,\ldots,b$, the p.f. of $X$ is $f\left(x\right)=\begin{cases}
\frac{1}{b-a+1} & for\,x=a,\ldots,b\\
0 & otherwise
\end{cases}$
\end{proposition}
\begin{proof}
\end{proof}

{\color{red} The uniform distribution on the integers $a, \ldots, b$ represents the outcome of an experiment that is often described by saying that one of the integers $a, \ldots, b$ is chosen at random. A uniform distribution cannot be assigned to an infinite sequence of possible values}


\subsection{Bernoulli Distributions}

\begin{example}
A random variable $Z$ that takes only two values $0$ and $1$ with $P\left(Z=1\right)=p$ has the Bernoulli distribution with parameter $p$. We also say that $Z$ is a Bernoulli random variable with parameter $p$.
\end{example}

\subsection{Binomial Distributions}

\begin{example}
Suppose we perform $N$ independent trials where each trial either succeeds or fails with probability of success $p$, and let $X$ the random variable defined by the number of successes. The probability of having exactly $n$ successes $P(X=n)$ follows a \emph{binomial distribution} with parameters $N, p$, defined by:
\[
f(n\mid N, p) = \binom{N}{n} p^n (1-p)^{(N-n)}
\]
\end{example}

\begin{definition}
The discrete distribution represented by the p.f.f $\left(x\right)=\begin{cases}
{n \choose x}p^{x}\left(1-p\right)^{n-x} & for\,x=0,1,\ldots,n\\
0 & otherwise
\end{cases}$
\end{definition}

{\color{red} is called the binomial distribution with parameters $n$ and $p$.}

{\color{red} Consider a general experiment that consists of observing $n$ independent trials with only two possible results for each trial: success and failure. Then the distribution of the number of trials that result in success will be binomial with parameters $n$ and $p$, where $p$ is the probability of success on each trial.} 



%
% Section: Large Random Samples
%

\section{Large Random Samples}
\label{sec:probability_random_samples}

{\color{red} The law of large numbers gives a mathematical foundation to the intuition that the average of a large sample of i.i.d. random variables should be close to their mean. The central limit theorem gives us a way to approximate the probability that the sample average is close to the mean.}

{\color{red} In probability theory, the law of large numbers (LLN) is a theorem that describes the result of performing the same experiment a large number of times.}

% Law of Large Numbers

\subsection{Law of Large Numbers}

{\color{red} In probability theory, the law of large numbers (LLN) is a theorem that describes the result of performing the same experiment a large number of times. According to the law, the average of the results obtained from a large number of trials should be close to the expected value and tends to become closer to the expected value as more trials are performed.}

{\color{red} The law of large numbers gives a mathematical foundation to the intuition that the average of a large sample of i.i.d. random variables should be close to their mean.}

{\color{red} According to the law, the average of the results obtained from a large number of trials should be close to the expected value and tends to become closer to the expected value as more trials are performed.}

\begin{theorem}[Law of Large Numbers]
Let $X_1, \ldots, X_n$ be a random sample from a distribution $f$ for which the mean is $\mu$ and the variance is finite. Then we have that for every number $\varepsilon>0$
\[
 \lim_{n \rightarrow \infty} Pr \left( \left| X_{n} - \mu \right| < \varepsilon \right) = 1
\]
\end{theorem}
\begin{proof}
\end{proof}

{\color{red} Stress the point in the requirements: i.i.d., finite mean and variance}

{\color{red} Since $\overline{X}_{n}$ converges to $\mu$ in probability, it follows that there is high probability that $\overline{X}_{n}$ will be close to $\mu$ if the sample size n is large.}

\begin{example}

{\color{red} For example, a fair coin toss is a Bernoulli trial. When a fair coin is flipped once, the theoretical probability that the outcome will be heads is equal to 1⁄2. Therefore, according to the law of large numbers, the proportion of heads in a "large" number of coin flips "should be" roughly 1⁄2. In particular, the proportion of heads after n flips will almost surely converge to 1⁄2 as n approaches infinity.}

{\color{red} Although the proportion of heads (and tails) approaches 1⁄2, almost surely the absolute difference in the number of heads and tails will become large as the number of flips becomes large. That is, the probability that the absolute difference is a small number approaches zero as the number of flips becomes large. Also, almost surely the ratio of the absolute difference to the number of flips will approach zero. Intuitively, the expected difference grows, but at a slower rate than the number of flips.}

It is also important to note that the LLN only applies to the average. Therefore, while

% {\displaystyle \lim _{n\to \infty }\sum _{i=1}^{n}{\frac {X_{i}}{n}}={\overline {X}}}{\displaystyle \lim _{n\to \infty }\sum _{i=1}^{n}{\frac {X_{i}}{n}}={\overline {X}}}
other formulas that look similar are not verified, such as the raw deviation from "theoretical results":

% {\displaystyle \sum _{i=1}^{n}X_{i}-n\times {\overline {X}}}{\displaystyle \sum _{i=1}^{n}X_{i}-n\times {\overline {X}}}
not only does it not converge toward zero as n increases, but it tends to increase in absolute value as n increases.

{\color{red} Although the proportion of heads (and tails) approaches 1⁄2, almost surely the absolute difference in the number of heads and tails will become large as the number of flips becomes large. That is, the probability that the absolute difference is a small number approaches zero as the number of flips becomes large. Also, almost surely the ratio of the absolute difference to the number of flips will approach zero. Intuitively, the expected difference grows, but at a slower rate than the number of flips.}

\end{example}

{\color{red} Shall we cover the gambler's fallacy?}

% Central Limit Theorem

\subsection{Central Limit Theorem}

{\color{red} Explain the relation to the law of large numbers}

The Central Limit Theorem is a fundamental concept in probability theory used in statistical analysis and inference. It states that the average of a random sample will closely resemble the mean of the population, as the sample sizes increases, and regardless of the shape of the population distribution. More especifically, if we take a random sample of size $n$ from an arbitrary distribution with mean $\mu$ and variance $\sigma^{2}$, the sample average $\overline{X}_{n}$ will have a distribution that is approximately normal with mean $\mu$ and variance $\sigma^{2}/n$.

\begin{theorem}[Central Limit Theorem]
Let $X_{1}, \ldots, X_{n}$ be a random sample of size $n$ from a distribution with mean $\mu$ and a finite variance $\sigma^{2}$. Then  for each fixed number $\varepsilon > 0$ we have that
\[
\lim_{n \rightarrow \infty} Pr \left( \frac{\overline{X}_{n}-\mu}{\sigma/n^{1/2}} \leq \varepsilon \right) = \Phi \left( x \right)
\]
where $\Phi \left( x \right)$ denotes the cumulative distribution function of the standard normal distribution.
\end{theorem}
\begin{proof}
{\color{red} Perhaps is too complex for this book to give a proof of the theorem.}
\end{proof}

The central limit theorem allows us to compute the probability that the sample average is close to the distribution mean. It is important to recall the conditons for the central limit theorem to be true: the samples must be independent and identically distributed, the original distribution has to have a finite variance, and the sample size must be sufficiently large.

\begin{example}
{\color{red} Start with a uniform distribution, for example, throwing a dice. Explain and show how the aritmethic mean can be described with a binomial distribution. Explain that going to the limit results in the normal distribution.}
\end{example}

{\color{red} Elaborate in the fact the central limit theorem does not help if our target metric is not the arithmetic mean, and implications.}
