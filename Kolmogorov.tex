%
% CHAPTER 4.- Kolmogorov Complexity
%

% TODO: Can we talk about independent topics? K(t|s) = K(t)
% TODO: Does K(t|s) != K(s|t)?


\chapterimage{Mandelbrot.pdf}

\chapter{Complexity}
\label{chap:Algorithmic_Information}

\begin{quote}
\begin{flushright}
\emph{Everything should be made as simple as possible, \\
but not simpler. \\}
Albert Einstein
\end{flushright}
\end{quote}
\bigskip

In Chapter \ref{chap:Coding}, the concept of the complexity of a string based on the lengths of the codewords of a prefix-free code was introduced. This definition is limited by two main factors: firstly, it necessitates prior knowledge of the set of possible strings, and secondly, it requires the definition of a probability distribution over this set a priori. It would be highly beneficial if we could expand the set of strings to encompass all strings (that is, $\mathcal{B}^\ast$) without necessitating a probability distribution, thereby providing an absolute notion of string complexity. Unfortunately, even if these issues are addressed, a more significant limitation exists in studying the complexity of strings using codes: certain strings, expected to be classified as simple, cannot be compressed. For instance, the binary expansion of the constant $\pi$ follows a uniform distribution over the set $\left\{0, 1\right\}$ and, as such, cannot be compressed. However, it can be fully and effectively described by a very short mathematical formula. This necessitates an alternative definition of string complexity.

\emph{Kolmogorov Complexity}, also known as \emph{Algorithmic Information Theory}, offers a definition of the complexity of a string that explicitly addresses these issues. Intuitively, the amount of information in a finite string is measured by the length of the shortest computer program capable of producing the string. This approach does not require prior knowledge of the set of valid strings or their probability distribution. Furthermore, objects like $\pi$ are appropriately classified as having low complexity. We could argue that Kolmogorov complexity provides a universal definition of the amount of information that closely aligns with our intuitive understanding. To compute the Kolmogorov complexity of a string, it is necessary to agree upon a universal description method or computer language, along with a universal computer. One might question whether, by doing so, the complexity of a string becomes dependent on the chosen computer language. Fortunately, it has been demonstrated that this is not the case, as all reasonable (and sufficiently powerful) computer languages yield the same description length, up to a fixed constant that depends on the languages chosen, but not on the string itself. Unfortunately, Kolmogorov complexity introduces a significant issue: it is a non-computable quantity, and as such, must be approximated in practice.

At this point, one might wonder if it is possible to compute the complexity of any object in general, not just strings. The answer is yes, at least theoretically. Given an object $x$, the task is to provide an encoding method that represents the object as a string  $x$. This encoding method would only be useful if we can losslessly and effectively reconstruct the original object from its encoded description. Unfortunately, providing such descriptions is not always feasible, either because the objects in question are abstract (as is often the case in mathematics) or because practical reconstruction of the object from its description is currently impossible (for example, with living organisms\footnote{As of now, it is not possible to recreate an animal solely based on its DNA.}).

%
% Section: String Complexity
%

\section{Strings Complexity}
\label{sec:strings_complexity}

In Section \ref{sec:Turing-Machines}, the concept of the Turing machine, an idealized model of computers, was introduced. We saw that Turing machines can be represented as partial computable functions $T:\mathcal{B}^\ast \rightarrow \mathcal{B}^\ast$, which assign to each input string $s \in \mathcal{B}^\ast$ an output string $T(s) \in \mathcal{B}^\ast$ (Definition \ref{def:computable-function}). We also introduced the concept of a universal Turing machine $U:\mathcal{B}^\ast \times \mathcal{B}^\ast \rightarrow \mathcal{B}^\ast$ (Definition \ref{def:Universal-Turing-Machine}), a machine that can simulate the behavior of any other Turing machine; that is, for all $(x,v) \in  \mathcal{B}^\ast \times \mathcal{B}^\ast$, we have that $U(x,v) = T_{x}(v)$. Later, in Section \ref{Codes}, the concept of a code, and in particular, the notion of a prefix-free code, was introduced (Definition \ref{def:Prefix-free-Code}). We saw that this kind of code presents important properties (Theorem \ref{th:Kraft-Inequality}). The next definition merges the best of both worlds, Turing machines and prefix-free codes, and introduces a new type of universal Turing machine.

\begin{definition}
A \emph{prefix-free universal Turing machine}\index{Prefix-free universal Turing machine} is a universal Turing machine $U:\mathcal{B}^\ast \times \mathcal{B}^\ast \rightarrow \mathcal{B}^\ast$ such that, for every $v \in \mathcal{B}^\ast$, the domain $U_{v}$ is prefix-free, where $U_{v}:\mathcal{B}^\ast \rightarrow \mathcal{B}^\ast$ and $U_{v}(p) = U(p, v)$ for all $p \in \mathcal{B}^\ast$.
\end{definition}

Using modern computer science terminology we could say that $U$ is the computer, $p$ is the program and $v$ is the input to the program. Intuitively, the above definition requires that no computer program can be a prefix of any other program. This is not a limitation from the point of view of string lengths, since, by applying McMillan's theorem (Theorem \ref{th:Kraft-Inequality}), given a uniquely decodable program, we could always find a prefix-free one that computes exactly the same function and has the same length. Moreover, in practice, real computer programs are usually prefix-free; for example, the C programming language requires that all functions must be enclosed by braces \{\}.

The concept of a prefix-free universal Turing machine allows us to introduce a new definition of the complexity of a string that aligns more closely with our intuitive understanding of the amount of computational information contained in an object (encoded as a string).

\begin{definition}[Kolmogorov Complexity]
\label{def:Kolmogorov-Complexity}\index{Kolmogorov Complexity}
Fix a prefix-free universal Turing machine $U:\mathcal{B}^\ast \times \mathcal{B}^\ast \rightarrow \mathcal{B}^\ast$. The \emph{Kolmogorov complexity} of a string $s \in \mathcal{B}^\ast$, denoted by $K(s)$, is defined as:
\[
K(s)=\min_{p,v \in \mathcal{B}^\ast}\left\{l(p) + l(v)\,:\, U(p,v)=s\right\}
\]
\end{definition}

Intuitively, the shortest description of a string $s$ is given by two elements: a program $p$ (a self delimited Turing machine) that compress all the regular patterns of the string, and a new string $v$ that comprises those parts of $s$ that do not present any regularity. We have to find the optimum balance between increasing the complexity of the program, trying to grasp more regularities, or increase the size of the non-compressible part\footnote{In the literature the Kolmogorov complexity of the string $s$ is defined as $K(s)=\min_{p \in \mathcal{B}^\ast}\left\{l(p)\,:\, U(p,\lambda)=s\right\}$, that is, the length of the shortest computer program that without any additional input can print the string $s$. We prefer to use the two parts definition $l(p) + l(v)$ because it is more in line with the requirements of the theory of nescience.}.

\begin{example}
Consider the string composed of one thousand times the substring "10", that is "$\underbrace{1010\ldots1010}_{1.000\,\mathrm{times}}$". We could write the following program:

\begin{verbatim}
    example(char *v) {
        for (int i=1; i<=1000; i++)
            printf("%s", v);
    }
\end{verbatim}
and then run it with:
\begin{verbatim}
    example("10");
\end{verbatim}

in order to print it. The length of the original string is 2.000 bits, but the length of the program is 480 bits (assuming that every symbol is encoded using an uniform code of 8 bits), and the length of the input is 2 bits, so we can conclude that the string has a low complexity. Of course, in order to compute the real Kolmogorov complexity of the string we should find the shortest Turing machine that prints that string.

On the contrary, a string composed of two thousands random 0's and 1's would have a high complexity, since it does not exists any program shorter than the program that prints the string itself.
\end{example}

As we mentioned in the preface of this chapter, Kolmogorov complexity would not be particularly useful if the complexity of strings depended on the choice of universal Turing machine. The following theorem demonstrates that this concern is unfounded, up to a constant that depends on the choice of machines, but not on the strings themselves. This establishes Kolmogorov complexity as an inherent property of the strings.

\begin{theorem}[Invariance theorem]
\label{def:Invariance-theorem}\index{Invariance Theorem}
Let $U$ and $U'$ two universal Turing machines. Then, there exists a constant $C_{U, U'}$, that depends on $U$ and $U'$, such that for each string $s \in \Sigma^{\ast}$ we have that:
\[
K_{U}(s) \leq K_{U'}(s) + C_{U, U'}
\]
\end{theorem}
\begin{proof}
Let $p, v$ be the shortest strings that $U'(p,v)=s$, then we have that $U(\langle U',p, v \rangle, \lambda) = U'(p, v) = s$, and that $l(\langle U',p \rangle) = \log(l(\langle U' \rangle)) + 1 + l(\langle U' \rangle) + l(p) = K_{U'}(s) + C$, where $l(\langle U' \rangle)$ is the length of the machine $U'$ using the encoding described in Section \ref{sec:Universal-Turing-Machines}.
\end{proof}

\begin{example}
Consider a universal programming language, such as Java, and an alternative language, such as Python. We can write a Python interpreter in Java, that is, a Java program that takes a Python script as input and executes it. Then, to compute the complexity of a string $s \in \mathcal{B}^\ast$ using Java, $C_J(s)$, it would be no greater than the complexity of the string using Python, $C_P(s)$, plus the length of the Python interpreter written in Java, $C_{J,P}$. Importantly, the length of the interpreter, $C_{J,P}$, does not depend on the string $s$.
\end{example}

Although we have proved that Kolmogorov complexity does not depend on the selected universal Turing machine, the size of the constant \(C_{U, U'}\) could pose a limitation in practical applications, especially when computing the complexity of short strings where the constant might significantly exceed the complexity of the string itself. This challenge is addressed by the Minimum Description Length principle, as described in Section \ref{sec:MML}.

\begin{notation}
We denote by \(s^\ast\) the shortest program that outputs the string \(s\) on the universal Turing machine \(U\), that is, \(s^\ast = \langle p,v \rangle\), \(U(s^\ast) = s\) and \(l(s^\ast) = K(s)\). If more than one program satisfies these properties, we select the first one using a lexicographical order induced by \(0 < 1\).
\end{notation}

The size of the \(C_{U, U'}\) constant is not the only challenge presented by Kolmogorov complexity; another issue is its non-computability, that is, there is no algorithm capable of determining the shortest program that generates any given string. The following theorem on the uncomputability of Kolmogorov complexity marks a pivotal insight into the intrinsic limits of complexity theory. By exploring the theorem, we delve into the heart of computability theory, confronting the paradoxical reality that some of the most fundamental aspects of information are inherently beyond the reach of algorithmic computation.

\begin{theorem}
The function \(K: \mathcal{B}^\ast \rightarrow \mathbb{N}\) that assigns to each string \(s\) its Kolmogorov complexity \(K(s)\) is not computable.
\end{theorem}
\begin{proof}
{\color{red} TODO: Review}
Assume for the sake of contradiction that \(K\) is computable. This means there exists a Turing machine \(T\) such that for any string \(s\), \(T\) halts with \(K(s)\) on its tape. Consider the following process, which uses \(T\) to construct a string \(s\) of arbitrary complexity: i) enumerate all binary strings using a shortlex ordering: \(s_1, s_2, \ldots\), ii) for each string \(s_i\), compute \(K(s_i)\) using \(T\) and select \(s_i\) such that \(K(s_i) > l(s_i) + C\), where \(C\) is a constant representing the length of this description plus the operation to add \(C\). Such a process would effectively produce a string \(s\) whose Kolmogorov complexity is greater than its length plus a constant, which contradicts the definition of Kolmogorov complexity as the length of the shortest description. The contradiction arises because our assumption that \(K\) is computable allows us to construct a string that defies the bounds set by \(K\) itself. Therefore, the function \(K\) is not computable.
\end{proof}

If \(K\) were computable, we could solve the Halting Problem by constructing a program that, for any input program and input, computes whether the program halts by checking if its Kolmogorov complexity is finite. Since the Halting Problem is known to be undecidable, this provides a contradiction.

In practice, we approximate the Kolmogorov complexity using the compressed version of the string, employing standard compression algorithms, such as the Huffman algorithm described in Section \ref{sec:Huffman-Algorithm}.

%
% Section: Properties of Kolmogorov Complexity
%

\section{Properties of Complexity}

In this section, we delve into the properties of Kolmogorov complexity. We will explore the foundational principles that govern this complexity measure, including its invariance, symmetry, and non-computability. Through examining these properties, we gain deeper insights into the complex interplay between information, computation, and randomness.

Kolmogorov complexity is a finite positive natural number.

\begin{proposition}
For all $s\in\mathcal{B}^{\ast}$ we have that $0 < K(s) < \infty$.
\end{proposition}
\begin{proof}
Since $K(s)$ is the length of a string, it must be greater than $0$. The property $K(s) < \infty$ is a consequence of Proposition \ref{prop:kolmogorov_length} and the fact that we are only dealing with finite strings.
\end{proof}

The Kolmogorov complexity of a string cannot surpass the sum of its own length and a constant.

\begin{proposition}
\label{prop:kolmogorov_length}
There is a constant $c$ such that for all $s\in\mathcal{B}^{\ast}$ we have that $K(s) \leq l(s) + c$.
\end{proposition}
\begin{proof}
Let $s \in \mathcal{B}^\ast$ be an arbitrary string, and consider the encoding of a Turing machine $p$ such that for any input $v = s$, it halts and outputs $s$. The program $p$ is designed to simply reproduce its input. Given this setup, when $p$ is executed on a universal Turing machine $U$ with $s$ as input, it satisfies the condition $U(p, s) = s$. The length of \(p\) is a constant $c$ across all strings $s$. This constancy arises because $p$'s mechanism (accepting an input and outputting it unchanged) does not vary with the size or content of $s$. By the definition of Kolmogorov Complexity $K(s)$, which seeks the minimum length of a program-input pair that generates $s$, the combination of $p$ and $s$ presents a feasible solution. Therefore, we have $K(s) \leq l(s) + l(p) = l(s) + c$.
\end{proof}

The size of the constant $c$ depends on the specific encoding schema used by the selected universal Turing machine $U$, but it is independent of the string $s$. In Section \ref{sec:incompressibility_randomness}, we will explore the characteristics of random strings, which are defined as strings that cannot be compressed. Such strings exhibit a Kolmogorov complexity that exceeds their own length, that is, $K(s) > l(s)$.

The absolute difference in Kolmogorov complexity between any string \(x\) and its transformed counterpart \(f(x)\), via a computable bijection, is bounded by a constant $c$. That is, not only does $f$ not increase the complexity of $x$ by more than a constant, but also $f$ does not decrease the complexity by more than a constant.

\begin{proposition}
Let $f:\mathcal{B}^{\ast} \to \mathcal{B}^{\ast}$ is a computable biyection, then there exists a constant $c$ such that $| K\left( f(x) \right) -  K(x) | < c$.
\end{proposition}
\begin{proof}
Let $P_f$ be the program that computes $f$ and $P_{f^{-1}}$ the program that computes the inverse of $f$. For any string $x$, let $P_x$ be the shortest program that generates $x$. Then, a program $P_{f(x)}$ that generates $f(x)$ can be constructed by concatenating $P_x$ with $P_f$. The length of this program is $|P_{f(x)}| = |P_f| + |P_x|$. Since $|P_f|$ is a constant that does not dependent on $x$, we can say that $K(f(x)) \leq K(x) + |P_f|$. Similarly, given $f(x)$, we can construct a program $P'_{x}$ to generate $x$ by applying $P_{f^{-1}}$ to $f(x)$. The length of this program is $|P'_{x}| = |P_f| + |P_{f^{-1}}|$. Thus, $K(x) \leq K(f(x)) + |P_{f^{-1}}|$. The two inequalities combined implies that $|K(f(x)) - K(x)| \leq \max(|P_f|, |P_{f^{-1}}|) = c$, where \(c\) is a constant that represents the maximum of the lengths of the programs that compute \(f\) and \(f^{-1}\). This constant \(c\) does not depend on \(x\), but rather on the complexity of the functions \(f\) and \(f^{-1}\).
\end{proof}

This proposition shows a remarkable stability of informational content under computable bijections, underscoring the intrinsic robustness of Kolmogorov complexity in the face of such transformations.

\begin{example}
{\color{red} TODO: Provide an example clarifying this concept.}
\end{example}

%
% Section: Joint Kolmogorov Complexity
%

\section{Joint Kolmogorov Complexity}

The joint Kolmogorov complexity of two strings $s$ and $t$ is defined as the length of the shortest is the shortest program $p$ that, when executed on an universal Turing machine $U$, outputs the pair $\langle s, t \rangle$, that is, in such a way that allows both strings to be unambiguously retrieved.

\begin{definition}[Joint Kolmogorov Complexity]
\label{def:Joint-Kolmogorov-Complexity}\index{Joint Kolmogorov complexity}
The \emph{Joint Kolmogorov complexity} of the strings $s, t \in \mathcal{B}^\ast$, denoted by $K(s, t)$, is defined as:
\[
K(s, t)=\min_{p,v \in \mathcal{B}^\ast}\left\{l(p) + l(v)\,:\, U(p,v)=\langle s, t \rangle \right\}
\]
\end{definition}

The notation $K(s, t)$ and $K(st)$ represent two different concepts in the context of Kolmogorov complexity. $K(s, t)$ refers to the joint Kolmogorov complexity of two strings $s$ and $t$ as per Definition \ref{def:Joint-Kolmogorov-Complexity}, meanwhile $K(st)$ represents the Kolmogorov complexity of the concatenation of $s$ and $t$, without any additional structure to distinguish between them, and so, Defintion \ref{def:Kolmogorov-Complexity} is applied. The choice between $K(s, t)$ and $K(st)$ depends on whether it's important to preserve and utilize the distinction and relationship between $s$ and $t$. If analyzing the interplay or the shared characteristics of $s$ and $t$ is relevant, $K(s, t)$ is more appropriate. If the focus is on the information content of the combined sequence without regard to its origin from two separate strings, $K(st)$ is used.

\begin{example}
{\color{red} TODO: Provide an example clarifying this concept.}
\end{example}

Our first proposition highlights a fundamental symmetry in Kolmogorov complexity, illustrating that the complexity of describing a pair of strings in either order differs by at most a constant. This reflects the intrinsic property that the information content is independent of the specific arrangement of the strings being described. The constant \(c\) encapsulates the overhead associated with the operations needed to reverse the order of the strings.

\begin{proposition}
\label{prop:kolmogorov_order}
There is a constant $c$ such that for all $x, y \in\mathcal{B}^{\ast}$ we have that $K(x, y) \leq K(y, x) + c$.
\end{proposition}
\begin{proof}
The key to this proof lies in the symmetry of information and the properties of a universal Turing machine. Let $U$ be a universal Turing machine, and let $p_{xy}$ and $p_{yx}$ be the shortest programs that output $x$ followed by $y$, and $y$ followed by $x$, respectively, when executed on $U$. To convert $p_{xy}$ into a program that outputs $yx$, we need to add or modify a finite set of instructions to swap the output order. This set of instructions has a fixed length, denoted by $c$. Therefore, the program that first computes $x$ and $y$ and then swaps their order can be at most $c$ bits longer than the program that directly computes $y$ and $x$.
\end{proof}

Next proposition underscores the subadditive nature of Kolmogorov complexity, proving that the total complexity of describing two strings jointly cannot exceed the sum of their individual complexities by more than a fixed constant, irrespective of the strings' content.

\begin{proposition}
\label{prop:additive_kolmogorov}
There is a constant $c$ such that for all $s, t \in\mathcal{B}^{\ast}$ we have that $K(s, t) \leq K(s) + K(t) + c$.
\end{proposition}
\begin{proof}
Let $s^\ast$ and $t^\ast$ be the shortest computer programs that generate $s$ and $t$, respectively. Given that $s^\ast$ and $t^\ast$ are designed to be prefix-free, a universal Turing machine $U$ can unambiguously decode each program without needing any additional information and subsequently generate the strings $s$ and $t$. The constant $c$ accounts for the requirement of $U$ to prepend the concatenated program for $st$ with the length $l(s)$ of $s$. This step ensures that the concatenated sequence $st$ can be decoded unambiguously.
\end{proof}

The last proposition states the relationship between the complexity of individual strings and their joint complexity, by establishing a lower bound on the joint complexity of two strings relative to their individual complexities.

\begin{proposition}
\label{prop:excess_kolmogorov}
There is a constant $c$ such that for all $s, t \in \mathcal{B}^{\ast}$ we have that $K(s, t) \geq K(s) + c$ and $K(s, t) \geq K(t) + c$.
\end{proposition}
\begin{proof}
Assume for the sake of contradiction that $K(s, t) < K(s) + c$. Let $p$ be the shortest computer program that outputs the pair $\langle s, t \rangle$, meaning that $K(s, t) = l(p)$. Since program $p$ unambiguously generates $s$ as part of the output pair, it follows that the complexity of generating $s$ alone, $K(s)$, should not exceed the length of $p$. which is a contradiction with respect the intial assumption of $K(s, t) < K(s) + c$. 
\end{proof}


%
% Section: Conditional Kolmogorov Complexity
%

\section{Conditional Kolmogorov complexity}

In this section, we explore the concept of \emph{conditional Kolmogorov complexity}, which examines how the complexity of a string $s$ can be significantly decreased by assuming prior knowledge of another string $t$. This notion highlights the impact of assuming some background information on the compressibility of a description.

\begin{definition}[Conditional Kolmogorov Complexity]
\index{Conditional Kolmogorov complexity}
The \emph{conditional Kolmogorov complexity} of a string $s \in \mathcal{B}^{\ast}$ given the string $t \in \mathcal{B}^{\ast}$ is defined as:
\[
K(s|t)=\min_{p, v \in \mathcal{B}^{\ast}}\left\{l(p) + l(v)\,:\, U(p,\langle v, t \rangle)=s\right\}
\]
\end{definition}

Similar to the non-conditional Kolmogorov complexity, the conditional complexity of a string $s$, given a background string $t$, depends on the strings themselves rather than the specific universal Turing machine used for computation. That is, for any two universal Turing machines, $U$ and $U'$, it is easy to show that there exists of a constant $C_{U, U'}$ such that for all strings $s, t \in \Sigma^{\ast}$, the inequality $K_{U}(s|t) \leq K_{U'}(s|t) + C_{U, U'}$ holds. This property underscores the inherent machine-independence of conditional complexity measures.

\begin{example}
{\color{red} TODO: Provide an example clarifying this concept.}
\end{example}

As it was the case of the unconditional Kolmogorov complexity, conditional Kolmogorov complexity is a finite positive natural numbrer.

\begin{proposition}
For all $s, t \in\mathcal{B}^{\ast}$ we have that $0 < K(s | t) < \infty$.
\end{proposition}
\begin{proof}
Since $K(s)$ is the length of a string, it must be greater than $0$. The property $K(s | t) < \infty$ is a consequence of Proposition \ref{prop:kolmogorov_length} and the fact that we are only dealing with finite strings.
\end{proof}

Next proposition posits that when a string $s$ is conditioned upon itself, its complexity stabilizes to a universal constant $c$.

\begin{proposition}
\label{prop:self_conditional}
There is a constant $c$ such that for all $s\in\mathcal{B}^{\ast}$ we have that $K(s | s ) = c$.
\end{proposition}
\begin{proof}
When a string $s$ is conditioned on itself, the information needed to generate $s$ from $s$ can be encapsulated in a Turing machine that simply copies its input to its output. This machine, being independent of the specific content of $s$, has a fixed length $c$. 
\end{proof}

This proposition explores the relationship between unconditional and conditional Kolmogorov complexities, establishing an upper bound for the latter. It asserts that for any strings $s$ and $t$, the complexity of $s$ given $t$ is at most the complexity of $s$ alone, plus a constant $c$. This highlights the intuitive notion that having additional information at most reduces the complexity of describing a string, or in the worst case, adds a constant overhead, but does not increase it beyond this bound.

\begin{proposition}
\label{prop:kolmogorov_conditional}
There is a constant $c$ such that for all $s, t \in \mathcal{B}^{\ast}$ we have that $K(s | t ) \leq K(s) + c$.
\end{proposition}
\begin{proof}
There exists a Turing machine $T'$ that ignores its input and $t$ directly generates $s$. The length of $T'$ is $K(s)$ by definition.
\end{proof}

The relationship between conditional, unconditional, and joint Kolmogorov complexities, offers a comprehensive perspective on the informational interdependencies of binary strings. It posits that the complexity of a string $s$ given another string $t$ is at most the complexity of $s$ alone, which in turn is no greater than the joint complexity of both $s$ and $t$. The proposition encapsulates the intuitive understanding that knowledge of additional data (in this case, $t$) cannot increase the complexity of describing $s$, and that the combined description of two strings is at least as complex as describing each independently.

\begin{proposition}
\label{prop:kolmogorov_relations}
For all $s, t\in\mathcal{B}^{\ast}$ we have that $K(s | t ) \leq K(s) \leq K(s, t)$.
\end{proposition}
\begin{proof}
Given Proposition \ref{prop:kolmogorov_conditional} and Proposition \ref{prop:excess_kolmogorov}.
\end{proof}

The Kolmogorov complexity chain rule is a fundamental principle that connects the joint complexity of two strings with their individual and conditional complexities. It asserts that the total complexity of a pair of strings $s$ and $t$ can be decomposed into the complexity of $s$ plus the complexity of $t$ given $s$. This relationship mirrors the additive property of entropy in information theory ({\color{red} see Proposition XXX}) and provides a powerful tool for understanding the interplay between information content and conditional information in the context of Kolmogorov complexity.

\begin{proposition}
\label{prop:kolmogorov_chain_rule}
For all $s, t \in \mathcal{B}^{\ast}$ we have that $K(s, t) = K(s) + K(t \mid s)$
\end{proposition}
\begin{proof}
{\color{red} TODO}
\end{proof}

\begin{example}
{\color{red} TODO: Provide an example clarifying this concept.}
\end{example}

%
% Section: Information Distance
%

\section{Information Distance}
\label{sec:information_distance}

In this section, we aim to introduce a universal metric for quantifying the absolute information distance between two or more individual entities encoded as strings of symbols. Intuitively, the information distance between two strings $s$ and $t$ can be understood as the length of the shortest computer program for a universal computer that enables the generation of $s$ given $t$ and vice versa.

\begin{definition}
The \emph{information distance}\index{Information distance} between two strings $s,  t \in \mathcal{B}^{\ast}$ with respect to a universal Turing machine $U$, denoted by $ID_U(s, t)$, is defined as
\[
ID_U(s, t) = \min \{ l(p) : U(p, s) = t, U(p, t) = s \}
\]
\end{definition}

It can be shown that for any pair of universal Turing machines, $U_1$ and $U_2$, the information distance between two strings $s$ and $t$ differs by no more than a constant $c$; this constant $c$ depends on the choice of $U_1$ and $U_2$ but is independent of the specific strings $s$ and $t$. It's also important to note that despite its theoretical utility, information distance is non-computable, meaning it does not exists and algorithm to calculate it in practice.

The idea of using the conditional Kolmogorov complexity of $s$ given $t$,that is $K(s \mid t)$, as a metric for information distance may seem appealing. However, this approach is unsuitable for capturing the essence of information distance because of its inherent asymmetry (refer to Proposition {\color{red} XXX}). For example, the complexity $K(\lambda \mid t)$ remains minimal even when $t$ significantly differs from the empty string. Alternatively, employing the sum $K(s \mid t) + K(t \mid s)$ as a measure of distance is also inadequate, as it overlooks the redundancy in the information necessary for transforming $s$ into $t$ and vice versa.

Next proposition shows the proper way in which the information distance between two strings can be computed using their conditional Kolmogorov complexity.

\begin{proposition}
Let $s,  t \in \mathcal{B}^{\ast}$ be two binary strings, then we have that:
\[
ID_U(s, t) = \max\{ K(s \mid t), K(t \mid s) \} + O ( \log \max\{ K(s \mid t), K(t \mid s) \}) 
\]
\end{proposition}
\begin{proof}
{\color{red} TODO: based on the maximal overlap.}
\end{proof}

\begin{example}
{\color{red} TODO: Give an example based on the bitwise exclusive or.}
\end{example}

{\color{red} Introduce the concept of max distance as:}
\[
E(x, y) = \max\{ K(x \mid y), K(y \mid x) \}
\]

{\color{red} Introduce the following proposition.}

\begin{proposition}
{\color{red} $E(x, y)$ is a metric.}
\end{proposition}
\begin{proof}
\end{proof}

{\color{red} Introduce the following proposition.}

\begin{proposition}
\[
E(x, y) = \max\{ K(x \mid y), K(y \mid x) \} = K(xy) - \min\{ K(x), K(y) \} + O(\log K(xy) )
\]
\end{proposition}
\begin{proof}
{\color{red} TODO: Pending}
\end{proof}

{\color{red} TODO: Introduce the concept of admisible information distance. Prove that $E(x, y)$ is an admisible information distance. Prove that it is minimal for every admisible information distance. Explain this notion of universality.}

{\color{red} This concept of information distance is universal because it encompasses all other computable distance metrics as special cases.}

\subsection*{Normalized Information Distance}

Information distance is an absolute measure; however, when assessing similarity, we are often more concerned with relative measures. For instance, consider two strings each of length 1,000,000 that differ by 1000 bits; we would perceive these strings as being relatively more similar compared to two strings of length 1000 bits that differ by the same amount of 1000 bits. This concept of normalization implies that the size of the description required for transformation should be evaluated in the context of the sizes of the objects involved.

\begin{definition}
The \emph{normalized information distance}\index{Normalized information distance} between two binary strings $s,  t \in \mathcal{B}^{\ast}$, denoted by $e(s, yt)$, is defined as:
\[
NID(s, t) = \frac{\max\{ K(s \mid t), K(t \mid s) \}}{\max \{ K(s), K(t) \} }
\]
\end{definition}

As expected, the normalized information distance is a number between zero and one.

\begin{proposition}
\label{prop:ncd_between_zero_and_one}
The normalized information distance $NID(s, t)$ takes values in the range $[0, 1]$.
\end{proposition}
\begin{proof}
{\color{red} TODO: Pending}
\end{proof}

{\color{red} Introduce the following proposition.}

\begin{proposition}
The normalized information distance $NID(x, y)$ is a metric, up to negligible errors.
\end{proposition}
\begin{proof}
{\color{red} TODO: Pending}
\end{proof}

{\color{red} Introduce the following proposition.}

\begin{proposition}
\[
NID(x, y) = \frac{ K(xy) - \min\{ K(x), K(y) \} + O(\log K(xy) ) }{ \max \{ K(s), K(t) \} }
\]
\end{proposition}
\begin{proof}
{\color{red} TODO: Pending}
\end{proof}

\subsection*{Normalized Compression Distance}

Although the normalized information distance metric is not computable, it has a potential wide range of applications. By approximating $K$ with practical compressors, where $Z(s)$ represents the binary length of the string $s$ compressed using a compressor $Z$ (such as "gzip", "bzip2", "PPMZ"), we can approximate the concept of normalized information distance. 

\begin{definition}
The \emph{normalized compression distance}\index{Normalized compression distance} between two strings $s,  t \in \mathcal{B}^{\ast}$, given the compressor $Z$, and denoted by $NCD_Z(s, t)$, is defined as:
\[
NCD_Z(s, t) = \frac{\max\{ Z(s \mid t), Z(t \mid s) \}}{\max \{ Z(s), Z(t) \} }
\]
\end{definition}

Unfortunately, for the majority of the compressors it is very difficult to compute the compressed conditional version of a string $Z(s \mid t)$. Fortunately, we can rewrite the NCD in a differnt way in which we do not need to use conditional compressions.

\begin{proposition}
The normalized compression distance between two strings $s,  t \in \mathcal{B}^{\ast}$, given the compressor $Z$, satisfies:
\[
NCD_Z(s, t) = \frac{ Z(st) - \min\{ Z(s), Z(t) \}}{\max \{ Z(s), Z(t) \} }
\]
\end{proposition}
\begin{proof}
{\color{red} TODO: prove.}
\end{proof}

The normalized compression distance constitutes a spectrum of distances, each defined by the choice of compressor $Z$. The effectiveness of $Z$ determines how closely the normalized compression distance mirrors the normalized information distance, ultimately influencing the quality of the outcomes.


%
% Section: Incompressibility and Randomness
%

\section{Incompressibility and Randomness}
\label{sec:incompressibility_randomness}

{\color{red} TODO: Section Pending!}

It is easy to prove that the mayority of strings cannot be compressed. For each $n$ there are $r^n$ string of length $n$, but only Sum possible shorter descriptions. Therefore, there is at least one string $x$ of length $n$ such that K(n) >= n. We call such strings \emph{incompressible}

\begin{definition}
For each constant $c$ we say that a string $s$ is \emph{c-incompressible} if $K(s)\geq l(s)-c$.
\end{definition}

We will use the term \emph{incompressible string} to refer to all the strings that are c-incompressible with small c. Those string are characterized because they do not contain any patterns, since a pattern could be used to reduce the description length. Intuitively, we think of such patternless sequences as being \emph{random}.

Th number of strings o length n thare are c-incompressible is ast least $2^n - 2^(n-c) + 1$. Hence there is ate least one 0-incompressible string of length $n$, at least one-half of all strings of length n are 1-incompressible, at lesat three-fourths of all strings of length $n$ are 2-incompressible, ..., and at leas the $(1-1/2^c)$ th part of all $2^n$ strings of length $n$ are c-incompressible. This means that for each constant c>1 the majority of all strings of length $n$ with n>c are c-incompressible.


%
% Section: Bibliography and References
%

\section*{Bibliography and References}

{\color{red} TODO: Section Pending!}

{\color{red} Kolmogorov Complexity, a fundamental concept in the field of information theory and computer science, emerged in the 1960s from the work of three pioneering researchers: Andrei Kolmogorov, Ray Solomonoff, and Gregory Chaitin. Independently arriving at similar concepts, these scholars sought to formalize a measure of the information content or complexity of an object (specifically, finite strings) in a way that is independent of the specific language or encoding used. Kolmogorov, a prominent Soviet mathematician, introduced his formulation in 1965, aiming to provide a theoretical foundation for understanding the complexity of individual objects beyond the probabilistic framework of Shannon's information theory. Solomonoff, an American information theorist, introduced a related concept as part of his work on algorithmic probability, emphasizing its implications for prediction and inductive inference. Chaitin, working in the United States as well, contributed by exploring the mathematical properties and implications of this notion, including its non-computability. The motivations behind the development of Kolmogorov Complexity were multifaceted, encompassing the desire to better understand the nature of information, randomness, and the limits of computation and prediction, thus laying the groundwork for numerous applications in theoretical computer science, mathematics, and beyond. }


The idea of measuring the amount of information contained in a string based on the shorter computer program that can reproduce it was introduced independelntly by Solomonoff \cite{solomonoff1964formal}, Kolmogorov \cite{kolmogorov1965three} and Chaiting \cite{chaitin1969simplicity}.

In the Kolomogorov complexity literature, the concept of Kolmogorov complexity is defined in terms of computable functions, i.e. Turing machines. Then it is shown that there are some machines that do not provide worse descriptiosn than others, up to a constant. And finally, it is proved that unversal Turing machines are not worse than any other machines. We have provided here an easier to understand short-cut to this approach. Also, we do not consider the case of of non-prefix Kolmogorov complexity. And we prefer to split code and data. See XX, XX, XX, or XX for a more classical introduction to Kolmogorov complexity.

