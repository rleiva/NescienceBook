%
% CHAPTER 6.- The Theory of Nescience
%

% TODO: Review that we always say "we define" instead of "it is defined"
% TODO: We promised to prove that \sigma(d_{t,s} \geq \sigma{d_t}

%
%  Section 1:
%    - (2nd)   How Shannon's entropy and nescience relates?
%    - (2nd)   Plot nescience. Is it concave or convex?
%    - (2nd)   How the complexity of a topic relates to randomness 
%  Section 2:
%    - TODO:   Talk about prefix-free string
%

\chapterimage{owl.pdf} % Chapter heading image

\chapter{Nescience}
\label{chap:Nescience}

\begin{quote}
\begin{flushright}
\emph{There are known knowns. These are things we know that we know.
There are known unknowns. That is to say, there are things that we know we don't know.
But there are also unknown unknowns. There are things we don't know we don't know.} \\
Donald Rumsfeld
\end{flushright}
\end{quote}
\bigskip

After a preliminary Chapter \ref{cha:Topics-and-Descriptions}, in which we formally defined the concepts of entity, representation, and description, and subsequent Chapters \ref{chap:Miscoding}, \ref{chap:Error} and \ref{chap:Redundancy}, where we introduced new metrics for miscoding, inaccuracy, and surfeit, we now have all the necessary elements to delve into the critical concept of nescience in this chapter and examine its primary properties.

Nescience, on the contrary of what happens with the entropy of Shannon or the complexity of Kolmogorov, is not a measure of the quantity of information; instead, what it measures is the lack of information, that is, the unknown. According to the theory of nescience, how much we do not know about a research entity is given by the three new metrics studied: miscoding, inaccuracy and surfeit. Miscoding measures how good is the representation of the original entity as a string of symbols that we can use in our research; inaccuracy measures how well our current best model describe that string of symbols; and sufeit measures how well we understand the description itself, based on the length, or number of symbols, of the model. The problem of these quantities is that they are conflicting, in the sense that decreasing one of them could increase the other two. We have to find a way to minimize all of them at the same time, that is, science is a multiobjective minimization problem.

One of the most important consequences of our definition of nescience, based on the metrics of miscoding, inaccuracy and surfeit, is that it divides the space of research topics into two different areas. The first area is what we call the known unknown, that is, the set of topics that we do not fully understand, but that we are aware that we do non fully understand them. The second area is the unknown unknown, composed by those topics that have not been discovered yet. An inportant application of the theory of nescience is as a methodology for the discovery of what hides in the unknown unknown. A second important consequence of the concept of nescience is the highly counterintuitive property that sometimes, for a certain class of topics, further research could be a counterproductive activity. That is, the more research we do, the less we know, since for these topics it is not possible to increase our knowledge beyond a critical point, even if this point we are far from having a perfect knowledge.

%
% Section: Nescience
%

\section{Nescience}

Intuitively, how much we do not know about a topic should be based on the quality of the model we are using to describe it, that is, its capacity to eplain why things happen. In the theory of nescience we propose to quantitatively measure how much we do not know about a research entity using the misscoding of a string-based representation of the entity, and the inaccuracy and surfeit of the model describing this representation. Miscoding because it tells us how good the representation is at encoding the entity, inaccuracy since it measures how close our model is to adequately describe the representation, and surfeit to quantify how much unnecessary effort we are putting into the model. We believe that the goal of Science should be to minimize these three quantities: miscoding, inaccuracy and surfeit. Unfortunately they are conflicting metrics, in the sense that decreasing one of them could increase the other two.

According to the theory of nescience, science is about solving the following multiobjective optimization problem\footnote{Thechnically speaking science is a deterministic discrete nonlinear nonconvex nondifferentiable multiobjective optimizaton problem with a single decision maker.}:

\begin{tBox}
\textbf{The Science Problem}
\begin{align*}
 & \text{minimize} \quad \{ \mu(r), \iota(d, r), \sigma(d, r)\} \\
 & \text{subject to} \quad (r, d) \in \mathcal{B}^\ast \times \mathcal{D}
\end{align*}
\end{tBox}

A \emph{scientific mehtod} (see Section \ref{sec:scientific_method}) would be any computable procedure to solve the above minimization problem.

The feasible region is the cartesian product $\mathcal{B}^\ast \times \mathcal{D}$ of the set of finite binary strings $\mathcal{B}^\ast$ and the set of descriptions $\mathcal{D}$, the decision vectors are pairs $(r, d)$ composed by a representation and a description, and the objective functions to minize are miscoding, surfeit and inaccuracy. The objective region is a subset of $\mathbf{Z} \subset \mathbb{R}^3$ and its elements are the objective vectors.

In our characterization of science and the scientific method we do not involve the set $\mathcal{E}$ of entities, since requiring to know the entity $e \in \mathcal{E}$ under study would make science an ill-defined problem for the majority of the research areas. Science is basically a problem of manipulating strings of symbols. From a practical point of view, science is about finding strings that have a meaningful intepretation in the real world, and that allow us to solve practical problems. From a more theoretical point of view, the goal of science would be to understand how a unknown abstract oracle works.

If the set $\mathcal{R}_e$ of representations of the particular entity $e$ in which we are interested is known, or approximately known, we could restrict the science problem to:
\begin{align*}
 & \text{minimize} \quad \{ \mu(r), \iota(d, r), \sigma(d, r)\} \\
 & \text{subject to} \quad (r, d) \in \mathcal{R}_e \times \mathcal{D}
\end{align*}
In the theory of nescience we are mostly interested in the decision space $\mathcal{B}^\ast \times \mathcal{D}$ of respresentations and descriptions reather than in the objective space $\mathbf{Z} \subset \mathbb{R}^3$ of metric values. In the following definitions we re-introduce some of the concepts of multiobjective optimization (see Section \ref{sec:multiobjective_optimization}) for the particular case of the science problem.

If the representation and description currently in use are not perfect, our goal is to find another representation, or another description, that reduces at least one of the metrics miscoding, inaccuracy or surfeit without deteriorating either of the other two.

\begin{definition}
We say that a decision vector $(r, d) \in \mathcal{B}^\ast \times \mathcal{D}$ \emph{dominates} another decision vector $(r', d') \in \mathcal{B}^\ast \times \mathcal{D}$ if it improves in one of the metrics miscoding, inaccuracy or surfeit without deteriorating either of the other two.
\end{definition}

For the majority of the applications, it does not exists a single solution that simultaneously minimize the three metrics. Instead, what we have is a collection of Pareto optimal solutions that define an optimal frontier.

\begin{definition}
We say that a decision vector $(r, d) \in \mathcal{B}^\ast \times \mathcal{D}$ is \emph{Pareto optimal} if there does not exists another decision vector $(r', d') \in \mathcal{B}^\ast \times \mathcal{D}$ such that $(r', d')$ dominates $(r, d)$. The set of Pareto optimal solutions, denoted by $\mathbf{P}_{\mathcal{B}^\ast \times \mathcal{D}}$, is called the \emph{Pareto frontier}.
\end{definition}

The concepts of dominiation and Pareto optimality can be defined in the same way for the restricted decision space $\mathcal{R}_e \times \mathcal{D}$.

In general, weakly Pareto optimal solutions are not relevant in the theory of nescience. {\color{red} Explain why.}

% Range of Solutions

\subsection{Range of Solutions}

As we have seen in Section \ref{sec:range_solutions}, an objective vector that minimizes all objective functions is called an ideal objective vector. For the case of the science problem, the ideal objective vector would be the origin $(0, 0, 0)$, in which we have zero miscoding, zero inaccuracy and zero surfeit.

\begin{proposition}
The ideal objective vector of the Science Problem is the origin $(0, 0, 0)$.
\end{proposition}
\begin{proof}
{\color{red} TODO}
\end{proof}

A decision vector $(r, d) \in \mathcal{B}^\ast \times \mathcal{D}$ is ideal if it has zero miscoding, zero inaccuracy and zero surfeit. That is, the representation $r$ is {\color{red}valid}, the output of the model $d$ is $r$, and it does not exist another shorter model $d'$ with zero inaccuracy. Intuitively, a decision vector $(r, d)$ is ideal if there exists an enty $e \in \mathcal{E}$, such that $r$ perfectly encodes $e$, and the model $d$ of $r$ is accurate and minimal. Ideal decision vectors represent the concept of perfect knowledge in the theory of nescience.

{\color{red} Introduce the following proposition.}

\begin{proposition}
Let $(r, d) \in \mathcal{B}^\ast \times \mathcal{D}$ be an ideal decision vector, then $d$ must be a random string.
\end{proposition}
\begin{proof}
{\color{red} TODO}
\end{proof}


The upper bound of the Pareto optimal set is given by the nadir objective vector. In the theory of nescience, the nadir vector would be the $(1, 1, 1)$ vector, in which we have the maximum miscoding, the minimal accuracy, and the maximun surfeit.

\begin{proposition}
The nadir objective vector of the Science Problem is the vector $(1, 1, 1)$.
\end{proposition}
\begin{proof}
{\color{red} TODO}
\end{proof}

The nadir vector represents the situation in which we have zero knowledge, that is, {\color{red} it is impossible to find another decision vector $(r, d)$ ... }

\begin{example}
{\color{red} TODO: an example that clarify the relation between knowledge and randomnes.}
\end{example}

{\color{red} TODO: Relative vs. absolute values in the objective space.}

{\color{red} TODO: Interpretation of the nadir vector in the decision space.}

% Trade-offs

\subsection{Trade-offs}

{\color{red} TODO: re-introduce the trade-offs and evaluate them in the global context of nescience.}

%
% Minimizing Nescience
%

\section{Minimizing Nescience}

{\color{red} Introduce the concept of nescience decision maker as a function of miscoding, inaccuracy and surfeit, value function}

% Global Criterion

\subsection{Global Criterion}

{\color{red} The good news are that we have designed those metrics so that they are conmensurable, that is, all of them are measured using the same unit: the length of a computer program. Morover, they are in the same scale, since they have been normalized to the interval between zero and one.}

The global criterion (see Section \ref{sub:multiobjective_global_criterion}) is a method for solving multi-objective optimization problems in which the distance between some reference point and the feasible objective region is minimized. As reference point it is usually used the ideal vector, which in our case is the origin $(0, 0, 0)$ of the objective space. And as distance, we could use different metrics. For example, the global criterion based on the origin and an Euclid distance requires to solve the following minimization problem:
\begin{align*}
    & \text{minimize} \quad \sqrt{ \mu(r)^2 + \iota(d, r)^2 + \sigma(d, r)^2 } \\
    & \text{subject to} \quad (r, d) \in \mathcal{B}^\ast \times \mathcal{D}
\end{align*}
As Proposition \ref{prop:global_criterion_pareto_optimal} proved, the solutions provided by the global criterion are Pareto optimal. However, not all the solutions from the Pareto frontier are considered as candidate solutions using this method.

Some examples of alternative distances than can be used with the global criterion method are the following:

{\color{red} TODO: Filter out non-distances.}

\begin{itemize}
\item Arithmetic mean: $\frac{\mu(r) + \iota(d, r) + \sigma(d, r)}{3}$
\item Geometric mean: $\left( \mu(r) \times \iota(d, r) \times \sigma(d, r) \right)^{1/3}$
\item Product: $\mu(r) \times \iota(d, r) \times \sigma(d, r)$
\item Addition: $\mu(r) + \iota(d, r) + \sigma(d, r)$
\item Harmonic mean: $\frac{3}{ \mu(r)^{-1} + \iota(d, r)^{-1} + \sigma(d, r)^{-1} }$
\end{itemize}

{\color{red} TODO: Mention advantages and drawbacks of this alternative solutions.} Geometric mean, product and harmonic mean have the problem that the nescience is zero, or not defined, if one of the three metrics (miscoding, inaccuracy or surfeit) is zero.

{\color{red} TODO: It is still an open question which one is the best utility function to compute the nescience of a dataset and a model.}

{\color{red} In practice it is convenient to normalize the range of the objective values, so that those points closer to the ideal vector do not receives more importance. A common normalization term used in practice is $z_i^{nad} - z_i^\star$.}


{\color{red}

\begin{example}
\label{ex:nescience}
Let $t \in \mathcal{T}$ a topic, and $d_1$ and $d_2$ two descriptions of $t$. Assume that the error of $d_1$ is $0.1$ and its redundancy $0.4$, and that $d_2$ has an error $0.2$ and a redundancy of $0.2$. According to Definition \ref{def:nescience}, the nescience of topic $t$ given the description $d_1$ would be $0.41$, and the nescience given the description $d_2$ would be $0.28$. Our unknown about topic $t$ would be smaller in case of description $d_2$ than with description $d_1$.
\end{example}

What Example \ref{ex:nescience} show us is that, given our definition of nescience, we should prefer a less redundant explanation that maybe does not describe perfectly a topic, to a highly redundant description that fits the topic much better. The Occam's razor principle states that between two indifferent alternatives we should select the simplest one. Our theory states that even in case they are not indifferent alternatives, it might be worth to select the the simplest one. The theory of nescience has not been designed to find the true about a topic, in its philosophical sense, but to find the best theory that can be used in practice. In this sense, we borrow concepts and ideas from the area of machine learning, and in particular, from the problem of \emph{model overfitting} when dealing with the results of an experiment (see Chapter \ref{chap:Model-Evaluation-Selection} for more information about this problem).

{\color{red} TODO: Explain why not other metrics of distance.}

\begin{proposition}
\label{prop:range_redundancy}
We have that $0 \leq \rho(d_t) \leq 1$ for all $t \in \mathcal{T}$ and $d_t \in \mathcal{D}$.
\end{proposition}
\begin{proof}
Given that $l\left(d_t\right)>0$ and that $K\left(t\right)>0$, since they are the lengths of non-empty strings, we only need to prove that $l\left(d_t\right) \geq K\left(t\right)$; however $l\left(d_t\right) < K\left(t\right)$ is a contradiction with the fact that $K\left(t\right)$ is the length of the shortest possible Turing machine that prints out $t$.
\end{proof}

TODO: Weak nescience

% Approximation of nescience

{\color{red} Approximation of Nescience}

Nescience is a theoretical concept that presents multiple issues when we try to use it in practice. In this section we are going to review these limitations, and will see how we can circumvent them.

The second problem with the theory of nescience is that we need to know the length of the shortest possible description of a topic. This poses a kind of contradiction: if we were aware of the shorted possible description, this one would be our best description, and the nescience would be equal to zero. The fact that our knowledge is incomplete implies that we do not know the shortest possible description of a topic. In order to deal with this problem, we will introduce a weak version of the concept of nescience. The weak nescience is based on the current best description alone, and it does not involves the best possible description at all:

\[
\textit{Weak Redundancy} = \frac{\textit{length of best description} - \textit{Kolmogorov complexity of best description}}{\textit{Kolmogorov complexity of best description}}
\]

Our definition of weak nescience is based on the Kolmogorov complexity of our best description. Kolmogorov complexity is a concept that also presents serious limitations when it is applied in practice. The first one is that, although the complexity does not change when we change the universal reference machine, this is true up to a constant (that depends on the selected machines, but not on the topic itself). In practice, the size of this constant could be very large in comparison to the complexity of the descriptions we are studying. What we propose to do in case of the theory of nescience is to fix a universal reference machine. Since our definition of nescience is a relative measure of what we do not know, intended to compare the topics among themselves, and not an universal measure of unknown, we can fix the an universal Turing machine, and get rid of these constants. The second problem with the Kolmogorov complexity is that it is, in general, a non-computable quantity. As we have said, there are well-defined mathematical concepts that are beyond of the theoretical capabilities of computers. Kolmogorov complexity is one of such mathematical concepts: given a description, there is no algorithm capable of computing the shortest computer program that prints that description. In practice, what we have to do is to approximate the Kolmogorov complexity by the length of the compressed text of the best known description.

TODO: How nescience and weak nescience relates to each other

}


% Weighting Method

\subsection{Weighting Method}

% The other method

\subsection{The other method}

% Evolutionary Methods

\subsection{Evolutionary Methods}


%
% Joint Nescience
%

\section{Joint Nescience}

We are also interested in our current understanding of the concatenation of two topics.

\begin{definition}
Given $t,s \in \mathcal{T}$ two different topics, and the set $\mathcal{D}_{t,s} = \{ d \in \mathcal{B}^\ast, d = \langle TM,a \rangle : TM(a) = \langle t,s \rangle \}$, let $\hat{d}_{t,s}$ be a distinguished element of $\mathcal{D}_{t,s}$. We call $\hat{d}_{t,s}$ our \emph{current best joint description} of $t$ and $s$.
\end{definition}

The concept of best joint description could be a little bit misleading, since given that the concatenation of any two topics is another topic, we could ask ourselves why do not simply use our current best description of the topic represented by that concatenation. That would make sense only in case that somebody has already studied both topics together. However, for the overwhelming majority of the possible combinations of topics, nobody has studied them yet.

\begin{example}
\label{ex:unknown_join}
Let $t$ and $s$ two different topics, and assume that nobody has studied them together before. In this case, our current best description $\hat{d}_{t, s}$ would be $\langle TM, \langle \hat{d}_t, \hat{d}_s \rangle \rangle$, where $TM$ is a Turing machine that given the input $\langle \hat{d}_t, \hat{d}_s \rangle$ prints out the string $ts$. If $\hat{d}_t = \langle TM_t, a_t \rangle$ and $\hat{d}_s = \langle TM_s, a_s \rangle$, the machine $TM$ will decode $\hat{d}_t$, run $TM_t(a_t)$ to print out $t$; then it would do the same for $\hat{d}_s$ to print $s$.
\end{example}

In the theory of nescience it is also needed to know the joint nescience of two topics. Intuitively, the joint nescience of two topics given by our current understanding of both topics, and the shortest string that allows us to effectively reconstruct them.

\begin{definition}[Joint Nescience]
Let $t,s \in T$ be two different topics. We define the \emph{joint nescience} of topic $t$ and $s$, denoted by $\nu(t,s)$, as: 
\[
\nu(t, s) = \left( \epsilon\left( t, s \right)^2 + \rho\left( t,s \right)^2 \right) ^ \frac{1}{2}
\]
\end{definition}

Next proposition shows that the combined nescience of two topics is greater or equal than the nescience of any of them isolated.

\begin{proposition}
Given any two topics $t,s \in T$, we have that $\nu(t,s) \geq \nu(t)$ and $\nu(t,s) \geq \nu(s)$.
\end{proposition}
\begin{proof}
{\color{red}TODO}
\end{proof}

%
% Conditional Nescience
%

\section{Conditional Nescience}

{\color{red} TODO: somewhere we have to prove this result}

\begin{proposition}
We have that $\sigma(d_t) > \sigma(d_{t \mid s^\star})$ for all topics $t,s \in \mathcal{T}$ and all $d_t \in \mathcal{D}_t$.
\end{proposition}
\begin{proof}
{\color{red} TODO}
\end{proof}

{\color{red} TODO: Introduce this concept}

{\color{red} TODO: Define the conditional nescience given a descriptio}

\begin{definition}[Conditional Nescience] Let $t,s\in T$ be two different topics. We define the \emph{conditional nescience} of topic $t$ given a perfect knowledge of topic $s$, denoted by $\nu( t \mid s)$, as: 
\[
\nu(t \mid s) = \left( \epsilon\left( t \mid s \right)^2 + \rho\left( t \mid s \right)^2 \right) ^ \frac{1}{2}
\]
\end{definition}

{\color{red} TODO: Explain the intuition behind this concept.}

{\color{red} TODO: Provide a practical example.}

{\color{red} TODO: Provide a maximun and a minimum for the concept}

The next proposition states that the nescience of a topic can only decrease if we assume the background knowledge given by another topic. {\color{red}: Explain the intution behind this property.}

{\color{red} TODO: Do the same for the conditonal nescience of a description.}

\begin{proposition}
We have that $\nu(t) > \nu(t \mid s)$ for all topics $t, s \in T$.
\end{proposition}
\begin{proof}
{\color{red} Review:}
\[
N_t = \frac{l\left(\hat{d_t}\right) - K(t)}{K(t)} > \frac{l\left(\hat{d_t}\right) - K(t)}{K(t) + l\left(d_s^\star \right)} = \frac{l\left(\hat{d_t}\right) + l\left(d_s^\star \right) - K(t) - l\left(d_s^\star \right)}{K(t) + l\left(d_s^\star \right)} = \frac{l\left(\langle d_s^\star, \hat{d_t} \rangle \right)-K(t \mid s)}{K(t \mid s)} = N_{t \mid s}
\]
\end{proof}

{\color{red} TODO: Prove something like N(t, s) = N(t) + N(s|t)}

{\color{red} TODO: Check the following proposition}

\begin{proposition}
$N_{t \mid s} = N_{s \mid t}$ if, and only if, $N_{t \mid s} = N_{s \mid t} = 0$.
\end{proposition}

Finally, next proposition states the relation between nescience, conditional nescience and joint nescience.

{\color{red} TODO:review}

\begin{proposition}
Given any two topics $t, s \in T$, we have that $N_{t \mid s} \leq N_{t} \leq N_{(s, t)}$.
\end{proposition}
\begin{proof}
{\color{red} TODO}
\end{proof}

The concept of conditional nescience can be extended to multiple topics $s_1, \ldots, s_n$, using the standard encoded string $\langle s_1^\star, \ldots, s_n^\star, a \rangle$ and the multiple conditional complexity $K (t \mid s_1, \ldots, s_n)$.

\begin{definition}
Let $t, s_1, \ldots, s_n \in T$ a collection of topics. The \emph{conditional nescience} of topic $t$ given the topics $s_1, \ldots, s_n$ and current best description $\hat{d_t}$ is defined as: 
\[
{\color{red} TODO}
\]
\end{definition}

%
% Section: Nescience of Areas
%

\section{Nescience of Areas}
\label{sec:nescience_areas}

Topics can be grouped into research areas. The concept of area is useful as long as all the topics included in the area share a common property. The particular details of the grouping properties depend on the practical applications of the theory.

\begin{definition}
An area $A$ is a subset of topics, that is $A \subset \mathcal{T}$.
\end{definition}

Areas can overlap, that is, given two areas $A$ and $B$ it might happen that $A \cap B \neq \varnothing$. Areas can be subsets of other areas, creating an hierarchy of areas.

\begin{example}
If the set of topics is "mathematics", examples of areas could be "calculus", "geometry" or "algebra". The areas "probability" and "statistics" largely overlap. The area "Bayesian inference" is a subarea of the area "probability".
\end{example}

{\color{red} TODO: study the properties of the research areas.}

In the same way we studied the properties of individual topics, we could study the properties of areas. An \emph{area} is a subset of topics $A\subset T$. The concept of area is useful as long as all the topics included in the area share a common property. What is exactly that property tehy share depends on the particular set $T$.

\begin{definition}
Given an area $A\subset T$, we define the \emph{average complexity of the area} $C_{A}$ as $C_{A}=\frac{1}{n}\sum_{t\in A}C_{t}$, and the \emph{average nescience of the area} $N_{A}$ as $N_{A}=\frac{1}{n}\sum_{t\in A}N_{t}$, where $n$ is the cardinality of $A$.
\end{definition}

For example, in case of research topics, an area could be a knowledge area, like biology, that will contain all the topics classified under that area. In this way we could compute and compare the complexity (how difficult is to understand) and the nescience (how ignorant we are) of mathematics, physics, biology, social sciences, and other disciplines.

{\color{red} TODO: This definition can only be introduced once we have the concept of best current description.}

An even easier approximation of the concept of redundancy of an area, is based on the average redundancy of the topics that compose that area.

\begin{definition}
Let $A \in \mathcal{T}$ an area, and $d_A$ a description. We define the \emph{average redundancy of the description} $d_A$ as:
\[
\bar{\rho}(d_{\hat{A}}) = \sum
\]
\end{definition}

{\color{red} TODO: Study some properties of this definition}

{\color{red} TODO: How these three definitions releate to each other?}


%
% Section: Perfect Knowledge
%

\section{Perfect Knowledge}

As we have said, in our opinion, the objective of scientific research should be to reduce the nescience of topics as much as possible. When it is not possible to reduce more the nescience of a topic, we propose to say we have reached a perfect knowledge. A consequence of 

\begin{definition}[Perfect Knowledge]
If the nescience of a topic $t$ is equal to cero ($\nu(t)=0$), we say that we have reached a \emph{perfect knowledge} about topic $t$.
\end{definition}

If $\nu(t)=0$ we have that $\epsilon(t) = \rho(t) = 0$, that is, perfect knowledge is achieved when we can fully reconstruct the original topic given its description, and the description does not contains any redundant elements. A consequence of our definition is that perfect knowledge implies randomness, that is, incompressible descriptions. The converse, in general, does not hold. The common point of view is that a random string should make nonsense, since this is what randomness is all about. However, in the theory of nescience, by random description we means a description that contains the maximum amount of information in the less space as possible (it contains no redundant elements).

\begin{example}
Aristotelian physics is an inaccurate description of our world, since it makes some predictions that does not hold in reality (for example, planets do not orbit around the earth). We could use a description of the Aristotelian physics and compress it using a standard compression program. The compressed file would be a random description (zero redundancy). However, given that description, our nescience would not be zero, that is, our knowledge would not be perfect, since the error of the description is not zero.
\end{example}

{\color{red} TODO: Show how nescience evolves with time}

{\color{red} TODO: Define the concept of weak nescience}

{\color{red} TODO: Explain how weak nescience and nescience relates to each other}

{\color{red} TODO: Show how the weak nescience converges to nescience in the limit}

\begin{theorem}
Let $t\in T$ a topic, and $\{d_1, d_2, \ldots \}$ where $d_i \in D_t$ a set of descriptions such that $ l(d_i) < l(d_j)$ for each $i < j$, then
\[
\lim_{x \to \infty} \hat{N_i} = N_t
\]
\end{theorem}
\begin{proof}
\textcolor{red}{To Be Done}
\end{proof}

\begin{example}
In order to clarify how the above theorem can be applied in practice, in Figure \ref{fig:Perfect_Knowledge} it is shown an hypothetical example of the typical research process required to understand a scientific topic $t\in T$. In time $t_{1}$ we have a description with length $12$, whose compressed version has a length of $5$, and so, its nescience is $1.4$. In time $t_{2}$, supposedly after some intense research effort, our current description length has been reduced to $8$ with a complexity of $4$, and our nescience has decreased to $1$. In the limit, the description length will be equal to its complexity (incompressible description), and the nescience will be 0. In this moment we could say that we have a \emph{perfect knowledge} about that particular research topic.
\end{example}

\begin{figure}[h]
\centering\includegraphics[scale=0.5]{Perfect_Knowledge}
\caption{\label{fig:Perfect_Knowledge}In Pursuit of Perfect Knowledge}
\end{figure}

{\color{red} TODO: Introduce the following definition.}

\begin{definition}
Let $s_{1}, \ldots, s_{n} \in T$ a collection of topics. The \emph{joint nescience} of topics topics $s_{1}, \ldots, s_{n}$ given the current best descriptions $\hat{d_{s_1}}, \ldots, \hat{d_{s_n}}$ is defined as: 
\[
{\color{red} TODO}
\]
\end{definition}

{\color{red} TODO: Mention, or prove, the properties of the generalized nescience}

%
% Current Best Description
%

\section{Current Best Description}

As we said in the preface of this chapter, in order to compute how much we do not know about a topic, first we need a way to quantify what we already know about that topic. How much we know about a topic will be given by our current best known description of that topic.

\begin{definition}
Given the set of descriptions $\mathcal{D}_t$ of a topic $t \in \mathcal{T}$, let $\hat{d_{t}}$ be a distinguished element of $\mathcal{D}_t$. We call $\hat{d_{t}}$ our \emph{current best description} of $t$.
\end{definition}

Which description is the current best description is something that depends on our current knowledge about the particular area in which the theory of nescience is being applied.


%
% Section: Nescience based on Datasets
%

\section{Nescience based on Datasets}
\label{sec:nescience_datasets}

Some topics can be described unsing a mathematical model that can
be evaluated by a computer. For those topics we could estimate their
complexity using a sample dataset, for example the result of an experiment.
This property allow us, among other things, to compare how well topics
are described by different models (see Chapter \ref{chap:Structured-Datasets}).

\begin{definition}
Let $t\in T$ be a topic, $D=\left\{ x_{1},x_{2},\ldots,x_{n}\right\} $
the result of running an experiment that describes $t$, and $H$
a mathematical hipothesis for $t$. The complexity of the topic $t$
given the current description $H$ and the dataset $D$ is given by

\[
\hat{C}_{t}=L(H)+L(D\mid H)
\]
as it is described by the minimum description length principle.
\end{definition}

Given the dataset $D$ and the model $H$ we can compute our current
nescience of a topic as it is described by the next definition.

\begin{definition}
Let $t\in T$ a topic, $D=\left\{ x_{1},x_{2},\ldots,x_{n}\right\} $
the result of running an experiment that describes $t$, $C$ a code
that minimizes the length of $D$, and $H$ a mathematical hipothesis
for $t$. The current complexity of the topic $t$ given the dataset
$D$ and the hyphotesis $H$ is given by

\[
N_{t}=\frac{\hat{C_{t}}-l_{C}(D)}{l_{C}(D)}
\]

\end{definition}

In practice, the dataset $D$ also allows us to approximate our current
nescience of a topic $t$, given the model $H$. What we have to do
is to use a near minimal encoding for $D$, for example, by using
a Huffam encoding.


%
% Section
%

\section{Unknonwn Unknown}

Knonw unknown

Unknown unknown

Finally, there exists a last category of unknonw, what we call the unknowable unknown unknown. This category refers to those entities we

Unknowable unknown unknonwn. In this book we are interested in the unknown unknown

%
% Section: References
%

\section*{References}

{\color{red} TODO: Add the paper of Chaitin about the Berry paradox}

{\color{red} TODO: That there are numbers that are not computable can be found in the original paper of Turing}

{\color{red} TODO: Perhaps Ii should provide a couple of references in epistemology and ontology}
