* Introduction

\begin{example}
In general it is very difficult to encode abstract objects, like for example, mathematical theories. An option would be to use as an approximation a text summarizing as accurately and concise as possible our current knowledge about them (encyclopedic coverage). For example, if the topic is "Theory X", we could use as its encoding a book called "Foundations of X". Unfortunately, the quality of this encoding will depend on how well we understand the topic itself.
\end{example}

, or even better, to the set of natural numbers $\mathbb{N}$. What we are looking for is a kind of Gödel numbering. However, as Gödel himself proved with his incompleteness theorem, this is not possible, at least in case of mathematical objects.

Sometimes it is cumbersome to include all the information needed to reconstruct a topic in its model, since that would require very large strings. In those cases, it is usually more convenient to assume some already existing background knowledge, and compute how much we do not know about a topic given that background. For example, in case of mathematics, a topic description should include all the definitions and theorems required to explain the topic until the axioms are reached.

Please note that what we propose is an mathematical approach to the \emph{assisted} discovery of potentially interesting questions. We do not intend that the computer will understand the meaning of the posed questions. Moreover, we do not intend that all the questions are relevant or even meaningful. It is up to the researchers to evaluate the proposed combinations of topics to decide which are exactly the questions they suggest, and to decide if it is worth to pursue them.

If we want to compute how much we do not know about a research topic we have to provide two things: what we already know about that topic, and where is the theoretical limit of what can be known. What we already know about a topic is given by our current best description of that topic. The most valuable descriptions are those that allow us to fully and effectively reconstruct the original topic given its description (they are computable), since this kind of descriptions allow us to avoid many of the paradoxes that arise when we try to formalize the concept of "description". The theoretical limit of what can be known about a topic is given by its perfect description. The perfect description is the shortest possible string that allows us to reconstruct the original topic. However, in general, we do not know that shortest description, since our knowledge is incomplete. Fortunately we know that this description must necessarily be a random string, and so, by means of computing how far our current description is from being a random string, we can estimate in practice how far we are from the ideal shortest one.


Representations

A \emph{research area}\index{Research area} $A$ is a subset of topics, in other words, $A \subset \mathcal{R}_\mathcal{E}$. In a practical context, areas prove valuable when their included topics share a common attribute. Our interest may lie in investigating our knowledge gaps about a particular area. For instance, if the set of topics is "animals", areas could include "invertebrates", "mammals", "birds", "amphibians", "reptiles", and "fish". To gauge our lack of knowledge about an area, we first need to describe the topics encompassed by it. However, as our understanding of the topics constituting an area is generally limited, we can only provide partial descriptions. Furthermore, as our comprehension of a research area evolves, the quantity of topics included in its known subset adjusts accordingly. As a result, we can only examine the properties of areas relative to our present knowledge.

\begin{example}
Should our topic set revolve around "astronomy," a conceivable research area might be "habitable planets," a category wherein we have knowledge of only a few currently. A model for this area could be a description compiled from our existing knowledge of these habitable planets.
\end{example}

--

* Miscoding

Miscoding is a metric that quantitatively measures the quality of the representations we are using to solve a problem or to describe a collection of objects. Intuitively, miscoding is based on the length of the shortest computer program that can reconstruct the original entity under study given a string based representation. The lower the miscoding, the better the quality of the representations. Miscoding is a very important concept in machine learning, since the errors present in a representation could trigger errors in the model developed to solve a problem.

\begin{example}
In 1900 the German mathematician David Hilbert proposed his famous collection of twenty-three problems. Problem number ten required to devise \emph{"a process according to which it can be determined in a finite number of operations whether a given Diophantine equation is solvable in rational integers"}. Let's call this problem formulation topic $t_1$. In 1970, the Russian mathematician Yuri Matiyaevich proved that such a "procedure" does not exists. Let's call Matiyaevich's theorem topic $t_2$. Mathematicians have used both topics, $t_1$ and $t_2$, as representations of the same entity. Of course, the miscoding of $t_2$ is much lower\footnote{Whether $\mu(t_2)$ is equal to $0$ or not is out of the scope of this book.} than the miscoding of $t_1$. But the important point is that in 1900 mathematicians had a very different idea of what kind of entity was represented by $t_1$.
\end{example}

* Decision trees

The nescience of a candidate solution will be approximated by computing the redundancy of the function that implements the tree, like the one show in \ref{ex:example_tree}, and the relative error of this function when it is applied to the original training dataset. The inaccuracy of a model $\iota(m_t)$ will by approximated by the ratio $|Comp(E)| / |Comp(\mathcal{X})|$, where $|Comp(E)|$ is the length of the compressed version of the subset $E \subset \machcal{X}$ composed by those points that have been misclassified by the tree, and $|Comp(\mathcal{X})|$ is the length of the compressed version of the full dataset. The redundancy of a model $\rho(m)$ will be approximated by computing $1 - |Comp(T)| / |T|$, where $|Comp(T)|$ is the length of the compressed string that represents the tree, and $|T|$ is the length o the uncompressed string. The approximated nescience of the candidate model will be given by the harmonic mean of these two quantities.

Let $f:\mathbb{R}^p \rightarrow \mathcal{G}$ be a classification function from the set of input vectors $\mathbb{R}^p$, where each vector $\textbf{x} = \{x_1, x_2, \ldots, x_p \}$ is composed by $p$ features, to the set of classification labels $\mathcal{G} = \{0, 1, \ldots, \ell\}$, and let $\mathbb{X} \subset \mathbb{R}^p$ be a subset of $n$ training vectors with their corresponding target vector $\textbf{y} \in \mathcal{G}^n$. We are interested in to find a model $\hat{f}$ from a family $\mathcal{F}$ of non-parametric models of binary decision trees, based on the training pair $\left( \mathbb{X}, \textbf{y} \right)$, such that $\hat{f}(\textbf{x})$ is as close as possible the real values $f(\textbf{x}) = y$. That is, we are interested in to solve a supervised classification problem (refer to Section \ref{sec:machine_learning} for more information about this kind of problems).

--



--

Bias vs variance

{\color{red} it is possible to show that the expected test MSE, for a given value $x_o$, can always be decomposed into the sum of three fundamental quantities: the variance of $\hat{f}(x_o)$, the squared bias of $\hat{f}(x_o)$ and the variance of the error terms $\epsilon$. That is,
\[
E \left( y_o - \hat{f}(x_o) \right) ^ 2 = Var \left( \hat{f} (x_o) \right) + \left[ Bias \left( \hat{f}(x_o) \right) \right] ^ 2 + Var(\epsilon)
\]
$E \left( y_o - \hat{f}(x_o) \right) ^ 2$ defines the expected test MSE, and refers to the average test MSE that we would obtain if we repeatedly estimated $f$ using a large number of training sets, and tested each at $x_o$ [...] in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias [...] the expected test MSE can never lie below $Var(\epsilon)$, the irreducible error [...] Variance refers to the amount by which $\hat{f}$ would change if we estimated it using a different training data set [...] in general, more flexible statistical methods have higher variance [...] bias efers to the error that is introduced by approximating a real-life problem, which may be extremely complicaed, by a much simpler model [...] As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases. As we increase the felexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the exected test MSE declines. However, at some point incresing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases [...] The relationship between vias, variance, and test set MSE [...] is referred to as the bias-variance trade-off [...] it is easy to obtain a method with very low variance but high variance [...] or a method with very low variance but high bias [...] the challenge lies in finding a method for which both the variance and the squared bias are low [...] In a real-life situation in which $f$ is unobserved, it is generally not possible to explicitly compute the test MSE, bias, or variance for a statistical learning method [...] cross-validation [...] is a way to estimate the test MSE using the training data.}

{\color{red} The degrees of freedom is a quantity that summarizes the flexibility of a curve [...] the irreducible error [...] corresponds to the lowest achievable test MSE among all possible methods [...] as the flexibility of the statistical learning method increases, we observe a monotone decrease in the training statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used [...] when a given method yields a small training MSE but a large test MSE, we aresaid to be overfitting the data [...] may be picking up some patterns that are just caused by random change rather than by true properties of the unknown function $f$ [...] estimating test MSE is [...] difficult because usually no test data are available [...] the flexibility level corresponds to the model with the minimal test MSE can vary considerable among data sets [...] we discuss a variety of approaches that can be used in practice to estimate this minimum point. One imprtatn method is cross-validation [...] which is a method for estimating test MSE using the training data.}

{\color{red} The accuracy of $\hat{Y}$ as a predictor for $Y$ depends on two quantities [...] the reducible error and he irreducible error [...] $\hat{f}$ will not be a perfect estimate for $f$ [...] this error is reducible because we can potentially imporve the accuracy of $\hat{f}$ by using the most appropiate statistical learning technique to estimate $f$ [...] $Y$ is also a function of $\epsilon$ [...] we cannot reduce the error introduced by $\epsilon$. [...] The quantity $\epsilon$ may contain unmeasured variables that are useful in predicting $Y$ [...] The quantity $\epsilon$ may also conain unmeasurale variation.
\[
E \left( Y - \hat{Y} \right) ^ 2 = E \left( f(X) + \epsilon - \hat{f} (X) \right) ^ 2 = \left( f(X) - \hat{f} (X) \right) ^ 2 + Var(\epsilon)
\]
[...] the irreducible error will always provide an upper bound on the accuracy of our prediction for $Y$. This bound is almost always unknown in practice.}

--

Multiobjective optimization

{\color{red} [...] we want to minimize all the objective functions simultaneously [...] we assume that there does not exist a single solution that is optimal with respect to every objective function, otherwise there would be a trivial solution [...] objective functions are at least partly conflicting [...] they may also be incommensurable [...]}

{\color{red} [...] we denote the image of the feasible region by $Z (=f(S))$ and call it a feasible objective region. It is a subset of the objective space $\mathbb{R}^k$. The elements of $Z$ are called objective vectors or criterion vectors and denoted by $f(\textbf{x})$ or $\textbf{z} = (z_1, z_2, \ldots, z_k)^T$, where $z_i = f_i(x)$ for all $i = 1, \ldots, k$ are objectieve values or criterion values.}

{\color{red} [...] we assume that all the objective functions are to be minimized. If an objective function $f_i$ is to be maximized, it is equivalent to minimize the function $-f_i$.}

{\color{red} If at least one of the objective or the constraint functions is nonlinear, the problem is called a nonlinear multiobjective problem}

{\color{red} TODO: Study if the sets $\mathcal{T}$ and $\mathcal{M}$ are convex; the same for the functions $\mu, \sigma, \rho$. If this is the case, add here the definiton and properties of convex multiobjective optimization problem.}

{\color{red} Because the contradiction and possible incommensurabiltiy of the objective functions, it is not possible to find a single solution that would be optimal for al the objectives simultaneously. Multiobjective optimization problems are in a sense ill-defined.}

{\color{red}
\begin{definition}
A decision vector $x^\ast \in S$ is \emph{Pareto optimal} if there does not exist another decision vector $x \in S$ such that $f_i(x) \leq f_i(x^\ast)$ for all $i=1, \ldots, k$ and $f_j(x) < f_j(x^\ast)$ for at least one index $j$.
\end{definition}
}

{\color{red} An objective vector $z^\ast \in Z$ is Pareto optimal if there does not exists another objective vector $z \in Z$ such that $z_i \leq z_i^\ast$ for all $i=1, \ldots, k$ and $z_j < z_j^\ast$ for at least one index $j$; or equivalently, $z^\ast$ is Pareto optimal if the decision vector corresponding to it is Pareto optimal}

{\color{red} There are usually a lot (infinite number) of Pareto optimal solutions. We can speak about a set of Pareto optimal set.}

\begin{definition}
A decision vector $x^\ast \in S$ is locally Pareto optimal if there exists a $\delta > 0$ such that $x^\ast$ is Pareto optimal in $S \cap B(x^\ast, \delta)$. An objective vector $z^\ast \in Z$ is locally Pareto optimal if the decision vector corresponding to it is locally Pareto optimal.
\end{definition}

{\color{red} [...] any globaly Pareto optimal solution is locally Pareto optimal. The converse is valid for convex multiobjective optimization problems.}

--

Machine Learning

Explain how the bias-variance trade-off is explained under the concept of miscoding.

The distinction between train data and test data makes no sense in the context of the theory of nescience.

In the theory of nescience we do not distinguish between the problem of inference and prediction. Both tasks produce exactly the same models.

The \emph{reducible error} is what we call \emph{inaccuracy}, and the \emph{reducible error} is our \emph{miscoding}. And of course, we do not agree that this kind of error is "irreducible". We thing it can, and it must, be reduced. In statistical learning it is assumed that the irreducible error puts a limit on what we can learn about a dataset. Here we show that this is not the case {\color{red} TODO: explain what we mean}.

 In practice the function $f$ could be partial, the training pair $\left( \mathcal{X}, \mathcal{Y} \right)$ might too small or contain errors, and the family $\mathcal{F}$ of functions does not have any function that solves the problem, and so, an approximation has to be found.

In terms of the theory of nescience, we have a unknown entity $e \in \mathcal{E}$, represented by the topic $\left( \mathcal{X}, \mathcal{Y} \right)$, and we want to find a model from a subset of all possible models. Inaccuracy OK. Surfeit can be only computed relatively to the subset of models used. And misconding can only be reduced by means of using subsets of the topic, instead of the full topic. That is, we can do anything about those elements of $e$ do not encoded by the topic, but we can try to get rid of the non-needed elements of t.

In this chapter we are going to see how we can combine methods from the area of statistical learning with our results of the theory of nescience, in order to find the best models (descriptions) that describe our entity based on its encoding as a dataset. The goal of the chapter is double, to produce an interpretable model that explain the abstract entity, and to proce a predictive model that allow us to forecast new, previously unseen, values.

% Approximation of nescience - weak nescience

{\color{red} Approximation of Nescience}

Nescience is a theoretical concept that presents multiple issues when we try to use it in practice. In this section we are going to review these limitations, and will see how we can circumvent them.

The second problem with the theory of nescience is that we need to know the length of the shortest possible description of a topic. This poses a kind of contradiction: if we were aware of the shorted possible description, this one would be our best description, and the nescience would be equal to zero. The fact that our knowledge is incomplete implies that we do not know the shortest possible description of a topic. In order to deal with this problem, we will introduce a weak version of the concept of nescience. The weak nescience is based on the current best description alone, and it does not involves the best possible description at all:

\[
\textit{Weak Redundancy} = \frac{\textit{length of best description} - \textit{Kolmogorov complexity of best description}}{\textit{Kolmogorov complexity of best description}}
\]

Our definition of weak nescience is based on the Kolmogorov complexity of our best description. Kolmogorov complexity is a concept that also presents serious limitations when it is applied in practice. The first one is that, although the complexity does not change when we change the universal reference machine, this is true up to a constant (that depends on the selected machines, but not on the topic itself). In practice, the size of this constant could be very large in comparison to the complexity of the descriptions we are studying. What we propose to do in case of the theory of nescience is to fix a universal reference machine. Since our definition of nescience is a relative measure of what we do not know, intended to compare the topics among themselves, and not an universal measure of unknown, we can fix the an universal Turing machine, and get rid of these constants. The second problem with the Kolmogorov complexity is that it is, in general, a non-computable quantity. As we have said, there are well-defined mathematical concepts that are beyond of the theoretical capabilities of computers. Kolmogorov complexity is one of such mathematical concepts: given a description, there is no algorithm capable of computing the shortest computer program that prints that description. In practice, what we have to do is to approximate the Kolmogorov complexity by the length of the compressed text of the best known description.

TODO: How nescience and weak nescience relates to each other

--

Difference between mean and expected value

It is worth to clarify the difference between mean and expected value. In case of continuous statistical variables, the mean is calculated based on the frequency of each attribute value observed in the population. Typically, we aggregate individuals into class intervals, and calculate the mean using class marks. However, for random variables, the expectation is computed directly from the theoretical probabilities associated with these intervals. Consequently, the mean and the theoretical expected value may differ due to measurement errors, rounding errors of empirical values, or inaccuracies in the theoretical probability model selected. In the context of a discrete probability space $\left( \Omega, \mathcal{A}, P \right)$, which is the type of space we focus on in this book, the concepts of the mean and expected might provide the same value. This equivalence holds as long as the probability space $\left( \Omega, \mathcal{A}, P \right)$ is sufficiently granular, that is, for every $\omega$ in $\Omega$ the singleton set $\{ \omega \}$ belongs to $\mathcal{A}$, and probabilities resembles frequencies. Neither the mean nor the expected value makes sense in the case of categorical variables.

---

Bayesian inference

Bayes' theorem is the tool we have at our disposal to derive the posterior distribution given a prior distribution and the observations.

\begin{proposition}
{\color{red} TODO: Review}
Let $\mathbf{X} = \left( X_{1}, \ldots, X_{n} \right)$ be a random sample from a random variable $X$ with parameter $\theta$ and probability mass function $f\left( x \mid \theta \right)$. Suppose that the value of the parameter $\theta$ is unknown and its prior probability mass function is $\xi\left( \theta \right)$. Then the posterior probability mass function $\xi\left( \theta \mid \mathbf{x} \right)$ of $\theta$ is
\[
\xi\left( \theta \mid \mathbf{x} \right) = \frac{ f\left( x_{1} \mid \theta \right) \cdots f\left( x_{n} \mid \theta \right) \pi\left( \theta \right) }{ m\left( \mathbf{x} \right) } \quad \text{for } \theta \in \Theta
\]
where $m\left( \mathbf{x} \right)$ is the marginal probability mass function of the observed data $\mathbf{x}$:
\[
m\left( \mathbf{x} \right) = \sum_{\theta \in \Theta} f\left( x_{1} \mid \theta \right) \cdots f\left( x_{n} \mid \theta \right) \pi\left( \theta \right)
\]
\end{proposition}
\begin{proof}
{\color{red} TODO: Pending}
\end{proof}

There is no guarantee that the posteriory distribution $\xi\left(\theta\mid\mathbf{x}\right)$ belong to the same family of distributions that the prior distribution $\xi\left(\theta\right)$. That situation only happens for special kinds of probability distributions called \emph{conjugate families}\index{Conjugate probability distribution}. Please, refer to the references included at the end of this chapter for more information on conjugate families of probability distributions.

\begin{example}
{\color{red} TODO: Review}
Suppose we are interested in estimating the probability $p$ that a coin lands on heads. Before conducting any experiments, we have no prior preference for any value of $p$, so we use a uniform prior over $p$ in the interval $[0,1]$. We flip the coin $n = 10$ times and observe $k = 7$ heads.

The likelihood function is given by the binomial probability mass function:
\[
f(k \mid p) = \binom{n}{k} p^{k} (1 - p)^{n - k}
\]
The prior distribution of $p$ is uniform on $[0,1]$, so the prior density is:
\[
\pi(p) = 1 \quad \text{for } 0 \leq p \leq 1
\]
Using Bayes' theorem, the posterior distribution of $p$ given the data is proportional to the likelihood times the prior:
\[
\pi(p \mid k) \propto f(k \mid p) \pi(p) = \binom{n}{k} p^{k} (1 - p)^{n - k}
\]
Since the prior is uniform (a Beta distribution with parameters $\alpha = 1$, $\beta = 1$), the posterior distribution is a Beta distribution with parameters $\alpha = k + 1$ and $\beta = n - k + 1$:
\[
\pi(p \mid k) = \frac{1}{B(k + 1, n - k + 1)} p^{k} (1 - p)^{n - k}
\]
where $B(\alpha, \beta)$ is the Beta function.

In our case, with $k = 7$ and $n = 10$:
\[
\pi(p \mid k) = \frac{1}{B(8, 4)} p^{7} (1 - p)^{3}
\]
Thus, after observing the data, our posterior distribution of $p$ is a Beta distribution with parameters $\alpha = 8$ and $\beta = 4$, reflecting that we now believe $p$ is more likely to be around $0.7$.
\end{example}

Recall that the random variables that compose the random sample $\mathbf{X}$ must be independent and identically distributed according to the probability mass function $\xi\left(\theta\mid\mathbf{x}\right)$. Also, recall our theoretical concerns on Bayes' theorem being a direct consequence of the definition of conditional probability (see Theorem \ref{th:Bayes_theorem}), rather than being derived from Kolmogorov's axioms.

It might also happen that our assumptions on the prior distribution of the parameter $\theta$ might are wrong. Fortunately, in practice, the consequences of assuming a wrong prior distribution can be mitigated by increasing the number of observations.

\begin{example}
{\color{red} TODO: Review}
Suppose we are estimating the probability $p$ of success in a Bernoulli trial (e.g., flipping a biased coin). We mistakenly believe that $p$ is likely to be small, so we choose a Beta prior distribution with parameters $\alpha = 1$, $\beta = 5$, which favors small values of $p$.

Now, suppose we perform a large number of trials, say $n = 1000$, and observe $k = 600$ successes.

The likelihood function is:
\[
f(k \mid p) = \binom{n}{k} p^{k} (1 - p)^{n - k}
\]
The prior distribution is:
\[
\pi(p) = \frac{1}{B(1,5)} p^{0} (1 - p)^{4}, \quad 0 \leq p \leq 1
\]
The posterior distribution is proportional to the likelihood times the prior:
\[
\pi(p \mid k) \propto p^{k} (1 - p)^{n - k + 4}
\]
This is a Beta distribution with parameters $\alpha' = k + \alpha = 600 + 1 = 601$, and $\beta' = n - k + \beta = 400 + 5 = 405$.

Even though our prior favored small values of $p$, the posterior distribution reflects the data, indicating that $p$ is around $0.597$. As the sample size increases, the influence of the prior diminishes, and the posterior distribution becomes more concentrated around the maximum likelihood estimate.
\end{example}
